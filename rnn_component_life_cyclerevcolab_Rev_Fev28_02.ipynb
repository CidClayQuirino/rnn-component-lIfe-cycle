{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/rnn_component_life_cyclerevcolab_Rev_Fev28_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oauv_Ubh_2H"
      },
      "source": [
        "**Title: \"TCC_USP ESALQ 25_01_2024\"\n",
        "Author: \"Cid Clay Quirino\"\n",
        "Date: \"2024-01-25\"**\n",
        "\n",
        "\n",
        "Instalaçao de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QO-qYFKDdQEl",
        "outputId": "65a271ba-6c4d-4950-b658-88f269f699a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnNynpDka56Y"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLjFpBYUFaL6"
      },
      "outputs": [],
      "source": [
        "!pip install markdown\n",
        "!pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install PyGithub\n",
        "!pip install gitpython\n",
        "!pip install statsmodels\n",
        "!pip install dash"
      ],
      "metadata": {
        "id": "FmeqzDVNHsHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_YyXAwShuZN"
      },
      "outputs": [],
      "source": [
        "# Atualizar pacotes\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from IPython.display import Image\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHszVB194n8u"
      },
      "source": [
        "# 1.0 Introduction\n",
        "\n",
        "Manufacturers and dealers around the world, are constantly approached by customers about how to consistently achieve best resulting in anticipate machines brackdown. One way to move forward on this issue is using the condition monitoring process output and recently the Tecnology is helping with some interesting solution for new peocces and products like internet of things (iot) that provide for industries a vision architectural elements for a better definition for a future directions. In certain mining assets components (Ex: Diesel Engines/Transmission and major components) used in mobile equipment models this measurement can be done directly through “Embedded electronic system” to collect and share online monitoring system data.\n",
        "\n",
        "In mobile equipment models, main components have highly predictability parameters, but some other type of components like Cylinders, Pumps and in general minor components is not possible detect failures, mainly due a low number of strong parameters to monitor and cost envolving on Engineering initiatives x cost saved in field failures.\n",
        "\n",
        "Hence the CMMS (Conditions Monitoring Management System) needs to be improved to envolve this components used the new approach package, using other practices/technologies to evaluate changes in component behavior and acting before failure. On the other hand, and as informed above, today is there in market, some interesting new technologies of low cost initiatives in comparison to cost saving, to be apply on this components, and increase the CMMS coverary. One real case is shared here on this aticle, to clarify this initiative, and inderstand how the new practices with low cost, is helping mining customer in the north of Brazil to have better results applying the practices of CMMS to monitor the behavior of minor components, and prevent unplanned failure and reducing asset downtime.\n",
        "\n",
        "This study understands the importance of this best practices, but the metodology used was not enougth to antecipate the failures, and we wants to improve this solution using IOTs available on our market to get data directly from a sender installer on this minor components to detect early as possible any changes in temperature and vibration.\n",
        "\n",
        "The main problem generated by this improvimentis: How to manager the higth number of data generated after installing this IOT? and how to ensure that humans make rigth decisions and act before failure?. To help on this problem, will be apply some Data Science and Analytics concepts to, after collecting and loading this data on Data Frame, evaluating this data using a statistical models and helping the humans to make this decision. Between this modelsit was make some testing used RNN, LSTM and others to be possible to evaluate the behavior of this parameters and any modification on thie data behavior that is considered an abnormal condition.\n",
        "\n",
        "1) Implement a decision support tool using _ IOTs _ and _ Statistical models _ in small components on mobile mining equipments, to improve exchange predictability and reduce failure rates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EIYypJ8FsWj"
      },
      "source": [
        "\n",
        "# 2.0 Project Description\n",
        "\n",
        "Approximately 3 years ago, practices and monitoring of small components were implemented by Mining customers in the most varied regions of Brazil, with good results reported specifically by customers in the North region. This good practice generated gains in improving the predictability of changing small components, identifying faults with a lower level of internal degradation of the assembly and reducing renovation costs as well as machine downtime. It is worth mentioning that such components, such as cylinders, pumps, reducers and rotating gears, were assessed for their condition only using sensitive methods that were considerably less efficient in terms of predictability of failure.\n",
        "\n",
        "One of the points of this study is to improve this monitoring process, using technologies available on the market and, additionally, to structure a robust analysis using statistical tools to support the decision. Greatly increasing the monitoring frequency as well as anticipating changes in the behavior of these components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGOXzrk4zHZ"
      },
      "source": [
        "## 2.1 Previous methodology (Periodic inspection with thermal camera)\n",
        "\n",
        "This previous practice is part of the client in the northern region in an effort to improve the process of monitoring and troubleshooting cylinders and small components of machines in the mobile equipment fleet.\n",
        "\n",
        "The development of monitoring/troubleshooting alternatives was based on the customer's need to reduce repair time and improve fleet availability/productivity.\n",
        "\n",
        "Thermography, together with other operational tests, provided, in this case, the best way to monitor and provide detection capabilities before catastrophic failures and, in some cases, helped define future interventions in the fleet, with gains in reducing costs, time stopping and operational safety of the fleet.\n",
        "\n",
        "Basic Operation:\n",
        "\n",
        "Greater detail on this previous process can be explained with the following steps, where the inspector evaluates the behavior of the hydraulic system following the manufacturer's operating procedures, and records parameters at each inspection, such as:\n",
        "\n",
        "=> Cylinder descent and ascent time,\n",
        "\n",
        "=> Hydraulic pressure during tests;\n",
        "\n",
        "=> Flow rate of hydraulic pumps;\n",
        "\n",
        "Image 1 below shows the ideal temperature of the hydraulic fluid for carrying out the tests, a premise to ensure that the values are being compared as recommended by the manufacturer.\n",
        "\n",
        "\n",
        "[Image 1.  Fluid Temperature](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%201.%20%20Fluid%20Temperature.png)\n",
        "\n",
        "Image 2 already shows the position of the equipment, to perform the descent and ascent speed test of the implement, after ensuring that the temperature is within the specified range.\n",
        "\n",
        "\n",
        "[Image 2. Imagem Escavadeira Hidraulico Antes Falha](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%202.%20Position%20of%20the%20drop%20test%20equipment.png)\n",
        "\n",
        "During performing the cycle speed test, the pump relief pressures were checked for the boom lift function, and found 5206 psi for the 01# pump and 5173 psi for the 02# pump (Figure 3).\n",
        "\n",
        "\n",
        "\n",
        "[Image 3 - Pump Pressure](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%203.%20Pump%20Pressure.png)\n",
        "\n",
        "\n",
        "\n",
        "Note: The pressures were a little above the specified, but not enough to be considered a failure.\n",
        "\n",
        "The machine implement was positioned as oriented by factory literature to perform the cylinder cycle speed test and the cylinder cycle time from soil to fully extended rod position, and ir was collected 7.26 seconds.\n",
        "\n",
        "The cylinder speed procedure specifies a time of 5.8 ± 0.5 for a new cylinder, a maximum of 7.5 seconds for reconditioning the cylinder and a maximum of 8.7 seconds for service limit for the boom cylinder (Figure 05).\n",
        "\n",
        "[Image 4  - Pump Pressure Test](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%204%20%20-%20Pump%20Pressure%20Test.png)\n",
        "\n",
        "The procedure for setting the cycle time defined in Troubleshooting CAT was carried out, and the lifting time specified by the lower operation and maintenance manual was detected.\n",
        "\n",
        "After finishing the cycle time and pressure tests, the thermography test was performed, with the aid of the thermographic camera, so that the differential temperature of the cylinders was recorded.\n",
        "\n",
        "Using thermography on the machine Cylinder, a thermal differential was identified in the boom cylinder on the right side, as can be seen in Figure 5a and 5b with a temperature differential of 4.7°C in relation to the left side.\n",
        "\n",
        "\n",
        "[Image 5a. Thermography on the Escavator Lift Cylinders](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%205a.%20Thermography%20on%20the%20EH3201%20Lift%20Cylinders.png)\n",
        "\n",
        "\n",
        "[Image 5b. Escavator on the operation front being inspected by predictive team](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%205b.%20EH3201%20on%20the%20operation%20front%20being%20inspected%20by%20predictive%20team.png)\n",
        "\n",
        "The hottest points of the cylinders were captured by the camera and it was observed that the lifting cylinder l/d of the boom has a temperature of 5° c above the lifting cylinder l/e (Image 6).\n",
        "\n",
        "\n",
        "[Image 6.  Thermographic image indicating a 5° c difference between the hottest points of the boom lift cylinders](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%206.%20%20Thermographic%20image%20indicating%20a%205%C2%B0%20c%20difference%20between%20the%20hottest%20points%20of%20the%20boom%20lift%20cylinders.png)\n",
        "\n",
        "After removing and disassembly the cylinder on repair workshop, allowed us to confirm that the field symptoms related was important to define and conclude that the thermographic method can be used to define with more assurance that the cylinder needed to be removed, see Figure 8. Reducing, but not so much, the repair cost and the impact on the hydraulic system contamination.\n",
        "\n",
        "[Image 8.  Cyclinder at the CRC](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%208.%20%20Cyclinder%20at%20the%20CRC.png)\n",
        "\n",
        "Next Image 8a, 8b and 8c shows more details after disassembling the cylinder .\n",
        "\n",
        "See bellow that the failure mode presented in Image 9, represent the cause os temperature increase in field termografic process.\n",
        "\n",
        "[Image 8a.  Images of disassemlbed cylinder at CRC 1](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%208.%20%20Cyclinder%20at%20the%20CRC.png)\n",
        "\n",
        "[Image 8b. Images of disassemlbed cylinder at CRC 2 ](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%209.%20%20Images%20of%20disassemlbed%20cylinder%20at%20CRC%201.png)\n",
        "\n",
        "\n",
        "[Image 8c. Images of disassemlbed cylinder at CRC 3 Image](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%209.%20%20Images%20of%20disassemlbed%20cylinder%20at%20CRC%203.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIQXksKn5KWA"
      },
      "source": [
        "\n",
        "# 3.0 New Application Using Deep Learning and Online Data Colection\n",
        "\n",
        "\n",
        "The improvement defined for this monitoring process, is a New Application, using technologies available on the market to increasing the monitoring frequency and anticipating changes in the behavior of these components usind a Machine Leaning process, see the Image 9.\n",
        "\n",
        "Image 9. New application on Major Mining Excavator\n",
        "\n",
        "Image 9. New application on Major Mining Excavator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duajl_fA46Lz"
      },
      "source": [
        "\n",
        "## 3.1 Project esquematic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfacXrx5CrL"
      },
      "source": [
        "\n",
        "## 3.2 Implementation Steps\n",
        "\n",
        "Image 7. Project Data Collection on CAT 6030\n",
        "\n",
        "[Image 7. Project Data Collection on New Machine](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%207.%20%20Project%20Datra%20Collection%20on%20CAT%206030.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdwDzUMvnSl0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# URL do repositório no GitHub\n",
        "repo_url = 'https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/archive/main.zip'\n",
        "dataframes = []\n",
        "\n",
        "# Baixe e extraia o arquivo zip do repositório\n",
        "response = requests.get(repo_url)\n",
        "with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "    zip_file.extractall()\n",
        "\n",
        "# Diretório onde os arquivos .xlsx foram extraídos\n",
        "extracted_dir = 'rnn-component-lIfe-cycle-main'\n",
        "\n",
        "# Loop pelos arquivos no diretório extraído\n",
        "for arquivo in os.listdir(extracted_dir):\n",
        "    if arquivo.endswith('.xlsx'):\n",
        "        # Construa o caminho completo para o arquivo\n",
        "        caminho_completo = os.path.join(extracted_dir, arquivo)\n",
        "\n",
        "        # Leia o arquivo Excel e adicione-o à lista de DataFrames\n",
        "        df = pd.read_excel(caminho_completo)\n",
        "\n",
        "        # Adicione uma coluna 'TagComp' contendo o nome do arquivo sem a extensão\n",
        "        df['nome_arquivo'] = os.path.splitext(arquivo)[0]\n",
        "\n",
        "        # Adicione o DataFrame à lista\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatene todos os DataFrames em um único DataFrame\n",
        "BDadosRNN = pd.concat(dataframes, ignore_index=True)\n",
        "BDadosRNN = BDadosRNN[(BDadosRNN != 0).all(axis=1)]\n",
        "BDadosRNN['Value'] = BDadosRNN['Value'].round(1)\n",
        "BDadosRNN = BDadosRNN.rename(columns={'Tag': 'Parametro'})\n",
        "BDadosRNN = BDadosRNN.rename(columns={'nome_arquivo': 'NmeComp'})\n",
        "#BDadosRNN = BDadosRNN.drop('timestamp', axis=1)\n",
        "BDadosRNN.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nM5ZIvmilk8"
      },
      "source": [
        "\n",
        "## 3.3 Exploration Data Analisys\n",
        "\n",
        "\n",
        "\n",
        "[Figure 8.  Exploration Data Analisys on new model](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%208.%20%20Exploration%20Data%20Analisys%20on%20CAT%206030.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb0Lqv2NIOq8"
      },
      "source": [
        "Video para instalação do Python Virtual Enviromental\n",
        "https://www.youtube.com/watch?v=UPaN3Z49myw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fts2d0FUis9t"
      },
      "outputs": [],
      "source": [
        "# Filtrar os dados\n",
        "\n",
        "BDados_Temp = BDadosRNN[BDadosRNN['Parametro'] == 'Temperature'][['NmeComp', 'Timestamp', 'Value']]\n",
        "BDados_Temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supondo que você tenha um DataFrame chamado df com as colunas necessárias\n",
        "df = BDados_Temp\n",
        "# Lista de componentes para os boxplots\n",
        "componentes = [\"BoomCylinder_LD\", \"BoomCylinder_LE\", \"ShellCylinder_LD\", \"ShellCylinder_LE\", \"BucketCylinder_LD\",\n",
        "               \"BucketCylinder_LE\", \"PumpDrive_1\", \"PumpDrive_2\", \"StickCylinder_LD\", \"StickCylinder_LE\",\n",
        "               \"SwingDrive_LD\", \"SwingDrive_LE\", \"SwingGear_1\", \"SwingGear_2\", \"FinalDrive_LD\", \"FinalDrive_LE\"]\n",
        "\n",
        "# Criando subplots para os boxplots\n",
        "fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n",
        "\n",
        "# Iterando sobre os componentes e criando boxplots para cada um\n",
        "for i, componente in enumerate(componentes):\n",
        "    row = i // 4\n",
        "    col = i % 4\n",
        "    sns.boxplot(x='NmeComp', y='Value', data=df[df['NmeComp'].str.contains(componente)], ax=axs[row, col])\n",
        "    axs[row, col].set_title(f'Boxplot para {componente}')\n",
        "    axs[row, col].set_xlabel('NmeComp')\n",
        "    axs[row, col].set_ylabel('Value')\n",
        "\n",
        "# Ajustando o layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v3c8dIthQM8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_HssZC3npDJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_BDados_Temp_Sumarize.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_BDados_Temp_Sumarize = pd.read_csv(github_url)\n",
        "\n",
        "# Verificando se as colunas 'mean' e 'std' existem no DataFrame\n",
        "if 'mean' in df_BDados_Temp_Sumarize.columns and 'std' in df_BDados_Temp_Sumarize.columns:\n",
        "    # Criando uma nova coluna 'mean_1.05' com a multiplicação por 1.05\n",
        "    df_BDados_Temp_Sumarize['mean_Max'] = (df_BDados_Temp_Sumarize['mean'] + df_BDados_Temp_Sumarize['std']) * 1.005\n",
        "else:\n",
        "    print(\"As colunas 'mean' e 'std' são necessárias no DataFrame.\")\n",
        "\n",
        "# Exibindo o DataFrame resultante\n",
        "print(df_BDados_Temp_Sumarize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_MeanTempDay.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_MeanTempDay = pd.read_csv(github_url)\n",
        "\n",
        "# Mostrar as primeiras linhas do DataFrame\n",
        "print(df_MeanTempDay.head())"
      ],
      "metadata": {
        "id": "1NwBCzsYqFuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivvsTyWkKHEc"
      },
      "outputs": [],
      "source": [
        "# Plotar o gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='NmeComp', y='mean', data=df_BDados_Temp_Sumarize, color='gray')\n",
        "plt.title('Temperatura Média por Componente')\n",
        "plt.xlabel('Componente')\n",
        "plt.ylabel('Temperatura Média')\n",
        "plt.ylim(0, 75)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.gca().invert_xaxis()  # Inverter a ordem dos componentes\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csiDxRdaJ9HD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plotar o gráfico com barras de erro\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='NmeComp', y='mean', data=df_BDados_Temp_Sumarize, color='gray')\n",
        "plt.errorbar(x=df_BDados_Temp_Sumarize['NmeComp'], y=df_BDados_Temp_Sumarize['mean'], yerr=df_BDados_Temp_Sumarize['std'],\n",
        "             fmt='none', ecolor='black', capsize=5, elinewidth=1.5)\n",
        "plt.title('Temperatura Média por Componente')\n",
        "plt.xlabel('Componente')\n",
        "plt.ylabel('Temperatura Média')\n",
        "plt.ylim(0, 100)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.gca().invert_xaxis()  # Inverter a ordem dos componentes\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnLqdopgQsjZ"
      },
      "outputs": [],
      "source": [
        "# Filtrar os dados para BoomCylinder_LD e BoomCylinder_LE\n",
        "BDados_Temp_BoomCil_LD_LE = BDados_Temp[BDados_Temp['NmeComp'].isin(['BoomCylinder_LD', 'BoomCylinder_LE'])]\n",
        "print(BDados_Temp_BoomCil_LD_LE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbO2qdLZRSZm"
      },
      "source": [
        "##### 3.3.1 Separating data from the BucketCylinder_LD and BucketCylinder_LE components to compare temperature variations over time for both components\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-AQ_0DRRi1w"
      },
      "source": [
        "##### 3.3.3 Separating data from the ShellCylinder_LD and ShellCylinder_LE components to compare temperature variations over time for both components\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eptaho2RqzG"
      },
      "source": [
        "##### 3.3.4 Separating data from the StickCylinder_LD and StickCylinder_LE components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC-ULKBYRwT7"
      },
      "source": [
        "##### 3.3.5 Separating data from the ShellCylinder_LD and ShellCylinder_LD components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-pkLzrORzzI"
      },
      "source": [
        "##### 3.3.6 Separating data from the SwingDrive_LD and SwingDrive_LE components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtKYwXOpR8yD"
      },
      "source": [
        "##### 3.3.7 Separating data from the SwingGear_1 and SwingGear_2 components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFYii6NqpdwY"
      },
      "source": [
        "##### 3.3.7 Separating temperature x time data for all components draining a DF: BDados_Temp_Tran"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCdZ1Ioqpc0l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpUEAshFrQeU"
      },
      "outputs": [],
      "source": [
        "BDados_Temp_Trans = BDados_Temp.pivot(index='Timestamp', columns='NmeComp', values='Value').reset_index()\n",
        "#BDados_Temp_Trans = BDados_Temp_Trans.drop(columns='NmeComp')\n",
        "\n",
        "# Visualizar o DataFrame resultante\n",
        "BDados_Temp_Trans.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_BDados_Temp.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_BDados_Temp = pd.read_csv(github_url)\n",
        "\n",
        "# Exemplo de dados fictícios para ilustração\n",
        "\n",
        "df = pd.DataFrame(df_BDados_Temp)\n",
        "\n",
        "# Converter a coluna Timestamp para datetime, caso não esteja\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "# Arredondar Timestamp para a hora mais próxima\n",
        "df['Timestamp'] = df['Timestamp'].dt.floor('H')\n",
        "\n",
        "# Calcular a média dos valores para cada NmeComp e cada hora\n",
        "result_df = df.groupby(['NmeComp', 'Timestamp'])['Value'].mean().reset_index()\n",
        "\n",
        "print(result_df)"
      ],
      "metadata": {
        "id": "zR4Cfxp-rOlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Supondo que você tenha um DataFrame chamado BDados_Temp_Trans\n",
        "# Substitua isso pelo seu DataFrame real\n",
        "df = BDados_Temp.copy()\n",
        "\n",
        "# Inicializando listas para armazenar os resultados\n",
        "results_list = []\n",
        "\n",
        "# Definindo o tamanho do intervalo\n",
        "interval_size = 100\n",
        "\n",
        "# Iterando sobre as amostras em intervalos de 100\n",
        "for nme_comp, subset in df.groupby('NmeComp'):\n",
        "    for i in range(0, len(subset)-interval_size+1, interval_size):\n",
        "        interval_subset = subset.iloc[i:i+interval_size]\n",
        "\n",
        "        # Calculando a média, desvio padrão e a variância para cada amostra\n",
        "        mean_value = interval_subset['Value'].mean()\n",
        "        std_dev_value = interval_subset['Value'].std()\n",
        "        variance_value = interval_subset['Value'].var()\n",
        "\n",
        "        # Adicionando os resultados à lista\n",
        "        results_list.append({\n",
        "            'NmeComp': nme_comp,\n",
        "            'Timestamp': interval_subset['Timestamp'].iloc[interval_size-1],  # Escolhendo o Timestamp do último elemento em cada intervalo\n",
        "            'Mean': mean_value,\n",
        "            'Std_Dev': std_dev_value,\n",
        "            'Variance': variance_value\n",
        "        })\n",
        "\n",
        "# Criando um DataFrame com os resultados\n",
        "result_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Criando gráficos interativos com plotly express\n",
        "fig = px.line(result_df, x='Timestamp', y=['Mean', 'Std_Dev', 'Variance'], color='NmeComp',\n",
        "              labels={'value': 'Valor', 'Timestamp': 'Timestamp'},\n",
        "              title='Resultados para NmeComp com Mean, Std_Dev e Variance',\n",
        "              line_shape='linear')  # Linear para linhas retas\n",
        "\n",
        "#Adicionando uma linha de tendência à média usando Ordinary Least Squares (OLS)\n",
        "for nme_comp in result_df['NmeComp'].unique():\n",
        "   subset = result_df[result_df['NmeComp'] == nme_comp]\n",
        "   line = go.Scatter(x=subset['Timestamp'], y=subset['Mean'], mode='lines', name=f'Tendência - {nme_comp}')\n",
        "   fig.add_trace(line)\n",
        "\n",
        "# Exibindo o gráfico interativo\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "XEKRpmNfeX9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7z9MmC05p0o"
      },
      "source": [
        "##### 3.3.8 Temperature x time assessment for component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hl9DPMIphb5"
      },
      "source": [
        "###### 3.3.8.1 Separating data from BoomCilindersLD_LE components\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnCt-9KOpYMi"
      },
      "outputs": [],
      "source": [
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransBoomCylinderLD_LE = BDados_Temp_Trans[['BoomCylinder_LD', 'BoomCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_BoomCylinderLD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)BoomCylinder_LD', '(C°)BoomCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para BoomCylinder_LD\n",
        "fig_BoomCylinderLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBoomCylinderLD_LE['Timestamp'], y=BDados_Temp_TransBoomCylinderLD_LE['BoomCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BoomCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para BoomCylinder_LE\n",
        "fig_BoomCylinderLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBoomCylinderLD_LE['Timestamp'], y=BDados_Temp_TransBoomCylinderLD_LE['BoomCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BoomCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_BoomCylinderLD_LE.update_layout(title_text='Temperatura BoomCylinder_LD e BoomCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_BoomCylinderLD_LE.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVkiDRGVpwPp"
      },
      "source": [
        "###### 3.3.8.2 Separating data from StickCilindersLD_LE components\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_BoomCylinder_LD = BDados_Temp_Trans[['BoomCylinder_LD', 'Timestamp']].dropna()\n",
        "df_BoomCylinder_LE = BDados_Temp_Trans[['BoomCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_BoomCylinder_LD['Timestamp'] = pd.to_datetime(df_BoomCylinder_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_BoomCylinder_LE['Timestamp'] = pd.to_datetime(df_BoomCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_BoomCylinder_LD[['Timestamp']]\n",
        "y_LD = df_BoomCylinder_LD['BoomCylinder_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_BoomCylinder_LE[['Timestamp']]\n",
        "y_LE = df_BoomCylinder_LE['BoomCylinder_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BoomCylinder_LE')\n",
        "plt.title('Previsto vs Real para BoomCylinder_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BoomCylinder_LD')\n",
        "plt.title('Previsto vs Real para BoomCylinder_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dT0BIFv3dUF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "df_BoomCylinder_LE = BDados_Temp_Trans[['BoomCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_BoomCylinder_LE['Timestamp'] = pd.to_datetime(df_BoomCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_BoomCylinder_LE.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Calcular ACF e PACF\n",
        "lags = 20  # Ajuste o número de lags conforme necessário\n",
        "acf_values = acf(df_BoomCylinder_LE['BoomCylinder_LE'], nlags=lags)\n",
        "pacf_values = pacf(df_BoomCylinder_LE['BoomCylinder_LE'], nlags=lags)\n",
        "\n",
        "# Criar DataFrame com os resultados\n",
        "BoomCylinder_LE_LAG_ACF_PACF = pd.DataFrame({\n",
        "    'Lag': range(1, lags+1),\n",
        "    'ACF': acf_values[1:],\n",
        "    'PACF': pacf_values[1:]\n",
        "})\n",
        "\n",
        "# Exibir o DataFrame\n",
        "print(BoomCylinder_LE_LAG_ACF_PACF)\n",
        "\n",
        "# Plotar ACF e PACF\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# ACF\n",
        "ax1.stem(BoomCylinder_LE_LAG_ACF_PACF['Lag'], BoomCylinder_LE_LAG_ACF_PACF['ACF'], basefmt=\" \", markerfmt=\"o\", linefmt=\"-\")\n",
        "ax1.set_title('Autocorrelation Function (ACF)')\n",
        "\n",
        "# PACF\n",
        "ax2.stem(BoomCylinder_LE_LAG_ACF_PACF['Lag'], BoomCylinder_LE_LAG_ACF_PACF['PACF'], basefmt=\" \", markerfmt=\"o\", linefmt=\"-\")\n",
        "ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2OE930roxlF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA (AutoRegressive Integrated Moving Average)\n",
        "\n",
        "p (Ordem do componente AR - AutoRegressive): Representa o número de termos autoregressivos no modelo. Esses são lags (atrasos) das observações anteriores, ou seja, quantos períodos anteriores são usados para prever o próximo período.\n",
        "Escolher\n",
        "\n",
        "p envolve analisar a função de autocorrelação (ACF) dos seus dados. Um gráfico ACF pode ajudar a identificar quantos lags são significativos.\n",
        "d (Ordem de diferenciação): Indica quantas vezes os dados são diferenciados. A diferenciação é usada para tornar a série temporal estacionária, o que facilita a modelagem.\n",
        "\n",
        "Se a série temporal já é estacionária, d seria 0. Caso contrário, você pode diferenciar a série uma ou mais vezes até atingir estacionariedade. q (Ordem do componente MA - Moving Average): Refere-se ao número de termos da média móvel no modelo. Os termos de média móvel são erros residuais dos períodos anteriores. Eles representam a média dos erros residuais até aquele ponto no tempo. Assim como p, a escolha de q pode ser baseada na função de autocorrelação dos resíduos (ACF dos resíduos)."
      ],
      "metadata": {
        "id": "LpvNDiKJurdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordem de Diferenciação (d): A função de autocorrelação (ACF) mostra uma queda significativa após o primeiro lag, sugerindo que uma diferenciação de ordem 1(d=1) pode ser suficiente para tornar a série temporal estacionária.\n",
        "\n",
        "Ordem do Componente AR (p): A função de autocorrelação parcial (PACF) mostra uma autocorrelação significativa no primeiro lag e uma queda gradual nos lags subsequentes. Isso sugere que um termo autoregressivo de ordem 1 (p=1) pode ser apropriado.\n",
        "\n",
        "Ordem do Componente MA (q): A função de autocorrelação (ACF) mostra uma autocorrelação significativa nos primeiros lags, indicando a presença de um componente de média móvel. Um termo de média móvel de ordem 1 (q=1) pode ser uma escolha razoável."
      ],
      "metadata": {
        "id": "AKwGOMGm2Bzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "024lQJIK2BwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Selecionei um intervalo de exemplo para fins de ilustração\n",
        "df_BoomCylinder_LE = BDados_Temp_Trans[['BoomCylinder_LE', 'Timestamp']].dropna().head(100)\n",
        "\n",
        "df_BoomCylinder_LE['Timestamp'] = pd.to_datetime(df_BoomCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_BoomCylinder_LE.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Ajustar um modelo ARIMA\n",
        "order = (1, 1, 1)  # Substitua p, d, q pelos valores apropriados\n",
        "model = ARIMA(df_BoomCylinder_LE['BoomCylinder_LE'], order=order)\n",
        "result = model.fit()\n",
        "\n",
        "# Fazer previsões\n",
        "forecast_steps = 10  # Número de passos de previsão\n",
        "forecast = result.get_forecast(steps=forecast_steps)\n",
        "\n",
        "# Plotar os resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_BoomCylinder_LE['BoomCylinder_LE'], label='Histórico')\n",
        "plt.plot(forecast.predicted_mean.index, forecast.predicted_mean, color='red', label='Previsão')\n",
        "plt.fill_between(forecast.predicted_mean.index,\n",
        "                 forecast.conf_int().iloc[:, 0],\n",
        "                 forecast.conf_int().iloc[:, 1], color='gray', alpha=0.2, label='Intervalo de Confiança')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BoomCylinder_LE')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ttU7tBponM4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B5EUlsppyQA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransStickCilindersLD_LE = BDados_Temp_Trans[['StickCylinder_LD', 'StickCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_StickCilindersLD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)StickCylinder_LD', '(C°)StickCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para StickCylinder_LD\n",
        "fig_StickCilindersLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransStickCilindersLD_LE['Timestamp'], y=BDados_Temp_TransStickCilindersLD_LE['StickCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)StickCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para StickCylinder_LE\n",
        "fig_StickCilindersLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransStickCilindersLD_LE['Timestamp'], y=BDados_Temp_TransStickCilindersLD_LE['StickCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)StickCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_StickCilindersLD_LE.update_layout(title_text='Temperatura StickCylinder_LD e StickCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_StickCilindersLD_LE.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_StickCylinder_LD = BDados_Temp_Trans[['StickCylinder_LD', 'Timestamp']].dropna()\n",
        "df_StickCylinder_LE = BDados_Temp_Trans[['StickCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_StickCylinder_LD['Timestamp'] = pd.to_datetime(df_StickCylinder_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_StickCylinder_LE['Timestamp'] = pd.to_datetime(df_StickCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_StickCylinder_LD[['Timestamp']]\n",
        "y_LD = df_StickCylinder_LD['StickCylinder_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_StickCylinder_LE[['Timestamp']]\n",
        "y_LE = df_StickCylinder_LE['StickCylinder_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('StickCylinder_LE')\n",
        "plt.title('Previsto vs Real para StickCylinder_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('StickCylinder_LD')\n",
        "plt.title('Previsto vs Real para StickCylinder_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HESwKGhFfaik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRnyWeD6ql4Z"
      },
      "source": [
        "###### 3.3.8.3 Separating data from FinalDrive_LD_LE components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UbJhbKVr9Yi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransFinalDrive_LD_LE = BDados_Temp_Trans[['FinalDrive_LD', 'FinalDrive_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_FinalDrive_LD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)FinalDrive_LD', '(C°)FinalDrive_LE'])\n",
        "\n",
        "# Adicionar traces para FinalDrive_LD\n",
        "fig_FinalDrive_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransFinalDrive_LD_LE['Timestamp'], y=BDados_Temp_TransFinalDrive_LD_LE['FinalDrive_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)FinalDrive_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para FinalDrive_LE\n",
        "fig_FinalDrive_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransFinalDrive_LD_LE['Timestamp'], y=BDados_Temp_TransFinalDrive_LD_LE['FinalDrive_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)FinalDrive_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_FinalDrive_LD_LE.update_layout(title_text='Temperatura FinalDrive_LD e FinalDrive_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_FinalDrive_LD_LE.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_FinalDrive_LD = BDados_Temp_Trans[['FinalDrive_LD', 'Timestamp']].dropna()\n",
        "df_FinalDrive_LE = BDados_Temp_Trans[['FinalDrive_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_FinalDrive_LD['Timestamp'] = pd.to_datetime(df_FinalDrive_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_FinalDrive_LE['Timestamp'] = pd.to_datetime(df_FinalDrive_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_FinalDrive_LD[['Timestamp']]\n",
        "y_LD = df_FinalDrive_LD['FinalDrive_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_FinalDrive_LE[['Timestamp']]\n",
        "y_LE = df_FinalDrive_LE['FinalDrive_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('FinalDrive_LE')\n",
        "plt.title('Previsto vs Real para FinalDrive_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('FinalDrive_LD')\n",
        "plt.title('Previsto vs Real para FinalDrive_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TuTZOS_aj5hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1nsoJjsIIu"
      },
      "source": [
        "###### 3.3.8.4 Separating data from Shell Cylinder LD/LE components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--mQZTyIsQFM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransShellCylinder_LD_LE = BDados_Temp_Trans[['ShellCylinder_LD', 'ShellCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_ShellCylinder_LD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)ShellCylinder_LD', '(C°)ShellCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para ShellCylinder_LD\n",
        "fig_ShellCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransShellCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransShellCylinder_LD_LE['ShellCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)ShellCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para ShellCylinder_LE\n",
        "fig_ShellCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransShellCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransShellCylinder_LD_LE['ShellCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)ShellCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_ShellCylinder_LD_LE.update_layout(title_text='Temperatura ShellCylinder_LD e ShellCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_ShellCylinder_LD_LE.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_ShellCylinder_LD = BDados_Temp_Trans[['ShellCylinder_LD', 'Timestamp']].dropna()\n",
        "df_ShellCylinder_LE = BDados_Temp_Trans[['ShellCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_ShellCylinder_LD['Timestamp'] = pd.to_datetime(df_FinalDrive_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_ShellCylinder_LE['Timestamp'] = pd.to_datetime(df_FinalDrive_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_ShellCylinder_LD[['Timestamp']]\n",
        "y_LD = df_ShellCylinder_LD['ShellCylinder_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_ShellCylinder_LE[['Timestamp']]\n",
        "y_LE = df_ShellCylinder_LE['ShellCylinder_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('df_ShellCylinder_LE')\n",
        "plt.title('Previsto vs Real para df_ShellCylinder_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('df_ShellCylinder_LD')\n",
        "plt.title('Previsto vs Real para df_ShellCylinder_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6A6tH7CHkbNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni02PQSJsTFx"
      },
      "source": [
        "###### 3.3.8.5 Separating data from PumpDrive_1_2 components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QtiMQkUsa8-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransPumpDrive_1_2 = BDados_Temp_Trans[['PumpDrive_1', 'PumpDrive_2', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_PumpDrive_1_2 = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)PumpDrive_1', '(C°)PumpDrive_2'])\n",
        "\n",
        "# Adicionar traces para PumpDrive_1\n",
        "fig_PumpDrive_1_2.add_trace(go.Scatter(x=BDados_Temp_TransPumpDrive_1_2['Timestamp'], y=BDados_Temp_TransPumpDrive_1_2['PumpDrive_1'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)PumpDrive_1'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para PumpDrive_2\n",
        "fig_PumpDrive_1_2.add_trace(go.Scatter(x=BDados_Temp_TransPumpDrive_1_2['Timestamp'], y=BDados_Temp_TransPumpDrive_1_2['PumpDrive_2'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)PumpDrive_2'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_PumpDrive_1_2.update_layout(title_text='Temperatura PumpDrive_1 e PumpDrive_2 ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_PumpDrive_1_2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_PumpDrive_1 = BDados_Temp_Trans[['PumpDrive_1', 'Timestamp']].dropna()\n",
        "df_PumpDrive_2 = BDados_Temp_Trans[['PumpDrive_2', 'Timestamp']].dropna()\n",
        "\n",
        "df_PumpDrive_1['Timestamp'] = pd.to_datetime(df_PumpDrive_1['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_PumpDrive_2['Timestamp'] = pd.to_datetime(df_PumpDrive_2['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_1 = df_PumpDrive_1[['Timestamp']]\n",
        "y_1 = df_PumpDrive_1['PumpDrive_1']\n",
        "\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_2 = df_PumpDrive_2[['Timestamp']]\n",
        "y_2 = df_PumpDrive_2['PumpDrive_2']\n",
        "\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_2 = StandardScaler()\n",
        "X_train_scaled_2 = scaler_2.fit_transform(X_train_2)\n",
        "X_test_scaled_2 = scaler_2.transform(X_test_2)\n",
        "\n",
        "scaler_1 = StandardScaler()\n",
        "X_train_scaled_1 = scaler_1.fit_transform(X_train_1)\n",
        "X_test_scaled_1 = scaler_1.transform(X_test_1)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_2 = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_2.fit(X_train_scaled_2, y_train_2)\n",
        "y_pred_2 = model_LE.predict(X_test_scaled_2)\n",
        "\n",
        "model_1 = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_1.fit(X_train_scaled_LD, y_train_1)\n",
        "y_pred_1 = model_1.predict(X_test_scaled_1)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_2 = mean_squared_error(y_test_2, y_pred_2)\n",
        "mse_1 = mean_squared_error(y_test_1, y_pred_1)\n",
        "print(f'Mean Squared Error LE: {mse_2}')\n",
        "print(f'Mean Squared Error LD: {mse_1}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_2, y_test_2, label='Real')\n",
        "plt.scatter(X_test_2, y_pred_2, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('PumpDrive_2')\n",
        "plt.title('Previsto vs Real para PumpDrive_2')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_1, y_test_1, label='Real')\n",
        "plt.scatter(X_test_1, y_pred_1, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('PumpDrive_1')\n",
        "plt.title('Previsto vs Real para PumpDrive_1')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Rq6u6vjZk7JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxLyO-qBscSg"
      },
      "source": [
        "###### 3.3.8.6 Separating data from ShellSwingGear_1_2 components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYpAHLDpskJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransShellSwingGear_1_2 = BDados_Temp_Trans[['SwingGear_1', 'SwingGear_2', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_SwingGear_1_2 = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)SwingGear_1', '(C°)SwingGear_2'])\n",
        "\n",
        "# Adicionar traces para SwingGear_1\n",
        "fig_SwingGear_1_2.add_trace(go.Scatter(x=BDados_Temp_TransShellSwingGear_1_2['Timestamp'], y=BDados_Temp_TransShellSwingGear_1_2['SwingGear_1'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)SwingGear_1'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para SwingGear_2\n",
        "fig_SwingGear_1_2.add_trace(go.Scatter(x=BDados_Temp_TransShellSwingGear_1_2['Timestamp'], y=BDados_Temp_TransShellSwingGear_1_2['SwingGear_2'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)SwingGear_2'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_SwingGear_1_2.update_layout(title_text='Temperatura SwingGear_1 e SwingGear_2 ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_SwingGear_1_2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_SwingGear_1 = BDados_Temp_TransShellSwingGear_1_2[['SwingGear_1', 'Timestamp']].dropna()\n",
        "\n",
        "df_SwingGear_1['Timestamp'] = pd.to_datetime(df_SwingGear_1['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_SwingGear_1[['Timestamp']]\n",
        "y = df_SwingGear_1['SwingGear_1']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('SwingGear_1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C35auy4rlh4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_SwingGear_2 = BDados_Temp_TransShellSwingGear_1_2[['SwingGear_2', 'Timestamp']].dropna()\n",
        "\n",
        "df_SwingGear_2['Timestamp'] = pd.to_datetime(df_SwingGear_2['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_SwingGear_2[['Timestamp']]\n",
        "y = df_SwingGear_2['SwingGear_2']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('SwingGear_2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2xn1xzmelxHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOaS77JPslwF"
      },
      "source": [
        "###### 3.3.8.7 Separating data from Bucket Cylinder LD LE components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOoGOYovtC9l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransBucketCylinder_LD_LE = BDados_Temp_Trans[['BucketCylinder_LD', 'BucketCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_BucketCylinder_LD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)BucketCylinder_LD', '(C°)BucketCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para BucketCylinder_LD\n",
        "fig_BucketCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBucketCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransBucketCylinder_LD_LE['BucketCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BucketCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para BucketCylinder_LE\n",
        "fig_BucketCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBucketCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransBucketCylinder_LD_LE['BucketCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BucketCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_BucketCylinder_LD_LE.update_layout(title_text='Temperatura BucketCylinder_LD e BucketCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_BucketCylinder_LD_LE.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_BucketCylinder_LD = BDados_Temp_TransBucketCylinder_LD_LE[['BucketCylinder_LD', 'Timestamp']].dropna()\n",
        "\n",
        "df_BucketCylinder_LD['Timestamp'] = pd.to_datetime(df_BucketCylinder_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_BucketCylinder_LD[['Timestamp']]\n",
        "y = df_BucketCylinder_LD['BucketCylinder_LD']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BucketCylinder_LD')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJZg8QBhl-g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_BucketCylinder_LE = BDados_Temp_TransBucketCylinder_LD_LE[['BucketCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_BucketCylinder_LE['Timestamp'] = pd.to_datetime(df_BucketCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_BucketCylinder_LE[['Timestamp']]\n",
        "y = df_BucketCylinder_LE['BucketCylinder_LE']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BucketCylinder_LE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wToyaKM0mPNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taXs1YwUmaAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFSs7OtxtH8M"
      },
      "source": [
        "\n",
        "\n",
        "## 3.4 Component LIfe Projection\n",
        "\n",
        "[Figure 9.  Component LIfe Projection](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%209.%20%20Component%20LIfe%20Projection%20on%20CAT%206030.png)\n",
        "\n",
        "\n",
        "\n",
        "# 4 Results/benefits\n",
        "\n",
        "Key benefits of implementing the best practice include the following:\n",
        "\n",
        "Improved Machine Availability and Productivity– Components like cylinders that have few typical condition monitoring parameters can be better monitored with additional methods like thermography; replacement plans (parts, scheduled downtime, etc) can be put in place before failure of the component.\n",
        "\n",
        "Additionally, these are direct and indirect benefits:\n",
        "•\tAllows greater agility in temperature assessment (lower MTTR)\n",
        "•\tReduces risks through less time spent in the asset's operating zone (Elimination of live work)\n",
        "•\tProvides greater sensitivity in decision making for asset shutdown and definition of the next cycle time measurement step\n",
        "•\tProvided greater predictability in the cylinder replacement planning and schedule process;\n",
        "o\tWear evaluation increase the accuracy in the replacement forecast;\n",
        "o\tImprovement in material stock planning by using the hours actually operated as a parameter;\n",
        "\n",
        "## R Markdown\n",
        "\n",
        "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R\n",
        "\n",
        "\n",
        "You can also embed plots, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6B4LofGxyjw"
      },
      "outputs": [],
      "source": [
        "#BDadosRNN.columns\n",
        "BDadosRNN_boomcylinder_ld_temperature = BDadosRNN[(BDadosRNN['NmeComp'] == 'BoomCylinder_LD') & (BDadosRNN['Parametro'] == 'Temperature')]\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "print(BDadosRNN_boomcylinder_ld_temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqkvJ5tfz0I4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "DfRnn = BDadosRNN[(BDadosRNN['NmeComp'] == 'BoomCylinder_LD') & (BDadosRNN['Parametro'] == 'Temperature')]\n",
        "\n",
        "DfRnn = DfRnn[['Timestamp', 'Value']].copy()\n",
        "DfRnn['Value_1'] = DfRnn['Value'].shift(1)\n",
        "DfRnn['Value_2'] = DfRnn['Value'].shift(2)\n",
        "DfRnn['Value_3'] = DfRnn['Value'].shift(3)\n",
        "\n",
        "# Remover linhas com NaN resultantes do deslocamento\n",
        "DfRnn = DfRnn.dropna()\n",
        "DfRnn = DfRnn.rename(columns={'Value': 'y', 'Value_1': 'x_1', 'Value_2': 'x_2', 'Value_3': 'x_3'})\n",
        "DfRnn = DfRnn.drop(columns=['Timestamp'])\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "print(DfRnn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k35pFDMuFhh"
      },
      "source": [
        "Normalização dos dados do Data Frame BoomCylinder_LD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIwhB9cUuIqN"
      },
      "outputs": [],
      "source": [
        "# Normalização MinMax\n",
        "def normalize(x):\n",
        "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "\n",
        "# Aplicar a normalização\n",
        "DFRnnNorm = pd.DataFrame({\n",
        "    'Y': normalize(DfRnn['y']),\n",
        "    'X_1': normalize(DfRnn['x_1']),\n",
        "    'X_2': normalize(DfRnn['x_2']),\n",
        "    'X_3': normalize(DfRnn['x_3'])\n",
        "})\n",
        "\n",
        "# Dividir em conjuntos de treino e teste\n",
        "Y_array = DFRnnNorm['Y'].values[:-500].reshape(-1, 1)\n",
        "X_array = DFRnnNorm[['X_1', 'X_2', 'X_3']].values[:-500].reshape(-1, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxWtQWDSuafl"
      },
      "source": [
        "## 4.1 Results Rede Neutal (RNN) e LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fm2OhPDdkUp"
      },
      "source": [
        "### 4.1.1 Results modelo RNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLPf1sWvJ61"
      },
      "source": [
        "Utilização de RNN com o valor target de 1,05 da Feature com o objetivo de testar as Features prevendo uma temperatura de 5,4 Graus maior, tal como descrito na introdução."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6s0BEF0f0_"
      },
      "source": [
        "### 4.1.2 Results modelo LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kyyxmbzRIJh"
      },
      "source": [
        "## 4.3 Results svm_linear, Regressão Linear e MLP (Multilayer Perceptron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99sSnQaeCXeo"
      },
      "source": [
        "MLP (Multilayer Perceptron) com a função de ativação Tangente Hiperbólica (tanh) é uma arquitetura de rede neural artificial que utiliza a função de ativação tangente hiperbólica em suas camadas ocultas. Vamos entender esses termos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_yQrP1oQTA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "\n",
        "# Carregando o DataFrame\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando a variável de destino transformada 'Value * 1.05'\n",
        "df['Target'] = df['Value'] * 1.05\n",
        "\n",
        "# Separando features e target\n",
        "X = df[['Value']]\n",
        "y = df['Target']\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializando os modelos\n",
        "svm_linear = SVR(kernel='linear')\n",
        "linear_reg = LinearRegression()\n",
        "mlp_tanh = MLPRegressor(activation='tanh', random_state=42)"
      ],
      "metadata": {
        "id": "lNsadvcdNXNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27TT7IYtiuS4"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "\n",
        "# Inicializando os modelos\n",
        "svm_linear = SVR(kernel='linear')\n",
        "\n",
        "# Treinando os modelos\n",
        "svm_linear.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Fazendo previsões nos conjuntos de teste\n",
        "svm_linear_predictions = svm_linear.predict(X_test)\n",
        "\n",
        "# Calculando o MAE para cada modelo\n",
        "svm_linear_mae = mean_absolute_error(y_test, svm_linear_predictions)\n",
        "\n",
        "# Exibindo resultados\n",
        "print(f'MAE para SVM com kernel linear: {svm_linear_mae}')\n",
        "\n",
        "# Criando um DataFrame para armazenar os resultados\n",
        "resultados_df_svm_linear = pd.DataFrame({\n",
        "    'Modelo': ['MLP com ativação linear'],\n",
        "    'MAE': [svm_linear_mae]\n",
        "})\n",
        "\n",
        "# Exibindo resultados\n",
        "print(resultados_df_svm_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIStZjKgi1Et"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Inicializando os modelos\n",
        "linear_reg = LinearRegression()\n",
        "\n",
        "# Treinando os modelos\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões nos conjuntos de teste\n",
        "linear_reg_predictions = linear_reg.predict(X_test)\n",
        "\n",
        "# Calculando o MAE para cada modelo\n",
        "linear_reg_mae = mean_absolute_error(y_test, linear_reg_predictions)\n",
        "\n",
        "# Exibindo resultados\n",
        "print(f'MAE para Regressão Linear: {linear_reg_mae}')\n",
        "\n",
        "\n",
        "# Criando um DataFrame para armazenar os resultados\n",
        "resultados_df_linear_reg_mae = pd.DataFrame({\n",
        "    'Modelo': ['MAE para Regressão Linear'],\n",
        "    'MAE': [linear_reg_mae]\n",
        "})\n",
        "\n",
        "# Exibindo resultados\n",
        "print(resultados_df_linear_reg_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJpRvZPji_61"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "# Inicializando o modelo MLP com ativação tanh\n",
        "mlp_tanh = MLPRegressor(hidden_layer_sizes=(50,), activation='tanh', max_iter=50)\n",
        "\n",
        "# Treinando o modelo\n",
        "mlp_tanh.fit(X_train.reshape((X_train.shape[0], X_train.shape[1])), y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "mlp_tanh_predictions = mlp_tanh.predict(X_test.reshape((X_test.shape[0], X_test.shape[1])))\n",
        "\n",
        "# Calculando o MAE para o modelo MLP com ativação tanh\n",
        "mlp_tanh_mae = mean_absolute_error(y_test, mlp_tanh_predictions)\n",
        "\n",
        "# Exibindo resultados\n",
        "print(f'MAE para SVM com kernel linear: {mlp_tanh_mae}')\n",
        "\n",
        "# Criando um DataFrame para armazenar os resultados\n",
        "resultados_df_mlp_tanh = pd.DataFrame({\n",
        "    'Modelo': ['MLP com ativação tanh'],\n",
        "    'MAE': [mlp_tanh_mae]\n",
        "})\n",
        "\n",
        "# Exibindo resultados\n",
        "print(resultados_df_mlp_tanh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponha que você tenha os seguintes DataFrames\n",
        "resultados_df_svm_linear = pd.DataFrame({'Modelo': ['SVM Linear'], 'Resultado MAE': [0.123]})\n",
        "resultados_df_lstm_mae = pd.DataFrame({'Modelo': ['LSTM'], 'Resultado MAE': [0.456]})\n",
        "resultados_df_linear_reg_mae = pd.DataFrame({'Modelo': ['Linear Regression'], 'Resultado MAE': [0.789]})\n",
        "resultados_df_mlp_tanh = pd.DataFrame({'Modelo': ['MLP Tanh'], 'Resultado MAE': [0.101]})\n",
        "\n",
        "# Lista de DataFrames\n",
        "dfs = [resultados_df_svm_linear, resultados_df_lstm_mae, resultados_df_linear_reg_mae, resultados_df_mlp_tanh]\n",
        "\n",
        "# Juntar DataFrames\n",
        "df_final = pd.concat(dfs, ignore_index=True)\n",
        "print(df_final)"
      ],
      "metadata": {
        "id": "I2KGEEZagHbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste modelo foi feito teste de RNN para os dados de temperatura do componente Boom Cylinder_ld somente, a fim de avaliar os resultados da RNN com 3 x features defasadas de uma linha.\n",
        "\n",
        "Resultado do MAE Médio (Cross Valid) = 0.12236133102496058"
      ],
      "metadata": {
        "id": "JzDK-4KqIvFP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Z0P8keQ5TE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Supondo que você já tenha seus dados e o DataFrame é chamado df\n",
        "# Certifique-se de ter a coluna 'Timestamp' como datetime e 'Value' como o alvo\n",
        "# Carregando DataFrame\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['Target1'] = df['Value'].shift(1)\n",
        "df['Target2'] = df['Value'].shift(2)\n",
        "df['Target3'] = df['Value'].shift(3)\n",
        "\n",
        "# Removendo linhas com NaN resultantes das defasagens\n",
        "df = df.dropna()\n",
        "\n",
        "# Separando features e target\n",
        "X = df[['Value']]\n",
        "y = df[['Target1', 'Target2', 'Target3']]\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Criando função para criar modelo RNN\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(50, activation='relu', input_shape=(input_shape, 1)))\n",
        "    model.add(Dense(3))  # 3 saídas para as três colunas alvo\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Classe wrapper para usar KerasRegressor com Scikit-Learn\n",
        "class KerasRNN(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y, epochs=15, batch_size=16):\n",
        "        self.model = create_rnn_model(self.input_shape)\n",
        "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# Criando o modelo KerasRegressor para Scikit-Learn\n",
        "rnn_model = make_pipeline(StandardScaler(), KerasRNN(input_shape=X_scaled.shape[1]))\n",
        "\n",
        "# Avaliação usando validação cruzada com TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mae_scores = -cross_val_score(rnn_model, X_scaled, y_scaled, cv=tscv, scoring=mae_scorer)\n",
        "\n",
        "# Exibindo resultados da validação cruzada\n",
        "print('MAE Médio (Cross Valid):', np.mean(mae_scores))\n",
        "\n",
        "# Treinando o modelo com todos os dados para visualização\n",
        "rnn_model.fit(X_scaled, y_scaled)\n",
        "\n",
        "# Função para plotar previsões\n",
        "def plot_rnn_predictions(model, X, y, timestamps, title):\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Desfazer a normalização\n",
        "    predictions = scaler_y.inverse_transform(predictions)\n",
        "    y = scaler_y.inverse_transform(y)\n",
        "\n",
        "# Plotar previsões para RNN com 3 features\n",
        "plot_rnn_predictions(rnn_model, X_scaled, y_scaled, df['Timestamp'], 'Previsões RNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo foi elaborado para testar os dados de temperatura do cilindro LD, com o modelo de LSTM (Long Short-Term Memory) com os seguintes parametros:\n",
        "\n",
        "a) epochs=15,\n",
        "\n",
        "b) batch_size=16\n",
        "\n",
        "c) activation='relu'\n",
        "\n",
        "Obteve resultado de MAE Médio (Cross Valid) = 0.12031637800939518\n"
      ],
      "metadata": {
        "id": "P9I1h2IMKzXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dHM6777XixR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm  # Importa a função tqdm para mostrar a barra de progresso\n",
        "\n",
        "# Supondo que você já tenha seus dados e o DataFrame é chamado df\n",
        "# Certifique-se de ter a coluna 'Timestamp' como datetime e 'Value' como o alvo\n",
        "# Carregando DataFrame\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['Target1'] = df['Value'].shift(1)\n",
        "df['Target2'] = df['Value'].shift(2)\n",
        "df['Target3'] = df['Value'].shift(3)\n",
        "\n",
        "# Removendo linhas com NaN resultantes das defasagens\n",
        "df = df.dropna()\n",
        "\n",
        "# Separando features e target\n",
        "X = df[['Value']]\n",
        "y = df[['Target1', 'Target2', 'Target3']]\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Lista de funções de ativação a serem testadas\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "\n",
        "# Dicionário para armazenar resultados de MAE para cada função de ativação\n",
        "mae_results_lstm_3target = {\n",
        "    'MAE': [],\n",
        "    'MSE': [],\n",
        "    'RMSE': [],\n",
        "    'R²': [],\n",
        "    'Nome_DF': [],\n",
        "    'Activation_Function': [],\n",
        "}\n",
        "\n",
        "# Loop sobre as funções de ativação\n",
        "for activation_function in activation_functions:\n",
        "    # Criando e treinando o modelo para a função de ativação atual\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(50, activation=activation_function, input_shape=(X_scaled.shape[1], 1)))\n",
        "    lstm_model.add(Dense(3))  # 3 saídas para as três colunas alvo\n",
        "    lstm_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "    # Criando o modelo KerasRegressor para Scikit-Learn\n",
        "    lstm_model_pipeline = make_pipeline(StandardScaler(), lstm_model)\n",
        "\n",
        "    # Avaliação usando validação cruzada com TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "    mae_scores = -cross_val_score(lstm_model_pipeline, X_scaled, y_scaled, cv=tscv, scoring=mae_scorer)\n",
        "\n",
        "    # Calculando as métricas médias\n",
        "    mae_mean = np.mean(mae_scores)\n",
        "\n",
        "    # Calculando MSE manualmente\n",
        "    mse_mean = np.mean(np.square(-mae_scores))\n",
        "\n",
        "    # Calculando RMSE\n",
        "    rmse_mean = np.sqrt(mse_mean)\n",
        "\n",
        "    # Calculando R²\n",
        "    r2_mean = r2_score(y_scaled, lstm_model_pipeline.fit(X_scaled, y_scaled).predict(X_scaled))\n",
        "\n",
        "    # Armazenando os resultados no dicionário\n",
        "    mae_results_lstm_3target['MAE'].append(mae_mean)\n",
        "    mae_results_lstm_3target['MSE'].append(mse_mean)\n",
        "    mae_results_lstm_3target['RMSE'].append(rmse_mean)\n",
        "    mae_results_lstm_3target['R²'].append(r2_mean)\n",
        "    mae_results_lstm_3target['Activation_Function'].append(activation_function)\n",
        "    mae_results_lstm_3target['Nome_DF'].append('df_results_LSTM_3Targ')\n",
        " #   mae_results_lstm_3target[activation_function] = {'Activation_Function': activation_function}\n",
        "\n",
        "\n",
        "# Convertendo o dicionário em DataFrame\n",
        "df_results_LSTM_3Targ = pd.DataFrame(mae_results_lstm_3target)\n",
        "# Adicionando uma coluna com o nome do DataFrame\n",
        "#df_results_LSTM_3Targ['Nome_DF'] = 'df_results_LSTM_3Targ'\n",
        "\n",
        "# Exibindo os resultados\n",
        "print(df_results_LSTM_3Targ)\n",
        "\n",
        "# Salvar o DataFrame em um arquivo CSV\n",
        "df_results_LSTM_3Targ.to_csv('df_results_LSTM_3Targ.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kMLBuvAoqAv"
      },
      "source": [
        "## 4.4 Results / Comparação das MAEs para modelos de RNN com funções de ativação distintos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeJpmEtZ2vd"
      },
      "source": [
        "O modelo a seguir, foi elaborado para testar os dados de temperatura do cilindro LD, com o modelo de RNN utilizando dados sequenciais de 80 amostras a cada envio, agora comparando o resultado da MAE variando a função de ativação conforme definiso nos parametros:\n",
        "\n",
        "a) epochs=50,\n",
        "\n",
        "b) batch_size=32\n",
        "\n",
        "c) activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "d) optimizer='adam',\n",
        "\n",
        "e) loss='mean_squared_error\n",
        "\n",
        "Modelo de RNN com amostras de 80 linhas, e com varias funções de ativação com comparação das MAE\n",
        "\n",
        "1) MAE para relu: 0.6380209729715359\n",
        "\n",
        "2) MAE para tanh: 0.5595733862566802\n",
        "\n",
        "3) MAE para sigmoid: 0.7872871740610322\n",
        "\n",
        "4) MAE para linear: 0.5702095221302993\n",
        "\n",
        "Como otimizador foi utilizado o otimizador 'adam' que é frequentemente usado por padrão, pois tem um bom desempenho em muitos casos (Inserir Referencia)\n",
        "\n",
        "SGD (Gradiente Descendente Estocástico): optimizer = 'sgd'\n",
        "\n",
        "RMSprop (Média Móvel do Quadrado dos Gradientes): optimizer = 'rmsprop'\n",
        "\n",
        "Adagrad (Gradiente Adaptativo): optimizer = 'adagrad'\n",
        "\n",
        "Adadelta (Adaptive Delta): optimizer = 'adadelta'\n",
        "\n",
        "Nadam (Nesterov-acelerado Adaptive Moment Estimation):optimizer = 'nadam'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QddRImRwbpm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm  # Importa a função tqdm para mostrar a barra de progresso\n",
        "\n",
        "# Define Data Frame para a temperatura do Cilindro do Boon LD\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['target'] = df['Value']\n",
        "\n",
        "# Extraindo os valores da coluna alvo\n",
        "target_values = df['target'].values\n",
        "\n",
        "# Normalizando os dados para o intervalo [0, 1]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target_values = scaler.fit_transform(target_values.reshape(-1, 1))\n",
        "\n",
        "# Criando sequências de 80 em 80\n",
        "sequence_length = 80\n",
        "sequences = [target_values[i:i + sequence_length] for i in range(len(target_values) - sequence_length)]\n",
        "\n",
        "# Convertendo para array numpy\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Separando em conjuntos de treinamento e teste\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data, test_data = sequences[:train_size], sequences[train_size:]\n",
        "\n",
        "# Separando as entradas (X) e saídas (y)\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "# Reshape dos dados para o formato de entrada da RNN (batch_size, timesteps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Lista de funções de ativação a serem testadas\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "# Dicionário para armazenar resultados de MAE para cada função de ativação\n",
        "metrics_results_RNN80Sample = {}\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    # Criando e treinando o modelo para a função de ativação atual\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=50, activation=activation_function, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Fazendo previsões\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Invertendo a normalização para obter os valores reais\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Calculando as métricas\n",
        "    mae = mean_absolute_error(y_test_original, predictions)\n",
        "    mse = mean_squared_error(y_test_original, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test_original, predictions)\n",
        "\n",
        "    # Armazenando as métricas no dicionário\n",
        "    metrics_results_RNN80Sample[activation_function] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R²': r2, 'Activation_Function': activation_function}\n",
        "\n",
        "\n",
        "# Criar um DataFrame com os resultados de MAE\n",
        "df_results_RNN80Sample = pd.DataFrame(metrics_results_RNN80Sample).transpose()\n",
        "\n",
        "# Adicionando uma coluna com o nome do DataFrame\n",
        "df_results_RNN80Sample['Nome_DF'] = 'df_results_RNN80Sample'\n",
        "\n",
        "# Exibir a tabela comparativa\n",
        "print(df_results_RNN80Sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHoZ2mnPaF8I"
      },
      "source": [
        "Agora com LSTM e comparação entre funções de ativação RELU, LINEAR, MPL e Tangente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X5y1kgwaPgi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy.stats import weightedtau  # Importar a função weightedtau da biblioteca scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # Importa a função tqdm para mostrar a barra de progresso\n",
        "\n",
        "# Define Data Frame para a temperatura do Cilindro do Boon LD\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['target'] = df['Value']\n",
        "\n",
        "# Extraindo os valores da coluna alvo\n",
        "target_values = df['target'].values\n",
        "\n",
        "# Normalizando os dados para o intervalo [0, 1]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target_values = scaler.fit_transform(target_values.reshape(-1, 1))\n",
        "\n",
        "# Criando sequências de 80 em 80\n",
        "sequence_length = 80\n",
        "sequences = [target_values[i:i + sequence_length] for i in range(len(target_values) - sequence_length)]\n",
        "\n",
        "# Convertendo para array numpy\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Separando em conjuntos de treinamento e teste\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data, test_data = sequences[:train_size], sequences[train_size:]\n",
        "\n",
        "# Separando as entradas (X) e saídas (y)\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "# Reshape dos dados para o formato de entrada da LSTM (batch_size, timesteps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Lista de funções de ativação a serem testadas\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "# Dicionário para armazenar resultados de MAE para cada função de ativação\n",
        "metrics_results_LSTM80Sample = {}\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    # Criando e treinando o modelo para a função de ativação atual\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, activation=activation_function, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Fazendo previsões\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    mae = mean_absolute_error(y_test_original, predictions)\n",
        "    mse = mean_squared_error(y_test_original, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test_original, predictions)\n",
        "\n",
        "    # Armazenando as métricas no dicionário\n",
        "    metrics_results_LSTM80Sample[activation_function] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R²': r2, 'Activation_Function': activation_function}\n",
        "\n",
        "# Criar um DataFrame com os resultados\n",
        "df_results_LSTM80Sample = pd.DataFrame(metrics_results_LSTM80Sample).transpose()\n",
        "\n",
        "# Adicionando uma coluna com o nome do DataFrame\n",
        "df_results_LSTM80Sample['Nome_DF'] = 'df_results_LSTM80Sample'\n",
        "\n",
        "# Exibir a tabela comparativa\n",
        "print(df_results_LSTM80Sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sugestão de outros parametros estatísticos\n",
        "\n",
        "***MSE (Erro Quadrático Médio):***\n",
        "\n",
        "O MSE é outra métrica de erro que calcula a média dos quadrados das diferenças entre os valores previstos e os valores reais. Pode ser mais sensível a grandes erros do que o MAE.\n",
        "\n",
        "***RMSE (Raiz do Erro Quadrático Médio):***\n",
        "\n",
        "O RMSE é a raiz quadrada do MSE. Ele fornece uma medida do erro na mesma unidade que a variável de destino, facilitando a interpretação.\n",
        "\n",
        "***R² (Coeficiente de Determinação):***\n",
        "\n",
        "O R² é uma métrica que varia de 0 a 1 e indica a proporção da variância na variável dependente que é previsível a partir da variável independente. Um R² próximo de 1 indica um bom ajuste do modelo.\n",
        "\n",
        "***MAPE (Erro Percentual Absoluto Médio):***\n",
        "\n",
        "O MAPE é uma métrica de erro percentual que calcula a média das percentagens absolutas de erro entre os valores previstos e reais. É útil quando os dados têm diferentes escalas ou magnitudes.\n",
        "\n",
        "***Índice de Concordância (em problemas de regressão):***\n",
        "\n",
        "O índice de concordância avalia o quão bem as previsões de um modelo se alinham com os valores reais."
      ],
      "metadata": {
        "id": "XiF86tRQ1u0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "from github import Github\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "# Concatenando os DataFrames ao longo das colunas\n",
        "df_combined = pd.concat([df_results_RNN80Sample, df_results_LSTM80Sample, df_results_LSTM_3Targ], ignore_index=True)\n",
        "\n",
        "\n",
        "# Autenticação no GitHub\n",
        "token = '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'\n",
        "\n",
        "g = Github(token)\n",
        "\n",
        "#TokenUSP_TCC — admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, admin:ssh_signing_key, audit_log, codespace, copilot, delete:packages, delete_repo, gist, notifications, project, repo, user, workflow, write:discussion, write:packages\n",
        "#Expires on Fri, May 17 2024.\n",
        "\n",
        "\n",
        "# Repositório\n",
        "usuario = 'CidClayQuirino'\n",
        "repositorio = 'rnn-component-lIfe-cycle'\n",
        "\n",
        "# Nome do arquivo\n",
        "nome_arquivo = 'df_combined.csv'\n",
        "# Salvar DataFrame como CSV em um BytesIO\n",
        "csv_bytes = BytesIO()\n",
        "df_combined.to_csv(csv_bytes, index=False)\n",
        "\n",
        "# Obter o repositório\n",
        "repo = g.get_user(usuario).get_repo(repositorio)\n",
        "\n",
        "# Caminho local para salvar temporariamente o arquivo\n",
        "caminho_local = 'df_combined.csv'\n",
        "\n",
        "# Salvar o arquivo localmente\n",
        "with open(caminho_local, 'wb') as file:\n",
        "    file.write(csv_bytes.getvalue())\n",
        "\n",
        "# Criar ou atualizar o arquivo no repositório\n",
        "try:\n",
        "    arquivo = repo.get_contents(nome_arquivo)\n",
        "    repo.update_file(nome_arquivo, 'Atualizando resultados', open(caminho_local, 'rb').read(), arquivo.sha)\n",
        "    print(f'{nome_arquivo} atualizado com sucesso!')\n",
        "except Exception as e:\n",
        "    repo.create_file(nome_arquivo, 'Adicionando resultados', open(caminho_local, 'rb').read())\n",
        "    print(f'{nome_arquivo} criado com sucesso!')\n",
        "df_combined.head()"
      ],
      "metadata": {
        "id": "jNuSKt5ZWLKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código pode ser otimizado para melhorar o desempenho. Aqui estão algumas sugestões:\n",
        "\n",
        "Amostragem de Dados: Em vez de usar todos os dados para treinar o modelo SVM, você pode considerar a amostragem dos dados, especialmente se o conjunto de dados for grande. Isso pode acelerar o treinamento e ainda fornecer uma boa aproximação.\n",
        "\n",
        "Grid Search para Otimização de Parâmetros: Utilize Grid Search para encontrar os melhores parâmetros para o modelo SVM. O Grid Search testa diferentes combinações de parâmetros e retorna os melhores. Isso pode ajudar a melhorar o desempenho do modelo.\n",
        "\n",
        "Aqui está uma versão modificada do código com essas sugestões:"
      ],
      "metadata": {
        "id": "WCUOX_QNdBPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora otimização do código para projetar os dados com base nas últimas 1000 amostras das features (Value_1, Value_2, Value_3). Além disso, vamos remover o loop e utilizar a função numpy para realizar as previsões de uma vez. Aqui está o código otimizado:\n",
        "\n",
        "Detalhamento:# Definindo os parâmetros para Grid Search\n",
        "\n",
        "'kernel': ['linear']: Aqui, estamos especificando que queremos usar um kernel linear para o SVR. O kernel linear é adequado quando há uma relação linear entre as características de entrada e a variável de destino.\n",
        "\n",
        "'C': [0.1, 1, 10]: O parâmetro C é um hiperparâmetro de regularização no SVR. Ele controla o equilíbrio entre ter um ajuste suave (evitar sobreajuste) e ajustar aos dados de treinamento. Valores menores de C indicam um modelo mais suave, enquanto valores maiores permitem um ajuste mais preciso aos dados de treinamento. Aqui, estamos testando três valores diferentes para C: 0.1, 1 e 10.\n",
        "\n",
        "O Grid Search é uma técnica que busca os melhores hiperparâmetros em uma grade predefinida. Ele treina o modelo para cada combinação possível de valores de hiperparâmetros e avalia o desempenho usando uma métrica especificada (neste caso, o negativo da média do erro absoluto). O conjunto de hiperparâmetros que resulta no melhor desempenho de acordo com a métrica escolhida é então escolhido como o modelo final.\n",
        "\n",
        "O Grid Search é uma maneira sistemática de ajustar os hiperparâmetros de um modelo para otimizar seu desempenho. Neste caso, estamos usando o Grid Search para encontrar os melhores parâmetros para o SVR com kernel linear e o parâmetro de regularização C."
      ],
      "metadata": {
        "id": "jFAD8KDf09eV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora com projeção até atingir 5% a mais da temperatura da Feature"
      ],
      "metadata": {
        "id": "rC4xzQxG1kPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "código para identificar variações de aumento na temperatura (Value) ao longo do tempo (Timestamp) e apresentar taxas de variações acima de 1% para cada conjunto de 1000 amostras:"
      ],
      "metadata": {
        "id": "l5KpeMiiWqrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Supondo que você já tenha um DataFrame chamado BDados_Temp com as colunas NmeComp, Timestamp e Value\n",
        "BDados_Temp = pd.DataFrame(BDados_Temp)\n",
        "\n",
        "# Convertendo a coluna Timestamp para o formato datetime\n",
        "BDados_Temp['Timestamp'] = pd.to_datetime(BDados_Temp['Timestamp'])\n",
        "\n",
        "# Ordenando o DataFrame pela coluna Timestamp\n",
        "BDados_Temp = BDados_Temp.sort_values(by='Timestamp')\n",
        "\n",
        "# Calculando a média móvel de 5 horas\n",
        "BDados_Temp['Moving_Avg_5H'] = BDados_Temp.groupby('NmeComp')['Value'].rolling(window=5).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "# Inicialização do Dash\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Layout do aplicativo\n",
        "app.layout = html.Div([\n",
        "    dcc.Dropdown(\n",
        "        id='NmeComp-dropdown',\n",
        "        options=[{'label': NmeComp, 'value': NmeComp} for NmeComp in BDados_Temp['NmeComp'].unique()],\n",
        "        value=BDados_Temp['NmeComp'].unique()[0],\n",
        "        multi=False\n",
        "    ),\n",
        "    dcc.Checklist(\n",
        "        id='hide-points-checkbox',\n",
        "        options=[\n",
        "            {'label': 'Ocultar Pontos de Dados', 'value': 'hide_points'}\n",
        "        ],\n",
        "        value=[]\n",
        "    ),\n",
        "    dcc.Graph(id='line-plot'),\n",
        "])\n",
        "\n",
        "# Callback para atualizar o gráfico com base na seleção do dropdown e na opção de ocultar pontos de dados\n",
        "@app.callback(\n",
        "    Output('line-plot', 'figure'),\n",
        "    [Input('NmeComp-dropdown', 'value'),\n",
        "     Input('hide-points-checkbox', 'value')]\n",
        ")\n",
        "def update_graph(selected_NmeComp, hide_points_option):\n",
        "    # Filtrando os dados com base no NmeComp selecionado\n",
        "    filtered_data = BDados_Temp[BDados_Temp['NmeComp'] == selected_NmeComp]\n",
        "\n",
        "    fig = px.line(filtered_data, x='Timestamp', y='Value', title=f'Série Temporal - NmeComp {selected_NmeComp}',\n",
        "                  labels={'Value': 'Valor', 'Timestamp': 'Timestamp'})\n",
        "\n",
        "    # Adicionando a média móvel de 5 horas\n",
        "    fig.add_scatter(x=filtered_data['Timestamp'], y=filtered_data['Moving_Avg_5H'], mode='lines', name='Média Móvel 5H')\n",
        "\n",
        "    # Adicionando a linha de tendência usando a regressão linear\n",
        "    X = sm.add_constant(filtered_data['Timestamp'].astype(int) // 10**9)  # Convertendo Timestamp para segundos\n",
        "    model = sm.OLS(filtered_data['Value'], X)\n",
        "    results = model.fit()\n",
        "    trendline = results.fittedvalues\n",
        "    fig.add_trace(go.Scatter(x=filtered_data['Timestamp'], y=trendline, mode='lines', name='Linha de Tendência'))\n",
        "\n",
        "    # Ocultando os pontos de dados, se a opção 'Ocultar Pontos de Dados' estiver selecionada\n",
        "    if 'hide_points' in hide_points_option:\n",
        "        fig.update_traces(mode='lines')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Executando o aplicativo\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "8iwxg3h9mxsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_BDados_Temp.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_BDados_Temp = pd.read_csv(github_url)\n",
        "\n",
        "# Exemplo de dados fictícios para ilustração\n",
        "df_BDados_Temp_Hora = pd.DataFrame(df_BDados_Temp)\n",
        "\n",
        "# Converter a coluna Timestamp para datetime, caso não esteja\n",
        "df_BDados_Temp_Hora['Timestamp'] = pd.to_datetime(df_BDados_Temp_Hora['Timestamp'])  # Correção aqui\n",
        "\n",
        "# Arredondar Timestamp para a hora mais próxima\n",
        "df_BDados_Temp_Hora['Timestamp'] = df_BDados_Temp_Hora['Timestamp'].dt.floor('H')  # Correção aqui\n",
        "\n",
        "# Calcular a média dos valores para cada NmeComp e cada hora\n",
        "df_BDados_Temp_Hora = df_BDados_Temp_Hora.groupby(['NmeComp', 'Timestamp'])['Value'].mean().reset_index()  # Correção aqui\n",
        "\n",
        "print(df_BDados_Temp_Hora)\n"
      ],
      "metadata": {
        "id": "MmRwuWvbtsSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supondo que você tenha um DataFrame chamado result_df com colunas NmeComp, Timestamp e Value\n",
        "# Substitua isso pelo seu DataFrame real\n",
        "\n",
        "df_BDados_Temp_Hora['Timestamp'] = pd.to_datetime(df_BDados_Temp_Hora['Timestamp'])\n",
        "\n",
        "# Calcular a média por hora para cada NmeComp\n",
        "df_BDados_Temp_Hora['Hour'] = df_BDados_Temp_Hora['Timestamp'].dt.hour\n",
        "mean_values = df_BDados_Temp_Hora.groupby(['NmeComp', 'Hour'])['Value'].mean().reset_index()\n",
        "\n",
        "# Plotar gráficos de dispersão separados para cada NmeComp com linha de tendência\n",
        "g = sns.FacetGrid(mean_values, col='NmeComp', col_wrap=4, height=4, sharey=False)\n",
        "g.map(sns.scatterplot, 'Hour', 'Value', marker='o', color='blue')\n",
        "g.map(sns.regplot, 'Hour', 'Value', scatter=False, color='red', ci=None)\n",
        "g.set_axis_labels('Hour', 'Mean Value')\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fNLr5usotxnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "# Converter a coluna Timestamp para datetime, caso não esteja\n",
        "df_BDados_Temp_Hora['Timestamp'] = pd.to_datetime(df_BDados_Temp_Hora['Timestamp'])\n",
        "\n",
        "# Calcular a média por dia para cada NmeComp\n",
        "df_BDados_Temp_Dia = df_BDados_Temp_Hora.copy()\n",
        "df_BDados_Temp_Dia['Day'] = df_BDados_Temp_Dia['Timestamp'].dt.date\n",
        "mean_values_daily = df_BDados_Temp_Dia.groupby(['NmeComp', 'Day'])['Value'].mean().reset_index()\n",
        "\n",
        "# Converter as datas para formato numérico\n",
        "mean_values_daily['Day'] = mdates.date2num(mean_values_daily['Day'])\n",
        "\n",
        "# Plotar gráficos de dispersão separados para cada NmeComp com linha de tendência\n",
        "g = sns.FacetGrid(mean_values_daily, col='NmeComp', col_wrap=4, height=4, sharey=False)\n",
        "g.map(sns.scatterplot, 'Day', 'Value', marker='o', color='blue')\n",
        "g.map(sns.regplot, 'Day', 'Value', scatter=False, color='red', ci=None)\n",
        "g.set_axis_labels('Day', 'Mean Value')\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "g.set_xticklabels(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KlUp2mPDBTqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from github import Github\n",
        "from io import BytesIO\n",
        "\n",
        "# Defina suas credenciais do GitHub\n",
        "seu_token = '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'\n",
        "seu_usuario = 'CidClayQuirino'\n",
        "seu_repositorio = 'rnn-component-lIfe-cycle'\n",
        "\n",
        "#TokenUSP_TCC — admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, admin:ssh_signing_key, audit_log, codespace, copilot, delete:packages, delete_repo, gist, notifications, project, repo, user, workflow, write:discussion, write:packages\n",
        "#Expires on Fri, May 17 2024.\n",
        "\n",
        "# Dicionário de DataFrames com seus nomes originais\n",
        "dataframes = {\n",
        "    'df_BDados_Temp_Dia': df_BDados_Temp_Dia,\n",
        "    'df_BDados_Temp_Hora': df_BDados_Temp_Hora,\n",
        "    'BDados_Temp_MM5H': BDados_Temp_MM5H,\n",
        "    'df_combined': df_combined,\n",
        "    'DfRnn': DfRnn,\n",
        "    'BDadosRNN_boomcylinder_ld_temperature': BDadosRNN_boomcylinder_ld_temperature,\n",
        "    'result_df': result_df,\n",
        "    'BDados_Temp': BDados_Temp,\n",
        "    'BDados_Temp_Trans': BDados_Temp_Trans,\n",
        "    'df_MeanTempDay': df_MeanTempDay,\n",
        "    'BDadosRNN': BDadosRNN\n",
        "    \"BoomCylinder_LE_LAG_ACF_PACF\": BoomCylinder_LE_LAG_ACF_PACF\n",
        "}\n",
        "\n",
        "# Função para salvar e enviar para o GitHub\n",
        "def salvar_e_enviar_para_github(dataframe, nome_arquivo, usuario, repositorio, token):\n",
        "    # Salvar DataFrame como CSV em um BytesIO\n",
        "    csv_bytes = BytesIO()\n",
        "    dataframe.to_csv(csv_bytes, index=False)\n",
        "\n",
        "    # Autenticar no GitHub\n",
        "    g = Github(token)\n",
        "\n",
        "    # Obter o repositório\n",
        "    repo = g.get_user(usuario).get_repo(repositorio)\n",
        "\n",
        "    # Criar ou atualizar o arquivo no repositório\n",
        "    try:\n",
        "        arquivo = repo.get_contents(nome_arquivo)\n",
        "        repo.update_file(nome_arquivo, f'Atualizando {nome_arquivo}', csv_bytes.getvalue(), arquivo.sha)\n",
        "        print(f'{nome_arquivo} atualizado com sucesso!')\n",
        "    except Exception as e:\n",
        "        repo.create_file(nome_arquivo, f'Adicionando {nome_arquivo}', csv_bytes.getvalue())\n",
        "        print(f'{nome_arquivo} criado com sucesso!')\n",
        "\n",
        "# Iterar sobre os DataFrames e salvá-los no GitHub\n",
        "for nome, df in dataframes.items():\n",
        "    nome_arquivo = f'{nome}.csv'  # Nome do arquivo usando o nome original do DataFrame\n",
        "    salvar_e_enviar_para_github(df, nome_arquivo, seu_usuario, seu_repositorio, seu_token)\n"
      ],
      "metadata": {
        "id": "jT_aoAcbRtit"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}