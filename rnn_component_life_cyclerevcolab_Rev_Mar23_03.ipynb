{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/rnn_component_life_cyclerevcolab_Rev_Mar23_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oauv_Ubh_2H"
      },
      "source": [
        "#Title: \"Aprimorando o monitoramento das condições em equipamentos de mineração: uma abordagem abrangente para manutenção proativa e análise preditiva\n",
        "Author: \"Cid Clay Quirino\"\n",
        "Date: \"2024-01-25\"**\n",
        "\n",
        "\n",
        "Instalaçao de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO-qYFKDdQEl"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnNynpDka56Y"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLjFpBYUFaL6"
      },
      "outputs": [],
      "source": [
        "!pip install markdown\n",
        "!pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install PyGithub\n",
        "!pip install gitpython\n",
        "!pip install statsmodels\n",
        "!pip install dash"
      ],
      "metadata": {
        "id": "FmeqzDVNHsHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_YyXAwShuZN"
      },
      "outputs": [],
      "source": [
        "# Atualizar pacotes\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from IPython.display import Image\n",
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHszVB194n8u"
      },
      "source": [
        "# Sumário Executivo\n",
        "\n",
        "Métodos destinados a aprimorar as inspeções em Máquinas Operatrizes são reconhecidos como uma abordagem inovadora para aprimorar a segurança e confiabilidade dos equipamentos industriais. Essa prática está alinhada com as demandas da Indústria 4.0, sendo percebida como uma solução inteligente, conectada, de acessibilidade avançada, e mais adaptável e autônoma. A implementação desses métodos envolve técnicas preditivas que fazem uso de sensores para coletar parâmetros em tempo real desses equipamentos e seus componentes, gerando uma grande quantidade de dados sobre o comportamento do ativo ao longo de sua operação. Este artigo apresenta um estudo de caso focado no monitoramento e inspeção baseados na Internet das Coisas (IoT) para um processo de inspeção preditiva de pequenos componentes em um equipamento de mineração. A solução proposta visa fornecer dados de temperatura e vibração dos pequenos componentes, disponibilizando essas informações na nuvem. Foi desenvolvida uma camada de análise desses dados, e uma arquitetura de monitoramento baseada em IoT foi implementada para acompanhar esses componentes. Os dados são atualizados e interpretados usando Machine Learning para permitir o monitoramento, integração da operação e análise, identificando padrões de comportamento que ultrapassam os limites ideais. Esta abordagem de monitoramento e inspeção demonstra alta precisão na detecção de variações nas condições dos componentes, possibilitando a tomada de decisões autônomas em casos de anormalidades graves. O estudo de caso validou que a abordagem anterior, que utilizava a temperatura como parâmetro decisório com câmera termográfica, apresentava bom desempenho, mas enfrentava desafios relacionados ao tempo médio de inspeção, especialmente devido à dificuldade de acesso e ao tempo de parada da máquina.\n",
        "Na abordagem com IoT, a frequência de coleta de dados foi eliminada, introduzindo um novo desafio relacionado à análise e tomada de decisão baseadas na grande quantidade de dados fornecidos pelos sensores. Com base nos resultados de ambas as abordagens, conclui-se que as tarefas podem ser otimizadas, permitindo aos clientes e fabricantes direcionar esforços para uma abordagem futurista e economicamente viável.\n",
        "Palavras-chave: Monitoramento de condições, manutenção preditiva, aprendizado de máquina, gerenciamento de ativos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Abstract\n",
        "Title: Enhancing Condition Monitoring in Mining Equipment: A Comprehensive Approach to Proactive Maintenance and Predictive Analysis\n",
        "\n",
        "Methods aimed at improving inspections in Machine Tools are currently recognized as a new way to enhance the safety and reliability of large-scale equipment used in production. This is also connected to the needs of Industry 4.0 as it is seen as an intelligent, well-connected, highly accessible, and more adaptable and autonomous solution. The implementation of these methods involves predictive techniques utilizing sensors to collect real-time parameters from these equipment and their components, generating a vast amount of data on the asset's behavior throughout its application. This article presents a case study on monitoring and inspection based on the Internet of Things (IoT) for a predictive inspection process of small components installed in mining equipment. The proposed solution aims to provide temperature and vibration data of the small components, making these data available in the cloud. A layer of data analysis was developed, and an IoT-based monitoring architecture was implemented for tracking these components. The data is updated and interpreted using Machine Learning to enable monitoring, operation integration, and analysis, ultimately identifying patterns of behavior considered beyond the ideal limits. This monitoring and inspection approach achieves high precision in detecting variations in component conditions, allowing for autonomous decision-making in case of severe abnormalities. The case study validated that the previous monitoring approach, which used temperature as a decision parameter with thermographic cameras, performed well but faced a fundamental evolution point concerning the average inspection time, specifically considering the difficulty of access and machine downtime for quality inspections. In the IoT-based inspection approach, the data collection frequency was eliminated, introducing a new challenge related to data analysis and decision-making based on the quantity of data provided by the sensors. Based on the results of both approaches, it is confirmed that tasks can be designed to support customers and manufacturers in directing efforts toward a futuristic and cost-effective approach.\n",
        "Keywords: Condition Monitoring, Predictive Maintenance, Machine Learning, Asset Management\n",
        "\n"
      ],
      "metadata": {
        "id": "F3s2jsUW_vkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1.0 Introdução\n",
        "\n"
      ],
      "metadata": {
        "id": "Tm3hL0sY_09I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.1.\tContextualização\n",
        "\n",
        "Nos últimos anos, avanços no monitoramento de equipamentos de grande porte são apresentados por diversas industriais, com objetivos de reduzir a utilização de seres humanos devido aos riscos inerentes, dificuldade de acesso e redução na parada dos equipamentos, com aponta Tian, Y. M., Gao, F., & Wu, P. (1992) relatando a intensidade de trabalho dos equipamentos mecânicos continua a aumentar, e a eficiência da produção e o nível de automação estão cada vez mais elevados.\n",
        "Por outro lado, equipamentos de grande porte apresentam dificuldades inerentes, como relatado por Schmidt and Berns (2013) a manutenção e inspeção de grandes estruturas com sistemas autônomos ainda é um problema sem solução, eles avaliaram e apresentaram diversas possíveis abordagem com a utilização de robôs para automatizar as inspeções aumentar a qualidade e confiabilidade.\n",
        "Para Park C, Moon D, et al (2016), a manutenção preditiva está atraindo mais interesse do que a manutenção de rotina, que é realizada quando ocorre uma falha na máquina. As técnicas de manutenção preditiva ajudam a determinar a condição dos equipamentos ou sistemas em serviço, a fim de prever quando a manutenção deve ser realizada. A manutenção preditiva permite o agendamento conveniente de ações corretivas e evita paradas inesperadas do equipamento. A chave é a informação certa no momento certo. Ao saber quais equipamentos ou componentes precisam de manutenção, os trabalhos de manutenção podem ser mais bem planejados, e o que seriam paradas não planejadas são transformadas em paradas mais curtas e menos planejadas, aumentando assim a disponibilidade do equipamento. Esta abordagem geralmente utiliza princípios de técnicas estatísticas de controle de processos para determinar em que ponto das futuras atividades de manutenção serão apropriadas. Para avaliar a condição do equipamento, a manutenção preditiva utiliza testes não destrutivos usando sensores, vibração, análise de nível sonoro e outros testes em tempo real.\n",
        "Além disso, Gbadamosi et al (2021) avalia que algumas abordagens atuais exigem o envio de inspetores para áreas de alto risco, para realizar verificações de rotina, o que representa riscos para a saúde e a segurança dos trabalhadores, sendo que quando temos um monitoramento eficiente dos ativos com métodos inovadores de coleta, processamento e análise de dados para obter informações oportunas sobre a condição presente e possível futura, esse risco pode ser eliminado.\n",
        "Quanto aos ativos utilizados em aplicações de mineração, e possível identificar diferentes níveis de monitoramento, ou seja, para os componentes essenciais denominados Grandes Componentes, tais como: Motores de combustão interna, Transmissão e Comandos Finais, possuem níveis elevados de monitoramento e registro de dados pelo sistema eletrônico embarcado. Por outro lado, para componentes menores, como cilindros, bombas e motores hidráulicos, são poucas as alternativas de cobertura de pontos de monitoramento remoto, e quando tem, são de maneira indireta, ou seja, coleta de temperatura do sistema como um todo, e não do componente em específico, o que dificulta a tomada de decisão.\n",
        "Isso se apresenta como uma oportunidade identificada neste estudo que pretende avançar sobre o processo de Gerenciamento de Monitoramento de Condições (CMMS) para esses componentes menores.\n",
        "Logo, este estudo aplicou sensores de temperatura e vibração nos pequenos componentes, com o objetivo de coletar parâmetros e identificar mudanças no comportamento desses parâmetros ao longo de uma série temporal, objetivando identificar variação nos padrões e definir por uma intervenção antes da ocorrência de falhas.\n",
        "Utilizando um caso real e uma abordagem menor tecnológica, este estudo prévio obteve resultados promissores em termos de monitoramento preditivo utilizando câmera termográfica, será explicado em detalhes a seguir neste estudo. Contudo, neste artigo o objetivo foi de expandir a metodologia, empregando a Internet das Coisas (IoT) para obter dados diretamente dos componentes, possibilitando a detecção precoce de alterações de temperatura e vibração.\n",
        "Posteriormente, em uma camada de Data Science, a análise dos dados foi feita utilizando Modelos estatísticos e técnicas de Machine Learning para identificar padrões de comportamento visando detectar condições consideradas anômalas. Essa abordagem proporcionou uma maior eficácia na antecipação de falhas, demonstrando o potencial do CMMS e da integração de tecnologias avançadas para otimizar o monitoramento de condições em componentes críticos de equipamentos de mineração."
      ],
      "metadata": {
        "id": "0bEWGpNuAUaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.2.\tjustificativa\n",
        "\n",
        "A justificativa deste estudo, está nas dificuldades encontradas para se obter parâmetros preditivos para o monitoramento das condições em pequenos componentes em equipamentos moveis, e com isso, melhorar a previsibilidade de intervenção antes das falhas, redução nos custos e redução no tempo de inatividade das máquinas.\n",
        "Com base nessa dificuldade, foi elaborado um projeto de sensoriamento com IOT, para pequenos componentes em uma escavadeira hidráulica de mineração e aplicado em campo em uma situação real de operação de mina de céu aberto.\n"
      ],
      "metadata": {
        "id": "Kg7eT3Ml_9El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.3.\tObjetivo\n",
        "\n",
        "Desenvolvimento de alternativas viáveis para coleta, análise de projeção de saúde em pequenos componentes aplicados em ativos moeis de mineração, tendo como principal objetivo a redução de falhas prematura, menor tempo de inatividade, atendendo também os requisitos de segurança, com a menor exposição aos riscos humanos para a inspeção."
      ],
      "metadata": {
        "id": "G1QRq-3x_-99"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EIYypJ8FsWj"
      },
      "source": [
        "# 2.\tReferencial teórico\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGOXzrk4zHZ"
      },
      "source": [
        "# 3.\tMetodologia\n",
        "\n",
        "Inicialmente, e como comentado acima, este estudo foi aplicado sem quaisquer usos de tecnologia embarcada, e com a utilização de um inspetor e uma câmera tecnográfica.\n",
        "Este procedimento operacional requerido anteriormente, demandava uma série de atividades obrigatórias para a compreensão de algum desvio nos parâmetros, sendo seguidas todas as etapas de avaliação de desempenho seguindo as diretrizes do fabricante tais como:\n",
        "=> Tempo de descida e subida do cilindro,\n",
        "=> Pressão hidráulica durante os testes,\n",
        "=> Taxa de fluxo das bombas hidráulicas.\n",
        "\n",
        "Imagem 1. Escavadeira na frente de operação sendo inspecionada pela equipe preditiva\n",
        "\n",
        "[Image 1.  Fluid Temperature](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%201.%20%20Fluid%20Temperature.png)\n",
        "\n",
        "Somente após estes pontos estarem dentro do parâmetro especificação, o teste de temperatura por termografia foi conduzido, em paralelo aos testes de tempo de ciclo e pressão.\n",
        "Utilizando uma câmera termográfica para registrar o diferencial de temperatura dos cilindros eram feitas medições nas temperaturas, procurando identificar possíveis diferencial térmico foi entre os cilindros, neste caso, e conforme ilustrado na Figura 4, foi possível identificar um diferencial de temperatura de 4,7°C em relação ao lado esquerdo.\n",
        "\n",
        "\n",
        "\n",
        "[Image 2. Imagem Escavadeira Hidraulico Antes Falha](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%202.%20Position%20of%20the%20drop%20test%20equipment.png)\n",
        "\n",
        "Os pontos mais quentes dos cilindros foram capturados pela câmera, revelando uma diferença de temperatura de ~5°C entre o cilindro de elevação do lado direito (l/d) e o cilindro de elevação do lado esquerdo (l/e) (Imagem 5).\n",
        "\n",
        "\n",
        "[Image 3 - Pump Pressure](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%203.%20Pump%20Pressure.png)\n",
        "\n",
        "Após remover e desmontar o cilindro na oficina de reparos, foi possível confirmar que os sintomas observados no campo foram importantes para definir e concluir que o método termográfico pode ser usado com mais segurança para determinar a necessidade de remoção, conforme mostra a imagem 6 a seguir. Isso resultou em uma redução, embora não significativa que iremos explicar mais a seguir, nos custos de reparo e no impacto na contaminação do sistema hidráulico.\n",
        "\n",
        "[Image 4  - Pump Pressure Test](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%204%20%20-%20Pump%20Pressure%20Test.png)\n",
        "\n",
        "As imagens 7a, 7b e 7c fornecem detalhes adicionais após a desmontagem e análise do cilindro, sendo que o modo de falha apresentado na Imagem 7 ilustra a causa do aumento de temperatura no processo termográfico de campo.\n",
        "\n",
        "[Image 5a. Thermography on the Escavator Lift Cylinders](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%205a.%20Thermography%20on%20the%20EH3201%20Lift%20Cylinders.png)\n",
        "\n",
        "\n",
        "[Image 5b. Escavator on the operation front being inspected by predictive team](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%205b.%20EH3201%20on%20the%20operation%20front%20being%20inspected%20by%20predictive%20team.png)\n",
        "\n",
        "Contudo, o que foi possível entender é que mesmo tendo sido avaliado antecipadamente, a frequência em que foram realizadas as inspeções e coletas em campo permitiram um avanço da falha até um nível que o sistema já havia sido contaminado, apesar de ter sido removido antes de uma falha catastrófica, tal abordagem permitiu avanço nos significativos na parte interna do componente.\n",
        "Após os resultados desta abordagem utilizando fluxo anterior, e consequentemente com base na conclusão após desmontagem e reforma, o principal questionamento a ser respondido foi, qual seria a forma de avançar neste processo de coleta e análise dos dados de temperatura a tempo de remover o componente, sem que ele possa estar em um estado de degradação avançado?\n",
        "Conforme apresentado a seguir na Imagem 8, foi identificado possíveis causas desse avanço na falha potencial e sugerido ações para mitigar tais anormalidades.\n",
        "\n",
        "\n",
        "\n",
        "[Image 6.  Thermographic image indicating a 5° c difference between the hottest points of the boom lift cylinders](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%206.%20%20Thermographic%20image%20indicating%20a%205%C2%B0%20c%20difference%20between%20the%20hottest%20points%20of%20the%20boom%20lift%20cylinders.png)\n",
        "\n",
        "Consequentemente, uma proposta de solução foi requerida, e esquematizada na imagem 9 e Imagem 10, com sendo um aprimoramento no processo de monitoramento, a fim de estruturar uma análise robusta, com uso de tecnologia de sensoriamento online e métodos estatísticos e suporte decisório mais sólido.\n",
        "\n",
        "[Image 8.  Cyclinder at the CRC](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%208.%20%20Cyclinder%20at%20the%20CRC.png)\n",
        "\n",
        "A prática anterior, realizada como parte dos esforços de aprimorar os processos de monitoramento e solução de problemas em pequenos componentes, foi um passo fundamental para que fosse possível ver identificar que, a análise de temperatura tem potencial de antever as falhas. No entanto, e como ressaltado anteriormente, o avanço proposto neste estudo ampliou essa aplicação, utilizando tecnologias de IOTs disponíveis no mercado e Algoritmos de Machine Leanning e Deep Learning como ferramentas de apoio a decisão.\n",
        "A Imagem 11 ilustra essa nova aplicação em uma escavadeira de mineração de grande porte.\n",
        "\n",
        "\n",
        "[Image 8a.  Images of disassemlbed cylinder at CRC 1](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%208.%20%20Cyclinder%20at%20the%20CRC.png)\n",
        "\n",
        "[Image 8b. Images of disassemlbed cylinder at CRC 2 ](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%209.%20%20Images%20of%20disassemlbed%20cylinder%20at%20CRC%201.png)\n",
        "\n",
        "\n",
        "[Image 8c. Images of disassemlbed cylinder at CRC 3 Image](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%209.%20%20Images%20of%20disassemlbed%20cylinder%20at%20CRC%203.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIQXksKn5KWA"
      },
      "source": [
        "\n",
        "# 3.1 New Application Using Deep Learning and Online Data Colection\n",
        "\n",
        "\n",
        "The improvement defined for this monitoring process, is a New Application, using technologies available on the market to increasing the monitoring frequency and anticipating changes in the behavior of these components usind a Machine Leaning process, see the Image 9.\n",
        "\n",
        "Image 9. New application on Major Mining Excavator\n",
        "\n",
        "Image 9. New application on Major Mining Excavator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duajl_fA46Lz"
      },
      "source": [
        "\n",
        "## 3.1 Project esquematic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWfacXrx5CrL"
      },
      "source": [
        "\n",
        "## 3.2 Implementation Steps\n",
        "\n",
        "Image 7. Project Data Collection on CAT 6030\n",
        "\n",
        "[Image 7. Project Data Collection on New Machine](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%207.%20%20Project%20Datra%20Collection%20on%20CAT%206030.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdwDzUMvnSl0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# URL do repositório no GitHub\n",
        "repo_url = 'https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/archive/main.zip'\n",
        "dataframes = []\n",
        "\n",
        "# Baixe e extraia o arquivo zip do repositório\n",
        "response = requests.get(repo_url)\n",
        "with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "    zip_file.extractall()\n",
        "\n",
        "# Diretório onde os arquivos .xlsx foram extraídos\n",
        "extracted_dir = 'rnn-component-lIfe-cycle-main'\n",
        "\n",
        "# Loop pelos arquivos no diretório extraído\n",
        "for arquivo in os.listdir(extracted_dir):\n",
        "    if arquivo.endswith('.xlsx'):\n",
        "        # Construa o caminho completo para o arquivo\n",
        "        caminho_completo = os.path.join(extracted_dir, arquivo)\n",
        "\n",
        "        # Leia o arquivo Excel e adicione-o à lista de DataFrames\n",
        "        df = pd.read_excel(caminho_completo)\n",
        "\n",
        "        # Adicione uma coluna 'TagComp' contendo o nome do arquivo sem a extensão\n",
        "        df['nome_arquivo'] = os.path.splitext(arquivo)[0]\n",
        "\n",
        "        # Adicione o DataFrame à lista\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatene todos os DataFrames em um único DataFrame\n",
        "BDadosRNN = pd.concat(dataframes, ignore_index=True)\n",
        "BDadosRNN = BDadosRNN[(BDadosRNN != 0).all(axis=1)]\n",
        "BDadosRNN['Value'] = BDadosRNN['Value'].round(1)\n",
        "BDadosRNN = BDadosRNN.rename(columns={'Tag': 'Parametro'})\n",
        "BDadosRNN = BDadosRNN.rename(columns={'nome_arquivo': 'NmeComp'})\n",
        "#BDadosRNN = BDadosRNN.drop('timestamp', axis=1)\n",
        "BDadosRNN.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nM5ZIvmilk8"
      },
      "source": [
        "\n",
        "## 3.3 Exploration Data Analisys\n",
        "\n",
        "\n",
        "\n",
        "[Figure 8.  Exploration Data Analisys on new model](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%208.%20%20Exploration%20Data%20Analisys%20on%20CAT%206030.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb0Lqv2NIOq8"
      },
      "source": [
        "Video para instalação do Python Virtual Enviromental\n",
        "https://www.youtube.com/watch?v=UPaN3Z49myw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fts2d0FUis9t"
      },
      "outputs": [],
      "source": [
        "# Filtrar os dados\n",
        "\n",
        "BDados_Temp = BDadosRNN[BDadosRNN['Parametro'] == 'Temperature'][['NmeComp', 'Timestamp', 'Value']]\n",
        "BDados_Temp.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supondo que você tenha um DataFrame chamado df com as colunas necessárias\n",
        "df = BDados_Temp\n",
        "# Lista de componentes para os boxplots\n",
        "componentes = [\"BoomCylinder_LD\", \"BoomCylinder_LE\", \"ShellCylinder_LD\", \"ShellCylinder_LE\", \"BucketCylinder_LD\",\n",
        "               \"BucketCylinder_LE\", \"PumpDrive_1\", \"PumpDrive_2\", \"StickCylinder_LD\", \"StickCylinder_LE\",\n",
        "               \"SwingDrive_LD\", \"SwingDrive_LE\", \"SwingGear_1\", \"SwingGear_2\", \"FinalDrive_LD\", \"FinalDrive_LE\"]\n",
        "\n",
        "# Criando subplots para os boxplots\n",
        "fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n",
        "\n",
        "# Iterando sobre os componentes e criando boxplots para cada um\n",
        "for i, componente in enumerate(componentes):\n",
        "    row = i // 4\n",
        "    col = i % 4\n",
        "    sns.boxplot(x='NmeComp', y='Value', data=df[df['NmeComp'].str.contains(componente)], ax=axs[row, col])\n",
        "    axs[row, col].set_title(f'Boxplot para {componente}')\n",
        "    axs[row, col].set_xlabel('NmeComp')\n",
        "    axs[row, col].set_ylabel('Value')\n",
        "\n",
        "# Ajustando o layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v3c8dIthQM8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_HssZC3npDJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_BDados_Temp_Sumarize.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_BDados_Temp_Sumarize = pd.read_csv(github_url)\n",
        "\n",
        "# Verificando se as colunas 'mean' e 'std' existem no DataFrame\n",
        "if 'mean' in df_BDados_Temp_Sumarize.columns and 'std' in df_BDados_Temp_Sumarize.columns:\n",
        "    # Criando uma nova coluna 'mean_1.05' com a multiplicação por 1.05\n",
        "    df_BDados_Temp_Sumarize['mean_Max'] = (df_BDados_Temp_Sumarize['mean'] + df_BDados_Temp_Sumarize['std']) * 1.005\n",
        "else:\n",
        "    print(\"As colunas 'mean' e 'std' são necessárias no DataFrame.\")\n",
        "\n",
        "# Exibindo o DataFrame resultante\n",
        "print(df_BDados_Temp_Sumarize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_MeanTempDay.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_MeanTempDay = pd.read_csv(github_url)\n",
        "\n",
        "# Mostrar as primeiras linhas do DataFrame\n",
        "print(df_MeanTempDay.head())"
      ],
      "metadata": {
        "id": "1NwBCzsYqFuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivvsTyWkKHEc"
      },
      "outputs": [],
      "source": [
        "# Plotar o gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='NmeComp', y='mean', data=df_BDados_Temp_Sumarize, color='gray')\n",
        "plt.title('Temperatura Média por Componente')\n",
        "plt.xlabel('Componente')\n",
        "plt.ylabel('Temperatura Média')\n",
        "plt.ylim(0, 75)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.gca().invert_xaxis()  # Inverter a ordem dos componentes\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csiDxRdaJ9HD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plotar o gráfico com barras de erro\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='NmeComp', y='mean', data=df_BDados_Temp_Sumarize, color='gray')\n",
        "plt.errorbar(x=df_BDados_Temp_Sumarize['NmeComp'], y=df_BDados_Temp_Sumarize['mean'], yerr=df_BDados_Temp_Sumarize['std'],\n",
        "             fmt='none', ecolor='black', capsize=5, elinewidth=1.5)\n",
        "plt.title('Temperatura Média por Componente')\n",
        "plt.xlabel('Componente')\n",
        "plt.ylabel('Temperatura Média')\n",
        "plt.ylim(0, 100)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.gca().invert_xaxis()  # Inverter a ordem dos componentes\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnLqdopgQsjZ"
      },
      "outputs": [],
      "source": [
        "# Filtrar os dados para BoomCylinder_LD e BoomCylinder_LE\n",
        "BDados_Temp_BoomCil_LD_LE = BDados_Temp[BDados_Temp['NmeComp'].isin(['BoomCylinder_LD', 'BoomCylinder_LE'])]\n",
        "print(BDados_Temp_BoomCil_LD_LE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbO2qdLZRSZm"
      },
      "source": [
        "##### 3.3.1 Separating data from the BucketCylinder_LD and BucketCylinder_LE components to compare temperature variations over time for both components\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-AQ_0DRRi1w"
      },
      "source": [
        "##### 3.3.3 Separating data from the ShellCylinder_LD and ShellCylinder_LE components to compare temperature variations over time for both components\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eptaho2RqzG"
      },
      "source": [
        "##### 3.3.4 Separating data from the StickCylinder_LD and StickCylinder_LE components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC-ULKBYRwT7"
      },
      "source": [
        "##### 3.3.5 Separating data from the ShellCylinder_LD and ShellCylinder_LD components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-pkLzrORzzI"
      },
      "source": [
        "##### 3.3.6 Separating data from the SwingDrive_LD and SwingDrive_LE components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtKYwXOpR8yD"
      },
      "source": [
        "##### 3.3.7 Separating data from the SwingGear_1 and SwingGear_2 components to compare temperature variations over time for both components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFYii6NqpdwY"
      },
      "source": [
        "##### 3.3.7 Separating temperature x time data for all components draining a DF: BDados_Temp_Tran"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCdZ1Ioqpc0l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpUEAshFrQeU"
      },
      "outputs": [],
      "source": [
        "BDados_Temp_Trans = BDados_Temp.pivot(index='Timestamp', columns='NmeComp', values='Value').reset_index()\n",
        "#BDados_Temp_Trans = BDados_Temp_Trans.drop(columns='NmeComp')\n",
        "\n",
        "# Visualizar o DataFrame resultante\n",
        "BDados_Temp_Trans.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_BDados_Temp.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_BDados_Temp = pd.read_csv(github_url)\n",
        "\n",
        "# Exemplo de dados fictícios para ilustração\n",
        "\n",
        "df = pd.DataFrame(df_BDados_Temp)\n",
        "\n",
        "# Converter a coluna Timestamp para datetime, caso não esteja\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "# Arredondar Timestamp para a hora mais próxima\n",
        "df['Timestamp'] = df['Timestamp'].dt.floor('H')\n",
        "\n",
        "# Calcular a média dos valores para cada NmeComp e cada hora\n",
        "result_df = df.groupby(['NmeComp', 'Timestamp'])['Value'].mean().reset_index()\n",
        "\n",
        "print(result_df)"
      ],
      "metadata": {
        "id": "zR4Cfxp-rOlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Supondo que você tenha um DataFrame chamado BDados_Temp_Trans\n",
        "# Substitua isso pelo seu DataFrame real\n",
        "df = BDados_Temp.copy()\n",
        "\n",
        "# Inicializando listas para armazenar os resultados\n",
        "results_list = []\n",
        "\n",
        "# Definindo o tamanho do intervalo\n",
        "interval_size = 100\n",
        "\n",
        "# Iterando sobre as amostras em intervalos de 100\n",
        "for nme_comp, subset in df.groupby('NmeComp'):\n",
        "    for i in range(0, len(subset)-interval_size+1, interval_size):\n",
        "        interval_subset = subset.iloc[i:i+interval_size]\n",
        "\n",
        "        # Calculando a média, desvio padrão e a variância para cada amostra\n",
        "        mean_value = interval_subset['Value'].mean()\n",
        "        std_dev_value = interval_subset['Value'].std()\n",
        "        variance_value = interval_subset['Value'].var()\n",
        "\n",
        "        # Adicionando os resultados à lista\n",
        "        results_list.append({\n",
        "            'NmeComp': nme_comp,\n",
        "            'Timestamp': interval_subset['Timestamp'].iloc[interval_size-1],  # Escolhendo o Timestamp do último elemento em cada intervalo\n",
        "            'Mean': mean_value,\n",
        "            'Std_Dev': std_dev_value,\n",
        "            'Variance': variance_value\n",
        "        })\n",
        "\n",
        "# Criando um DataFrame com os resultados\n",
        "result_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Criando gráficos interativos com plotly express\n",
        "fig = px.line(result_df, x='Timestamp', y=['Mean', 'Std_Dev', 'Variance'], color='NmeComp',\n",
        "              labels={'value': 'Valor', 'Timestamp': 'Timestamp'},\n",
        "              title='Resultados para NmeComp com Mean, Std_Dev e Variance',\n",
        "              line_shape='linear')  # Linear para linhas retas\n",
        "\n",
        "#Adicionando uma linha de tendência à média usando Ordinary Least Squares (OLS)\n",
        "for nme_comp in result_df['NmeComp'].unique():\n",
        "   subset = result_df[result_df['NmeComp'] == nme_comp]\n",
        "   line = go.Scatter(x=subset['Timestamp'], y=subset['Mean'], mode='lines', name=f'Tendência - {nme_comp}')\n",
        "   fig.add_trace(line)\n",
        "\n",
        "# Exibindo o gráfico interativo\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "XEKRpmNfeX9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7z9MmC05p0o"
      },
      "source": [
        "##### 3.3.8 Temperature x time assessment for component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hl9DPMIphb5"
      },
      "source": [
        "###### 3.3.8.1 Separating data from BoomCilindersLD_LE components\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnCt-9KOpYMi"
      },
      "outputs": [],
      "source": [
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransBoomCylinderLD_LE = BDados_Temp_Trans[['BoomCylinder_LD', 'BoomCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_BoomCylinderLD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)BoomCylinder_LD', '(C°)BoomCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para BoomCylinder_LD\n",
        "fig_BoomCylinderLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBoomCylinderLD_LE['Timestamp'], y=BDados_Temp_TransBoomCylinderLD_LE['BoomCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BoomCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para BoomCylinder_LE\n",
        "fig_BoomCylinderLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBoomCylinderLD_LE['Timestamp'], y=BDados_Temp_TransBoomCylinderLD_LE['BoomCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BoomCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_BoomCylinderLD_LE.update_layout(title_text='Temperatura BoomCylinder_LD e BoomCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_BoomCylinderLD_LE.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVkiDRGVpwPp"
      },
      "source": [
        "###### 3.3.8.2 Separating data from StickCilindersLD_LE components\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_BoomCylinder_LD = BDados_Temp_Trans[['BoomCylinder_LD', 'Timestamp']].dropna()\n",
        "df_BoomCylinder_LE = BDados_Temp_Trans[['BoomCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_BoomCylinder_LD['Timestamp'] = pd.to_datetime(df_BoomCylinder_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_BoomCylinder_LE['Timestamp'] = pd.to_datetime(df_BoomCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_BoomCylinder_LD[['Timestamp']]\n",
        "y_LD = df_BoomCylinder_LD['BoomCylinder_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_BoomCylinder_LE[['Timestamp']]\n",
        "y_LE = df_BoomCylinder_LE['BoomCylinder_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BoomCylinder_LE')\n",
        "plt.title('Previsto vs Real para BoomCylinder_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BoomCylinder_LD')\n",
        "plt.title('Previsto vs Real para BoomCylinder_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dT0BIFv3dUF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "\n",
        "df_BoomCylinder_LE = BDados_Temp_Trans[['BoomCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_BoomCylinder_LE['Timestamp'] = pd.to_datetime(df_BoomCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_BoomCylinder_LE.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Calcular ACF e PACF\n",
        "lags = 20  # Ajuste o número de lags conforme necessário\n",
        "acf_values = acf(df_BoomCylinder_LE['BoomCylinder_LE'], nlags=lags)\n",
        "pacf_values = pacf(df_BoomCylinder_LE['BoomCylinder_LE'], nlags=lags)\n",
        "\n",
        "# Criar DataFrame com os resultados\n",
        "BoomCylinder_LE_LAG_ACF_PACF = pd.DataFrame({\n",
        "    'Lag': range(1, lags+1),\n",
        "    'ACF': acf_values[1:],\n",
        "    'PACF': pacf_values[1:]\n",
        "})\n",
        "\n",
        "# Exibir o DataFrame\n",
        "print(BoomCylinder_LE_LAG_ACF_PACF)\n",
        "\n",
        "# Plotar ACF e PACF\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# ACF\n",
        "ax1.stem(BoomCylinder_LE_LAG_ACF_PACF['Lag'], BoomCylinder_LE_LAG_ACF_PACF['ACF'], basefmt=\" \", markerfmt=\"o\", linefmt=\"-\")\n",
        "ax1.set_title('Autocorrelation Function (ACF)')\n",
        "\n",
        "# PACF\n",
        "ax2.stem(BoomCylinder_LE_LAG_ACF_PACF['Lag'], BoomCylinder_LE_LAG_ACF_PACF['PACF'], basefmt=\" \", markerfmt=\"o\", linefmt=\"-\")\n",
        "ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2OE930roxlF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA (AutoRegressive Integrated Moving Average)\n",
        "\n",
        "p (Ordem do componente AR - AutoRegressive): Representa o número de termos autoregressivos no modelo. Esses são lags (atrasos) das observações anteriores, ou seja, quantos períodos anteriores são usados para prever o próximo período.\n",
        "Escolher\n",
        "\n",
        "p envolve analisar a função de autocorrelação (ACF) dos seus dados. Um gráfico ACF pode ajudar a identificar quantos lags são significativos.\n",
        "d (Ordem de diferenciação): Indica quantas vezes os dados são diferenciados. A diferenciação é usada para tornar a série temporal estacionária, o que facilita a modelagem.\n",
        "\n",
        "Se a série temporal já é estacionária, d seria 0. Caso contrário, você pode diferenciar a série uma ou mais vezes até atingir estacionariedade. q (Ordem do componente MA - Moving Average): Refere-se ao número de termos da média móvel no modelo. Os termos de média móvel são erros residuais dos períodos anteriores. Eles representam a média dos erros residuais até aquele ponto no tempo. Assim como p, a escolha de q pode ser baseada na função de autocorrelação dos resíduos (ACF dos resíduos)."
      ],
      "metadata": {
        "id": "LpvNDiKJurdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordem de Diferenciação (d): A função de autocorrelação (ACF) mostra uma queda significativa após o primeiro lag, sugerindo que uma diferenciação de ordem 1(d=1) pode ser suficiente para tornar a série temporal estacionária.\n",
        "\n",
        "Ordem do Componente AR (p): A função de autocorrelação parcial (PACF) mostra uma autocorrelação significativa no primeiro lag e uma queda gradual nos lags subsequentes. Isso sugere que um termo autoregressivo de ordem 1 (p=1) pode ser apropriado.\n",
        "\n",
        "Ordem do Componente MA (q): A função de autocorrelação (ACF) mostra uma autocorrelação significativa nos primeiros lags, indicando a presença de um componente de média móvel. Um termo de média móvel de ordem 1 (q=1) pode ser uma escolha razoável."
      ],
      "metadata": {
        "id": "AKwGOMGm2Bzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "024lQJIK2BwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Selecionei um intervalo de exemplo para fins de ilustração\n",
        "df_BoomCylinder_LE = BDados_Temp_Trans[['BoomCylinder_LE', 'Timestamp']].dropna().head(100)\n",
        "\n",
        "df_BoomCylinder_LE['Timestamp'] = pd.to_datetime(df_BoomCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_BoomCylinder_LE.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Ajustar um modelo ARIMA\n",
        "order = (1, 1, 1)  # Substitua p, d, q pelos valores apropriados\n",
        "model = ARIMA(df_BoomCylinder_LE['BoomCylinder_LE'], order=order)\n",
        "result = model.fit()\n",
        "\n",
        "# Fazer previsões\n",
        "forecast_steps = 10  # Número de passos de previsão\n",
        "forecast = result.get_forecast(steps=forecast_steps)\n",
        "\n",
        "# Plotar os resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_BoomCylinder_LE['BoomCylinder_LE'], label='Histórico')\n",
        "plt.plot(forecast.predicted_mean.index, forecast.predicted_mean, color='red', label='Previsão')\n",
        "plt.fill_between(forecast.predicted_mean.index,\n",
        "                 forecast.conf_int().iloc[:, 0],\n",
        "                 forecast.conf_int().iloc[:, 1], color='gray', alpha=0.2, label='Intervalo de Confiança')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BoomCylinder_LE')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ttU7tBponM4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B5EUlsppyQA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransStickCilindersLD_LE = BDados_Temp_Trans[['StickCylinder_LD', 'StickCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_StickCilindersLD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)StickCylinder_LD', '(C°)StickCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para StickCylinder_LD\n",
        "fig_StickCilindersLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransStickCilindersLD_LE['Timestamp'], y=BDados_Temp_TransStickCilindersLD_LE['StickCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)StickCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para StickCylinder_LE\n",
        "fig_StickCilindersLD_LE.add_trace(go.Scatter(x=BDados_Temp_TransStickCilindersLD_LE['Timestamp'], y=BDados_Temp_TransStickCilindersLD_LE['StickCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)StickCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_StickCilindersLD_LE.update_layout(title_text='Temperatura StickCylinder_LD e StickCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_StickCilindersLD_LE.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_StickCylinder_LD = BDados_Temp_Trans[['StickCylinder_LD', 'Timestamp']].dropna()\n",
        "df_StickCylinder_LE = BDados_Temp_Trans[['StickCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_StickCylinder_LD['Timestamp'] = pd.to_datetime(df_StickCylinder_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_StickCylinder_LE['Timestamp'] = pd.to_datetime(df_StickCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_StickCylinder_LD[['Timestamp']]\n",
        "y_LD = df_StickCylinder_LD['StickCylinder_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_StickCylinder_LE[['Timestamp']]\n",
        "y_LE = df_StickCylinder_LE['StickCylinder_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('StickCylinder_LE')\n",
        "plt.title('Previsto vs Real para StickCylinder_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('StickCylinder_LD')\n",
        "plt.title('Previsto vs Real para StickCylinder_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HESwKGhFfaik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRnyWeD6ql4Z"
      },
      "source": [
        "###### 3.3.8.3 Separating data from FinalDrive_LD_LE components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UbJhbKVr9Yi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransFinalDrive_LD_LE = BDados_Temp_Trans[['FinalDrive_LD', 'FinalDrive_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_FinalDrive_LD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)FinalDrive_LD', '(C°)FinalDrive_LE'])\n",
        "\n",
        "# Adicionar traces para FinalDrive_LD\n",
        "fig_FinalDrive_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransFinalDrive_LD_LE['Timestamp'], y=BDados_Temp_TransFinalDrive_LD_LE['FinalDrive_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)FinalDrive_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para FinalDrive_LE\n",
        "fig_FinalDrive_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransFinalDrive_LD_LE['Timestamp'], y=BDados_Temp_TransFinalDrive_LD_LE['FinalDrive_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)FinalDrive_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_FinalDrive_LD_LE.update_layout(title_text='Temperatura FinalDrive_LD e FinalDrive_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_FinalDrive_LD_LE.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_FinalDrive_LD = BDados_Temp_Trans[['FinalDrive_LD', 'Timestamp']].dropna()\n",
        "df_FinalDrive_LE = BDados_Temp_Trans[['FinalDrive_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_FinalDrive_LD['Timestamp'] = pd.to_datetime(df_FinalDrive_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_FinalDrive_LE['Timestamp'] = pd.to_datetime(df_FinalDrive_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_FinalDrive_LD[['Timestamp']]\n",
        "y_LD = df_FinalDrive_LD['FinalDrive_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_FinalDrive_LE[['Timestamp']]\n",
        "y_LE = df_FinalDrive_LE['FinalDrive_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('FinalDrive_LE')\n",
        "plt.title('Previsto vs Real para FinalDrive_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('FinalDrive_LD')\n",
        "plt.title('Previsto vs Real para FinalDrive_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TuTZOS_aj5hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1nsoJjsIIu"
      },
      "source": [
        "###### 3.3.8.4 Separating data from Shell Cylinder LD/LE components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--mQZTyIsQFM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransShellCylinder_LD_LE = BDados_Temp_Trans[['ShellCylinder_LD', 'ShellCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_ShellCylinder_LD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)ShellCylinder_LD', '(C°)ShellCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para ShellCylinder_LD\n",
        "fig_ShellCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransShellCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransShellCylinder_LD_LE['ShellCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)ShellCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para ShellCylinder_LE\n",
        "fig_ShellCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransShellCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransShellCylinder_LD_LE['ShellCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)ShellCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_ShellCylinder_LD_LE.update_layout(title_text='Temperatura ShellCylinder_LD e ShellCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_ShellCylinder_LD_LE.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_ShellCylinder_LD = BDados_Temp_Trans[['ShellCylinder_LD', 'Timestamp']].dropna()\n",
        "df_ShellCylinder_LE = BDados_Temp_Trans[['ShellCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_ShellCylinder_LD['Timestamp'] = pd.to_datetime(df_FinalDrive_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_ShellCylinder_LE['Timestamp'] = pd.to_datetime(df_FinalDrive_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_LD = df_ShellCylinder_LD[['Timestamp']]\n",
        "y_LD = df_ShellCylinder_LD['ShellCylinder_LD']\n",
        "\n",
        "X_train_LD, X_test_LD, y_train_LD, y_test_LD = train_test_split(X_LD, y_LD, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_LE = df_ShellCylinder_LE[['Timestamp']]\n",
        "y_LE = df_ShellCylinder_LE['ShellCylinder_LE']\n",
        "\n",
        "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X_LE, y_LE, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_LE = StandardScaler()\n",
        "X_train_scaled_LE = scaler_LE.fit_transform(X_train_LE)\n",
        "X_test_scaled_LE = scaler_LE.transform(X_test_LE)\n",
        "\n",
        "scaler_LD = StandardScaler()\n",
        "X_train_scaled_LD = scaler_LD.fit_transform(X_train_LD)\n",
        "X_test_scaled_LD = scaler_LD.transform(X_test_LD)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_LE = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LE.fit(X_train_scaled_LE, y_train_LE)\n",
        "y_pred_LE = model_LE.predict(X_test_scaled_LE)\n",
        "\n",
        "model_LD = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_LD.fit(X_train_scaled_LD, y_train_LD)\n",
        "y_pred_LD = model_LD.predict(X_test_scaled_LD)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_LE = mean_squared_error(y_test_LE, y_pred_LE)\n",
        "mse_LD = mean_squared_error(y_test_LD, y_pred_LD)\n",
        "print(f'Mean Squared Error LE: {mse_LE}')\n",
        "print(f'Mean Squared Error LD: {mse_LD}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_LE, y_test_LE, label='Real')\n",
        "plt.scatter(X_test_LE, y_pred_LE, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('df_ShellCylinder_LE')\n",
        "plt.title('Previsto vs Real para df_ShellCylinder_LE')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_LD, y_test_LD, label='Real')\n",
        "plt.scatter(X_test_LD, y_pred_LD, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('df_ShellCylinder_LD')\n",
        "plt.title('Previsto vs Real para df_ShellCylinder_LD')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6A6tH7CHkbNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni02PQSJsTFx"
      },
      "source": [
        "###### 3.3.8.5 Separating data from PumpDrive_1_2 components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QtiMQkUsa8-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransPumpDrive_1_2 = BDados_Temp_Trans[['PumpDrive_1', 'PumpDrive_2', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_PumpDrive_1_2 = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)PumpDrive_1', '(C°)PumpDrive_2'])\n",
        "\n",
        "# Adicionar traces para PumpDrive_1\n",
        "fig_PumpDrive_1_2.add_trace(go.Scatter(x=BDados_Temp_TransPumpDrive_1_2['Timestamp'], y=BDados_Temp_TransPumpDrive_1_2['PumpDrive_1'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)PumpDrive_1'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para PumpDrive_2\n",
        "fig_PumpDrive_1_2.add_trace(go.Scatter(x=BDados_Temp_TransPumpDrive_1_2['Timestamp'], y=BDados_Temp_TransPumpDrive_1_2['PumpDrive_2'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)PumpDrive_2'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_PumpDrive_1_2.update_layout(title_text='Temperatura PumpDrive_1 e PumpDrive_2 ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_PumpDrive_1_2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_PumpDrive_1 = BDados_Temp_Trans[['PumpDrive_1', 'Timestamp']].dropna()\n",
        "df_PumpDrive_2 = BDados_Temp_Trans[['PumpDrive_2', 'Timestamp']].dropna()\n",
        "\n",
        "df_PumpDrive_1['Timestamp'] = pd.to_datetime(df_PumpDrive_1['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "df_PumpDrive_2['Timestamp'] = pd.to_datetime(df_PumpDrive_2['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste LD\n",
        "X_1 = df_PumpDrive_1[['Timestamp']]\n",
        "y_1 = df_PumpDrive_1['PumpDrive_1']\n",
        "\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Divida os dados em treinamento e teste LE\n",
        "X_2 = df_PumpDrive_2[['Timestamp']]\n",
        "y_2 = df_PumpDrive_2['PumpDrive_2']\n",
        "\n",
        "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler_2 = StandardScaler()\n",
        "X_train_scaled_2 = scaler_2.fit_transform(X_train_2)\n",
        "X_test_scaled_2 = scaler_2.transform(X_test_2)\n",
        "\n",
        "scaler_1 = StandardScaler()\n",
        "X_train_scaled_1 = scaler_1.fit_transform(X_train_1)\n",
        "X_test_scaled_1 = scaler_1.transform(X_test_1)\n",
        "\n",
        "# Crie e ajuste o modelo SVM para ambos os DataFrames\n",
        "model_2 = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_2.fit(X_train_scaled_2, y_train_2)\n",
        "y_pred_2 = model_LE.predict(X_test_scaled_2)\n",
        "\n",
        "model_1 = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model_1.fit(X_train_scaled_LD, y_train_1)\n",
        "y_pred_1 = model_1.predict(X_test_scaled_1)\n",
        "\n",
        "# Avalie o desempenho dos modelos para ambos os DataFrames\n",
        "mse_2 = mean_squared_error(y_test_2, y_pred_2)\n",
        "mse_1 = mean_squared_error(y_test_1, y_pred_1)\n",
        "print(f'Mean Squared Error LE: {mse_2}')\n",
        "print(f'Mean Squared Error LD: {mse_1}')\n",
        "\n",
        "# Plotar resultados lado a lado\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_test_2, y_test_2, label='Real')\n",
        "plt.scatter(X_test_2, y_pred_2, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('PumpDrive_2')\n",
        "plt.title('Previsto vs Real para PumpDrive_2')\n",
        "plt.legend()\n",
        "\n",
        "# Gráfico para df_BoomCylinder_LD\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_test_1, y_test_1, label='Real')\n",
        "plt.scatter(X_test_1, y_pred_1, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('PumpDrive_1')\n",
        "plt.title('Previsto vs Real para PumpDrive_1')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Garante que os gráficos não se sobreponham\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Rq6u6vjZk7JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxLyO-qBscSg"
      },
      "source": [
        "###### 3.3.8.6 Separating data from ShellSwingGear_1_2 components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYpAHLDpskJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransShellSwingGear_1_2 = BDados_Temp_Trans[['SwingGear_1', 'SwingGear_2', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_SwingGear_1_2 = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)SwingGear_1', '(C°)SwingGear_2'])\n",
        "\n",
        "# Adicionar traces para SwingGear_1\n",
        "fig_SwingGear_1_2.add_trace(go.Scatter(x=BDados_Temp_TransShellSwingGear_1_2['Timestamp'], y=BDados_Temp_TransShellSwingGear_1_2['SwingGear_1'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)SwingGear_1'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para SwingGear_2\n",
        "fig_SwingGear_1_2.add_trace(go.Scatter(x=BDados_Temp_TransShellSwingGear_1_2['Timestamp'], y=BDados_Temp_TransShellSwingGear_1_2['SwingGear_2'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)SwingGear_2'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_SwingGear_1_2.update_layout(title_text='Temperatura SwingGear_1 e SwingGear_2 ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_SwingGear_1_2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_SwingGear_1 = BDados_Temp_TransShellSwingGear_1_2[['SwingGear_1', 'Timestamp']].dropna()\n",
        "\n",
        "df_SwingGear_1['Timestamp'] = pd.to_datetime(df_SwingGear_1['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_SwingGear_1[['Timestamp']]\n",
        "y = df_SwingGear_1['SwingGear_1']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('SwingGear_1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C35auy4rlh4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_SwingGear_2 = BDados_Temp_TransShellSwingGear_1_2[['SwingGear_2', 'Timestamp']].dropna()\n",
        "\n",
        "df_SwingGear_2['Timestamp'] = pd.to_datetime(df_SwingGear_2['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_SwingGear_2[['Timestamp']]\n",
        "y = df_SwingGear_2['SwingGear_2']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('SwingGear_2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2xn1xzmelxHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOaS77JPslwF"
      },
      "source": [
        "###### 3.3.8.7 Separating data from Bucket Cylinder LD LE components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOoGOYovtC9l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDados_Temp_TransBucketCylinder_LD_LE = BDados_Temp_Trans[['BucketCylinder_LD', 'BucketCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_BucketCylinder_LD_LE = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)BucketCylinder_LD', '(C°)BucketCylinder_LE'])\n",
        "\n",
        "# Adicionar traces para BucketCylinder_LD\n",
        "fig_BucketCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBucketCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransBucketCylinder_LD_LE['BucketCylinder_LD'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BucketCylinder_LD'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para BucketCylinder_LE\n",
        "fig_BucketCylinder_LD_LE.add_trace(go.Scatter(x=BDados_Temp_TransBucketCylinder_LD_LE['Timestamp'], y=BDados_Temp_TransBucketCylinder_LD_LE['BucketCylinder_LE'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)BucketCylinder_LE'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_BucketCylinder_LD_LE.update_layout(title_text='Temperatura BucketCylinder_LD e BucketCylinder_LE ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_BucketCylinder_LD_LE.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_BucketCylinder_LD = BDados_Temp_TransBucketCylinder_LD_LE[['BucketCylinder_LD', 'Timestamp']].dropna()\n",
        "\n",
        "df_BucketCylinder_LD['Timestamp'] = pd.to_datetime(df_BucketCylinder_LD['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_BucketCylinder_LD[['Timestamp']]\n",
        "y = df_BucketCylinder_LD['BucketCylinder_LD']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BucketCylinder_LD')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DJZg8QBhl-g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_BucketCylinder_LE = BDados_Temp_TransBucketCylinder_LD_LE[['BucketCylinder_LE', 'Timestamp']].dropna()\n",
        "\n",
        "df_BucketCylinder_LE['Timestamp'] = pd.to_datetime(df_BucketCylinder_LE['Timestamp']).astype(int) / 10**9  # Converta para segundos desde a época\n",
        "\n",
        "# Divida os dados em treinamento e teste\n",
        "X = df_BucketCylinder_LE[['Timestamp']]\n",
        "y = df_BucketCylinder_LE['BucketCylinder_LE']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalização dos dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crie e ajuste o modelo SVM\n",
        "model = SVR(kernel='linear')  # Você pode ajustar o tipo de kernel conforme necessário\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Faça previsões\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Avalie o desempenho do modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plotar resultados\n",
        "plt.scatter(X_test, y_test, label='Real')\n",
        "plt.scatter(X_test, y_pred, label='Previsto')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('BucketCylinder_LE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wToyaKM0mPNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taXs1YwUmaAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFSs7OtxtH8M"
      },
      "source": [
        "\n",
        "\n",
        "## 3.4 Component LIfe Projection\n",
        "\n",
        "[Figure 9.  Component LIfe Projection](https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/Figure%209.%20%20Component%20LIfe%20Projection%20on%20CAT%206030.png)\n",
        "\n",
        "\n",
        "\n",
        "# 4 Results/benefits\n",
        "\n",
        "Key benefits of implementing the best practice include the following:\n",
        "\n",
        "Improved Machine Availability and Productivity– Components like cylinders that have few typical condition monitoring parameters can be better monitored with additional methods like thermography; replacement plans (parts, scheduled downtime, etc) can be put in place before failure of the component.\n",
        "\n",
        "Additionally, these are direct and indirect benefits:\n",
        "•\tAllows greater agility in temperature assessment (lower MTTR)\n",
        "•\tReduces risks through less time spent in the asset's operating zone (Elimination of live work)\n",
        "•\tProvides greater sensitivity in decision making for asset shutdown and definition of the next cycle time measurement step\n",
        "•\tProvided greater predictability in the cylinder replacement planning and schedule process;\n",
        "o\tWear evaluation increase the accuracy in the replacement forecast;\n",
        "o\tImprovement in material stock planning by using the hours actually operated as a parameter;\n",
        "\n",
        "## R Markdown\n",
        "\n",
        "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R\n",
        "\n",
        "\n",
        "You can also embed plots, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6B4LofGxyjw"
      },
      "outputs": [],
      "source": [
        "#BDadosRNN.columns\n",
        "BDadosRNN_boomcylinder_ld_temperature = BDadosRNN[(BDadosRNN['NmeComp'] == 'BoomCylinder_LD') & (BDadosRNN['Parametro'] == 'Temperature')]\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "print(BDadosRNN_boomcylinder_ld_temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqkvJ5tfz0I4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "DfRnn = BDadosRNN[(BDadosRNN['NmeComp'] == 'BoomCylinder_LD') & (BDadosRNN['Parametro'] == 'Temperature')]\n",
        "\n",
        "DfRnn = DfRnn[['Timestamp', 'Value']].copy()\n",
        "DfRnn['Value_1'] = DfRnn['Value'].shift(1)\n",
        "DfRnn['Value_2'] = DfRnn['Value'].shift(2)\n",
        "DfRnn['Value_3'] = DfRnn['Value'].shift(3)\n",
        "\n",
        "# Remover linhas com NaN resultantes do deslocamento\n",
        "DfRnn = DfRnn.dropna()\n",
        "DfRnn = DfRnn.rename(columns={'Value': 'y', 'Value_1': 'x_1', 'Value_2': 'x_2', 'Value_3': 'x_3'})\n",
        "DfRnn = DfRnn.drop(columns=['Timestamp'])\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "print(DfRnn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k35pFDMuFhh"
      },
      "source": [
        "Normalização dos dados do Data Frame BoomCylinder_LD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIwhB9cUuIqN"
      },
      "outputs": [],
      "source": [
        "# Normalização MinMax\n",
        "def normalize(x):\n",
        "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "\n",
        "# Aplicar a normalização\n",
        "DFRnnNorm = pd.DataFrame({\n",
        "    'Y': normalize(DfRnn['y']),\n",
        "    'X_1': normalize(DfRnn['x_1']),\n",
        "    'X_2': normalize(DfRnn['x_2']),\n",
        "    'X_3': normalize(DfRnn['x_3'])\n",
        "})\n",
        "\n",
        "# Dividir em conjuntos de treino e teste\n",
        "Y_array = DFRnnNorm['Y'].values[:-500].reshape(-1, 1)\n",
        "X_array = DFRnnNorm[['X_1', 'X_2', 'X_3']].values[:-500].reshape(-1, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxWtQWDSuafl"
      },
      "source": [
        "## 4.1 Results Rede Neutal (RNN) e LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fm2OhPDdkUp"
      },
      "source": [
        "### 4.1.1 Results modelo RNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLPf1sWvJ61"
      },
      "source": [
        "Utilização de RNN com o valor target de 1,05 da Feature com o objetivo de testar as Features prevendo uma temperatura de 5,4 Graus maior, tal como descrito na introdução."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6s0BEF0f0_"
      },
      "source": [
        "### 4.1.2 Results modelo LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kyyxmbzRIJh"
      },
      "source": [
        "## 4.3 Results svm_linear, Regressão Linear e MLP (Multilayer Perceptron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99sSnQaeCXeo"
      },
      "source": [
        "MLP (Multilayer Perceptron) com a função de ativação Tangente Hiperbólica (tanh) é uma arquitetura de rede neural artificial que utiliza a função de ativação tangente hiperbólica em suas camadas ocultas. Vamos entender esses termos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_yQrP1oQTA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "\n",
        "# Carregando o DataFrame\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando a variável de destino transformada 'Value * 1.05'\n",
        "df['Target'] = df['Value'] * 1.05\n",
        "\n",
        "# Separando features e target\n",
        "X = df[['Value']]\n",
        "y = df['Target']\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializando os modelos\n",
        "svm_linear = SVR(kernel='linear')\n",
        "linear_reg = LinearRegression()\n",
        "mlp_tanh = MLPRegressor(activation='tanh', random_state=42)"
      ],
      "metadata": {
        "id": "lNsadvcdNXNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27TT7IYtiuS4"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "\n",
        "# Inicializando os modelos\n",
        "svm_linear = SVR(kernel='linear')\n",
        "\n",
        "# Treinando os modelos\n",
        "svm_linear.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Fazendo previsões nos conjuntos de teste\n",
        "svm_linear_predictions = svm_linear.predict(X_test)\n",
        "\n",
        "# Calculando o MAE para cada modelo\n",
        "svm_linear_mae = mean_absolute_error(y_test, svm_linear_predictions)\n",
        "\n",
        "# Exibindo resultados\n",
        "print(f'MAE para SVM com kernel linear: {svm_linear_mae}')\n",
        "\n",
        "# Criando um DataFrame para armazenar os resultados\n",
        "resultados_df_svm_linear = pd.DataFrame({\n",
        "    'Modelo': ['MLP com ativação linear'],\n",
        "    'MAE': [svm_linear_mae]\n",
        "})\n",
        "\n",
        "# Exibindo resultados\n",
        "print(resultados_df_svm_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIStZjKgi1Et"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Inicializando os modelos\n",
        "linear_reg = LinearRegression()\n",
        "\n",
        "# Treinando os modelos\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões nos conjuntos de teste\n",
        "linear_reg_predictions = linear_reg.predict(X_test)\n",
        "\n",
        "# Calculando o MAE para cada modelo\n",
        "linear_reg_mae = mean_absolute_error(y_test, linear_reg_predictions)\n",
        "\n",
        "# Exibindo resultados\n",
        "print(f'MAE para Regressão Linear: {linear_reg_mae}')\n",
        "\n",
        "\n",
        "# Criando um DataFrame para armazenar os resultados\n",
        "resultados_df_linear_reg_mae = pd.DataFrame({\n",
        "    'Modelo': ['MAE para Regressão Linear'],\n",
        "    'MAE': [linear_reg_mae]\n",
        "})\n",
        "\n",
        "# Exibindo resultados\n",
        "print(resultados_df_linear_reg_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJpRvZPji_61"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "# Inicializando o modelo MLP com ativação tanh\n",
        "mlp_tanh = MLPRegressor(hidden_layer_sizes=(50,), activation='tanh', max_iter=50)\n",
        "\n",
        "# Treinando o modelo\n",
        "mlp_tanh.fit(X_train.reshape((X_train.shape[0], X_train.shape[1])), y_train)\n",
        "\n",
        "# Fazendo previsões no conjunto de teste\n",
        "mlp_tanh_predictions = mlp_tanh.predict(X_test.reshape((X_test.shape[0], X_test.shape[1])))\n",
        "\n",
        "# Calculando o MAE para o modelo MLP com ativação tanh\n",
        "mlp_tanh_mae = mean_absolute_error(y_test, mlp_tanh_predictions)\n",
        "\n",
        "# Exibindo resultados\n",
        "print(f'MAE para SVM com kernel linear: {mlp_tanh_mae}')\n",
        "\n",
        "# Criando um DataFrame para armazenar os resultados\n",
        "resultados_df_mlp_tanh = pd.DataFrame({\n",
        "    'Modelo': ['MLP com ativação tanh'],\n",
        "    'MAE': [mlp_tanh_mae]\n",
        "})\n",
        "\n",
        "# Exibindo resultados\n",
        "print(resultados_df_mlp_tanh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponha que você tenha os seguintes DataFrames\n",
        "resultados_df_svm_linear = pd.DataFrame({'Modelo': ['SVM Linear'], 'Resultado MAE': [0.123]})\n",
        "resultados_df_lstm_mae = pd.DataFrame({'Modelo': ['LSTM'], 'Resultado MAE': [0.456]})\n",
        "resultados_df_linear_reg_mae = pd.DataFrame({'Modelo': ['Linear Regression'], 'Resultado MAE': [0.789]})\n",
        "resultados_df_mlp_tanh = pd.DataFrame({'Modelo': ['MLP Tanh'], 'Resultado MAE': [0.101]})\n",
        "\n",
        "# Lista de DataFrames\n",
        "dfs = [resultados_df_svm_linear, resultados_df_lstm_mae, resultados_df_linear_reg_mae, resultados_df_mlp_tanh]\n",
        "\n",
        "# Juntar DataFrames\n",
        "df_final = pd.concat(dfs, ignore_index=True)\n",
        "print(df_final)"
      ],
      "metadata": {
        "id": "I2KGEEZagHbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste modelo foi feito teste de RNN para os dados de temperatura do componente Boom Cylinder_ld somente, a fim de avaliar os resultados da RNN com 3 x features defasadas de uma linha.\n",
        "\n",
        "Resultado do MAE Médio (Cross Valid) = 0.12236133102496058"
      ],
      "metadata": {
        "id": "JzDK-4KqIvFP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Z0P8keQ5TE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Supondo que você já tenha seus dados e o DataFrame é chamado df\n",
        "# Certifique-se de ter a coluna 'Timestamp' como datetime e 'Value' como o alvo\n",
        "# Carregando DataFrame\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['Target1'] = df['Value'].shift(1)\n",
        "df['Target2'] = df['Value'].shift(2)\n",
        "df['Target3'] = df['Value'].shift(3)\n",
        "\n",
        "# Removendo linhas com NaN resultantes das defasagens\n",
        "df = df.dropna()\n",
        "\n",
        "# Separando features e target\n",
        "X = df[['Value']]\n",
        "y = df[['Target1', 'Target2', 'Target3']]\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Criando função para criar modelo RNN\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(50, activation='relu', input_shape=(input_shape, 1)))\n",
        "    model.add(Dense(3))  # 3 saídas para as três colunas alvo\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Classe wrapper para usar KerasRegressor com Scikit-Learn\n",
        "class KerasRNN(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y, epochs=15, batch_size=16):\n",
        "        self.model = create_rnn_model(self.input_shape)\n",
        "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "# Criando o modelo KerasRegressor para Scikit-Learn\n",
        "rnn_model = make_pipeline(StandardScaler(), KerasRNN(input_shape=X_scaled.shape[1]))\n",
        "\n",
        "# Avaliação usando validação cruzada com TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mae_scores = -cross_val_score(rnn_model, X_scaled, y_scaled, cv=tscv, scoring=mae_scorer)\n",
        "\n",
        "# Exibindo resultados da validação cruzada\n",
        "print('MAE Médio (Cross Valid):', np.mean(mae_scores))\n",
        "\n",
        "# Treinando o modelo com todos os dados para visualização\n",
        "rnn_model.fit(X_scaled, y_scaled)\n",
        "\n",
        "# Função para plotar previsões\n",
        "def plot_rnn_predictions(model, X, y, timestamps, title):\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Desfazer a normalização\n",
        "    predictions = scaler_y.inverse_transform(predictions)\n",
        "    y = scaler_y.inverse_transform(y)\n",
        "\n",
        "# Plotar previsões para RNN com 3 features\n",
        "plot_rnn_predictions(rnn_model, X_scaled, y_scaled, df['Timestamp'], 'Previsões RNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo foi elaborado para testar os dados de temperatura do cilindro LD, com o modelo de LSTM (Long Short-Term Memory) com os seguintes parametros:\n",
        "\n",
        "a) epochs=15,\n",
        "\n",
        "b) batch_size=16\n",
        "\n",
        "c) activation='relu'\n",
        "\n",
        "Obteve resultado de MAE Médio (Cross Valid) = 0.12031637800939518\n"
      ],
      "metadata": {
        "id": "P9I1h2IMKzXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dHM6777XixR",
        "outputId": "0bd7a200-4a42-4765-9452-be035dbbce0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 1s 3ms/step - loss: 1.9415\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 1s 3ms/step - loss: 1.1535\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 1s 3ms/step - loss: 0.8163\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1s 3ms/step - loss: 0.5980\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 1s 3ms/step - loss: 0.4951\n",
            "43/43 [==============================] - 0s 3ms/step\n",
            "257/257 [==============================] - 3s 3ms/step - loss: 0.3903\n",
            "257/257 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 1s 3ms/step - loss: 1.9005\n",
            "43/43 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 1s 3ms/step - loss: 1.1152\n",
            "43/43 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 2s 3ms/step - loss: 0.7831\n",
            "43/43 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 2s 4ms/step - loss: 0.5626\n",
            "43/43 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 2s 3ms/step - loss: 0.4725\n",
            "43/43 [==============================] - 1s 2ms/step\n",
            "257/257 [==============================] - 3s 3ms/step - loss: 0.3780\n",
            "257/257 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 1s 6ms/step - loss: 1.2752\n",
            "43/43 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 1s 3ms/step - loss: 1.2586\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 1s 3ms/step - loss: 1.0459\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1s 3ms/step - loss: 0.8816\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 1s 3ms/step - loss: 0.8520\n",
            "43/43 [==============================] - 0s 2ms/step\n",
            "257/257 [==============================] - 2s 3ms/step - loss: 0.7603\n",
            "257/257 [==============================] - 1s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43/43 [==============================] - 1s 3ms/step - loss: 1.9361\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 1s 3ms/step - loss: 1.1732\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - 1s 3ms/step - loss: 0.8433\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 1s 3ms/step - loss: 0.6018\n",
            "43/43 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 11 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215/215 [==============================] - 1s 3ms/step - loss: 0.5100\n",
            "43/43 [==============================] - 0s 2ms/step\n",
            "257/257 [==============================] - 4s 5ms/step - loss: 0.4027\n",
            "257/257 [==============================] - 1s 2ms/step\n",
            "        MAE       MSE      RMSE        R²                Nome_DF  \\\n",
            "0  0.431469  0.237227  0.487059  0.968044  df_results_LSTM_3Targ   \n",
            "1  0.424205  0.222475  0.471673  0.967573  df_results_LSTM_3Targ   \n",
            "2  0.841759  0.786326  0.886750  0.601641  df_results_LSTM_3Targ   \n",
            "3  0.444205  0.241940  0.491874  0.966983  df_results_LSTM_3Targ   \n",
            "\n",
            "  Activation_Function  \n",
            "0                relu  \n",
            "1                tanh  \n",
            "2             sigmoid  \n",
            "3              linear  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm  # Importa a função tqdm para mostrar a barra de progresso\n",
        "\n",
        "# Supondo que você já tenha seus dados e o DataFrame é chamado df\n",
        "# Certifique-se de ter a coluna 'Timestamp' como datetime e 'Value' como o alvo\n",
        "# Carregando DataFrame\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['Target1'] = df['Value'].shift(1)\n",
        "df['Target2'] = df['Value'].shift(2)\n",
        "df['Target3'] = df['Value'].shift(3)\n",
        "\n",
        "# Removendo linhas com NaN resultantes das defasagens\n",
        "df = df.dropna()\n",
        "\n",
        "# Separando features e target\n",
        "X = df[['Value']]\n",
        "y = df[['Target1', 'Target2', 'Target3']]\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Lista de funções de ativação a serem testadas\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "\n",
        "# Dicionário para armazenar resultados de MAE para cada função de ativação\n",
        "mae_results_lstm_3target = {\n",
        "    'MAE': [],\n",
        "    'MSE': [],\n",
        "    'RMSE': [],\n",
        "    'R²': [],\n",
        "    'Nome_DF': [],\n",
        "    'Activation_Function': [],\n",
        "}\n",
        "\n",
        "# Loop sobre as funções de ativação\n",
        "for activation_function in activation_functions:\n",
        "    # Criando e treinando o modelo para a função de ativação atual\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(50, activation=activation_function, input_shape=(X_scaled.shape[1], 1)))\n",
        "    lstm_model.add(Dense(3))  # 3 saídas para as três colunas alvo\n",
        "    lstm_model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "    # Criando o modelo KerasRegressor para Scikit-Learn\n",
        "    lstm_model_pipeline = make_pipeline(StandardScaler(), lstm_model)\n",
        "\n",
        "    # Avaliação usando validação cruzada com TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "    mae_scores = -cross_val_score(lstm_model_pipeline, X_scaled, y_scaled, cv=tscv, scoring=mae_scorer)\n",
        "\n",
        "    # Calculando as métricas médias\n",
        "    mae_mean = np.mean(mae_scores)\n",
        "\n",
        "    # Calculando MSE manualmente\n",
        "    mse_mean = np.mean(np.square(-mae_scores))\n",
        "\n",
        "    # Calculando RMSE\n",
        "    rmse_mean = np.sqrt(mse_mean)\n",
        "\n",
        "    # Calculando R²\n",
        "    r2_mean = r2_score(y_scaled, lstm_model_pipeline.fit(X_scaled, y_scaled).predict(X_scaled))\n",
        "\n",
        "    # Armazenando os resultados no dicionário\n",
        "    mae_results_lstm_3target['MAE'].append(mae_mean)\n",
        "    mae_results_lstm_3target['MSE'].append(mse_mean)\n",
        "    mae_results_lstm_3target['RMSE'].append(rmse_mean)\n",
        "    mae_results_lstm_3target['R²'].append(r2_mean)\n",
        "    mae_results_lstm_3target['Activation_Function'].append(activation_function)\n",
        "    mae_results_lstm_3target['Nome_DF'].append('df_results_LSTM_3Targ')\n",
        " #   mae_results_lstm_3target[activation_function] = {'Activation_Function': activation_function}\n",
        "\n",
        "\n",
        "# Convertendo o dicionário em DataFrame\n",
        "df_results_LSTM_3Targ = pd.DataFrame(mae_results_lstm_3target)\n",
        "# Adicionando uma coluna com o nome do DataFrame\n",
        "#df_results_LSTM_3Targ['Nome_DF'] = 'df_results_LSTM_3Targ'\n",
        "\n",
        "# Exibindo os resultados\n",
        "print(df_results_LSTM_3Targ)\n",
        "\n",
        "# Salvar o DataFrame em um arquivo CSV\n",
        "df_results_LSTM_3Targ.to_csv('df_results_LSTM_3Targ.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kMLBuvAoqAv"
      },
      "source": [
        "## 4.4 Results / Comparação das MAEs para modelos de RNN com funções de ativação distintos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWeJpmEtZ2vd"
      },
      "source": [
        "O modelo a seguir, foi elaborado para testar os dados de temperatura do cilindro LD, com o modelo de RNN utilizando dados sequenciais de 80 amostras a cada envio, agora comparando o resultado da MAE variando a função de ativação conforme definiso nos parametros:\n",
        "\n",
        "a) epochs=50,\n",
        "\n",
        "b) batch_size=32\n",
        "\n",
        "c) activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "d) optimizer='adam',\n",
        "\n",
        "e) loss='mean_squared_error\n",
        "\n",
        "Modelo de RNN com amostras de 80 linhas, e com varias funções de ativação com comparação das MAE\n",
        "\n",
        "1) MAE para relu: 0.6380209729715359\n",
        "\n",
        "2) MAE para tanh: 0.5595733862566802\n",
        "\n",
        "3) MAE para sigmoid: 0.7872871740610322\n",
        "\n",
        "4) MAE para linear: 0.5702095221302993\n",
        "\n",
        "Como otimizador foi utilizado o otimizador 'adam' que é frequentemente usado por padrão, pois tem um bom desempenho em muitos casos (Inserir Referencia)\n",
        "\n",
        "SGD (Gradiente Descendente Estocástico): optimizer = 'sgd'\n",
        "\n",
        "RMSprop (Média Móvel do Quadrado dos Gradientes): optimizer = 'rmsprop'\n",
        "\n",
        "Adagrad (Gradiente Adaptativo): optimizer = 'adagrad'\n",
        "\n",
        "Adadelta (Adaptive Delta): optimizer = 'adadelta'\n",
        "\n",
        "Nadam (Nesterov-acelerado Adaptive Moment Estimation):optimizer = 'nadam'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QddRImRwbpm",
        "outputId": "42ff9079-4d57-44fb-8491-78015738315f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 1s 7ms/step\n",
            "51/51 [==============================] - 1s 7ms/step\n",
            "51/51 [==============================] - 1s 12ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm  # Importa a função tqdm para mostrar a barra de progresso\n",
        "\n",
        "# Define Data Frame para a temperatura do Cilindro do Boon LD\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['target'] = df['Value']\n",
        "\n",
        "# Extraindo os valores da coluna alvo\n",
        "target_values = df['target'].values\n",
        "\n",
        "# Normalizando os dados para o intervalo [0, 1]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target_values = scaler.fit_transform(target_values.reshape(-1, 1))\n",
        "\n",
        "# Criando sequências de 80 em 80\n",
        "sequence_length = 80\n",
        "sequences = [target_values[i:i + sequence_length] for i in range(len(target_values) - sequence_length)]\n",
        "\n",
        "# Convertendo para array numpy\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Separando em conjuntos de treinamento e teste\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data, test_data = sequences[:train_size], sequences[train_size:]\n",
        "\n",
        "# Separando as entradas (X) e saídas (y)\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "# Reshape dos dados para o formato de entrada da RNN (batch_size, timesteps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Lista de funções de ativação a serem testadas\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "# Dicionário para armazenar resultados de MAE para cada função de ativação\n",
        "metrics_results_RNN80Sample = {}\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    # Criando e treinando o modelo para a função de ativação atual\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=50, activation=activation_function, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Fazendo previsões\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Invertendo a normalização para obter os valores reais\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Calculando as métricas\n",
        "    mae = mean_absolute_error(y_test_original, predictions)\n",
        "    mse = mean_squared_error(y_test_original, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test_original, predictions)\n",
        "\n",
        "    # Armazenando as métricas no dicionário\n",
        "    metrics_results_RNN80Sample[activation_function] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R²': r2, 'Activation_Function': activation_function}\n",
        "\n",
        "\n",
        "# Criar um DataFrame com os resultados de MAE\n",
        "df_results_RNN80Sample = pd.DataFrame(metrics_results_RNN80Sample).transpose()\n",
        "\n",
        "# Adicionando uma coluna com o nome do DataFrame\n",
        "df_results_RNN80Sample['Nome_DF'] = 'df_results_RNN80Sample'\n",
        "\n",
        "# Exibir a tabela comparativa\n",
        "print(df_results_RNN80Sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHoZ2mnPaF8I"
      },
      "source": [
        "Agora com LSTM e comparação entre funções de ativação RELU, LINEAR, MPL e Tangente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X5y1kgwaPgi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy.stats import weightedtau  # Importar a função weightedtau da biblioteca scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # Importa a função tqdm para mostrar a barra de progresso\n",
        "\n",
        "# Define Data Frame para a temperatura do Cilindro do Boon LD\n",
        "df = BDadosRNN_boomcylinder_ld_temperature.copy()\n",
        "\n",
        "# Criando colunas defasadas\n",
        "df['target'] = df['Value']\n",
        "\n",
        "# Extraindo os valores da coluna alvo\n",
        "target_values = df['target'].values\n",
        "\n",
        "# Normalizando os dados para o intervalo [0, 1]\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "target_values = scaler.fit_transform(target_values.reshape(-1, 1))\n",
        "\n",
        "# Criando sequências de 80 em 80\n",
        "sequence_length = 80\n",
        "sequences = [target_values[i:i + sequence_length] for i in range(len(target_values) - sequence_length)]\n",
        "\n",
        "# Convertendo para array numpy\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Separando em conjuntos de treinamento e teste\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data, test_data = sequences[:train_size], sequences[train_size:]\n",
        "\n",
        "# Separando as entradas (X) e saídas (y)\n",
        "X_train, y_train = train_data[:, :-1], train_data[:, -1]\n",
        "X_test, y_test = test_data[:, :-1], test_data[:, -1]\n",
        "\n",
        "# Reshape dos dados para o formato de entrada da LSTM (batch_size, timesteps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Lista de funções de ativação a serem testadas\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "\n",
        "# Dicionário para armazenar resultados de MAE para cada função de ativação\n",
        "metrics_results_LSTM80Sample = {}\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    # Criando e treinando o modelo para a função de ativação atual\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, activation=activation_function, input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Fazendo previsões\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculando as métricas\n",
        "    mae = mean_absolute_error(y_test_original, predictions)\n",
        "    mse = mean_squared_error(y_test_original, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test_original, predictions)\n",
        "\n",
        "    # Armazenando as métricas no dicionário\n",
        "    metrics_results_LSTM80Sample[activation_function] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R²': r2, 'Activation_Function': activation_function}\n",
        "\n",
        "# Criar um DataFrame com os resultados\n",
        "df_results_LSTM80Sample = pd.DataFrame(metrics_results_LSTM80Sample).transpose()\n",
        "\n",
        "# Adicionando uma coluna com o nome do DataFrame\n",
        "df_results_LSTM80Sample['Nome_DF'] = 'df_results_LSTM80Sample'\n",
        "\n",
        "# Exibir a tabela comparativa\n",
        "print(df_results_LSTM80Sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sugestão de outros parametros estatísticos\n",
        "\n",
        "***MSE (Erro Quadrático Médio):***\n",
        "\n",
        "O MSE é outra métrica de erro que calcula a média dos quadrados das diferenças entre os valores previstos e os valores reais. Pode ser mais sensível a grandes erros do que o MAE.\n",
        "\n",
        "***RMSE (Raiz do Erro Quadrático Médio):***\n",
        "\n",
        "O RMSE é a raiz quadrada do MSE. Ele fornece uma medida do erro na mesma unidade que a variável de destino, facilitando a interpretação.\n",
        "\n",
        "***R² (Coeficiente de Determinação):***\n",
        "\n",
        "O R² é uma métrica que varia de 0 a 1 e indica a proporção da variância na variável dependente que é previsível a partir da variável independente. Um R² próximo de 1 indica um bom ajuste do modelo.\n",
        "\n",
        "***MAPE (Erro Percentual Absoluto Médio):***\n",
        "\n",
        "O MAPE é uma métrica de erro percentual que calcula a média das percentagens absolutas de erro entre os valores previstos e reais. É útil quando os dados têm diferentes escalas ou magnitudes.\n",
        "\n",
        "***Índice de Concordância (em problemas de regressão):***\n",
        "\n",
        "O índice de concordância avalia o quão bem as previsões de um modelo se alinham com os valores reais."
      ],
      "metadata": {
        "id": "XiF86tRQ1u0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "from github import Github\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "# Concatenando os DataFrames ao longo das colunas\n",
        "df_combined = pd.concat([df_results_RNN80Sample, df_results_LSTM80Sample, df_results_LSTM_3Targ], ignore_index=True)\n",
        "\n",
        "\n",
        "# Autenticação no GitHub\n",
        "token = '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'\n",
        "\n",
        "g = Github(token)\n",
        "\n",
        "#TokenUSP_TCC — admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, admin:ssh_signing_key, audit_log, codespace, copilot, delete:packages, delete_repo, gist, notifications, project, repo, user, workflow, write:discussion, write:packages\n",
        "#Expires on Fri, May 17 2024.\n",
        "\n",
        "\n",
        "# Repositório\n",
        "usuario = 'CidClayQuirino'\n",
        "repositorio = 'rnn-component-lIfe-cycle'\n",
        "\n",
        "# Nome do arquivo\n",
        "nome_arquivo = 'df_combined.csv'\n",
        "# Salvar DataFrame como CSV em um BytesIO\n",
        "csv_bytes = BytesIO()\n",
        "df_combined.to_csv(csv_bytes, index=False)\n",
        "\n",
        "# Obter o repositório\n",
        "repo = g.get_user(usuario).get_repo(repositorio)\n",
        "\n",
        "# Caminho local para salvar temporariamente o arquivo\n",
        "caminho_local = 'df_combined.csv'\n",
        "\n",
        "# Salvar o arquivo localmente\n",
        "with open(caminho_local, 'wb') as file:\n",
        "    file.write(csv_bytes.getvalue())\n",
        "\n",
        "# Criar ou atualizar o arquivo no repositório\n",
        "try:\n",
        "    arquivo = repo.get_contents(nome_arquivo)\n",
        "    repo.update_file(nome_arquivo, 'Atualizando resultados', open(caminho_local, 'rb').read(), arquivo.sha)\n",
        "    print(f'{nome_arquivo} atualizado com sucesso!')\n",
        "except Exception as e:\n",
        "    repo.create_file(nome_arquivo, 'Adicionando resultados', open(caminho_local, 'rb').read())\n",
        "    print(f'{nome_arquivo} criado com sucesso!')\n",
        "df_combined.head()"
      ],
      "metadata": {
        "id": "jNuSKt5ZWLKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código pode ser otimizado para melhorar o desempenho. Aqui estão algumas sugestões:\n",
        "\n",
        "Amostragem de Dados: Em vez de usar todos os dados para treinar o modelo SVM, você pode considerar a amostragem dos dados, especialmente se o conjunto de dados for grande. Isso pode acelerar o treinamento e ainda fornecer uma boa aproximação.\n",
        "\n",
        "Grid Search para Otimização de Parâmetros: Utilize Grid Search para encontrar os melhores parâmetros para o modelo SVM. O Grid Search testa diferentes combinações de parâmetros e retorna os melhores. Isso pode ajudar a melhorar o desempenho do modelo.\n",
        "\n",
        "Aqui está uma versão modificada do código com essas sugestões:"
      ],
      "metadata": {
        "id": "WCUOX_QNdBPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora otimização do código para projetar os dados com base nas últimas 1000 amostras das features (Value_1, Value_2, Value_3). Além disso, vamos remover o loop e utilizar a função numpy para realizar as previsões de uma vez. Aqui está o código otimizado:\n",
        "\n",
        "Detalhamento:# Definindo os parâmetros para Grid Search\n",
        "\n",
        "'kernel': ['linear']: Aqui, estamos especificando que queremos usar um kernel linear para o SVR. O kernel linear é adequado quando há uma relação linear entre as características de entrada e a variável de destino.\n",
        "\n",
        "'C': [0.1, 1, 10]: O parâmetro C é um hiperparâmetro de regularização no SVR. Ele controla o equilíbrio entre ter um ajuste suave (evitar sobreajuste) e ajustar aos dados de treinamento. Valores menores de C indicam um modelo mais suave, enquanto valores maiores permitem um ajuste mais preciso aos dados de treinamento. Aqui, estamos testando três valores diferentes para C: 0.1, 1 e 10.\n",
        "\n",
        "O Grid Search é uma técnica que busca os melhores hiperparâmetros em uma grade predefinida. Ele treina o modelo para cada combinação possível de valores de hiperparâmetros e avalia o desempenho usando uma métrica especificada (neste caso, o negativo da média do erro absoluto). O conjunto de hiperparâmetros que resulta no melhor desempenho de acordo com a métrica escolhida é então escolhido como o modelo final.\n",
        "\n",
        "O Grid Search é uma maneira sistemática de ajustar os hiperparâmetros de um modelo para otimizar seu desempenho. Neste caso, estamos usando o Grid Search para encontrar os melhores parâmetros para o SVR com kernel linear e o parâmetro de regularização C."
      ],
      "metadata": {
        "id": "jFAD8KDf09eV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora com projeção até atingir 5% a mais da temperatura da Feature"
      ],
      "metadata": {
        "id": "rC4xzQxG1kPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "código para identificar variações de aumento na temperatura (Value) ao longo do tempo (Timestamp) e apresentar taxas de variações acima de 1% para cada conjunto de 1000 amostras:"
      ],
      "metadata": {
        "id": "l5KpeMiiWqrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import dash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Supondo que você já tenha um DataFrame chamado BDados_Temp com as colunas NmeComp, Timestamp e Value\n",
        "BDados_Temp = pd.DataFrame(BDados_Temp)\n",
        "\n",
        "# Convertendo a coluna Timestamp para o formato datetime\n",
        "BDados_Temp['Timestamp'] = pd.to_datetime(BDados_Temp['Timestamp'])\n",
        "\n",
        "# Ordenando o DataFrame pela coluna Timestamp\n",
        "BDados_Temp = BDados_Temp.sort_values(by='Timestamp')\n",
        "\n",
        "# Calculando a média móvel de 5 horas\n",
        "BDados_Temp['Moving_Avg_5H'] = BDados_Temp.groupby('NmeComp')['Value'].rolling(window=5).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "# Inicialização do Dash\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Layout do aplicativo\n",
        "app.layout = html.Div([\n",
        "    dcc.Dropdown(\n",
        "        id='NmeComp-dropdown',\n",
        "        options=[{'label': NmeComp, 'value': NmeComp} for NmeComp in BDados_Temp['NmeComp'].unique()],\n",
        "        value=BDados_Temp['NmeComp'].unique()[0],\n",
        "        multi=False\n",
        "    ),\n",
        "    dcc.Checklist(\n",
        "        id='hide-points-checkbox',\n",
        "        options=[\n",
        "            {'label': 'Ocultar Pontos de Dados', 'value': 'hide_points'}\n",
        "        ],\n",
        "        value=[]\n",
        "    ),\n",
        "    dcc.Graph(id='line-plot'),\n",
        "])\n",
        "\n",
        "# Callback para atualizar o gráfico com base na seleção do dropdown e na opção de ocultar pontos de dados\n",
        "@app.callback(\n",
        "    Output('line-plot', 'figure'),\n",
        "    [Input('NmeComp-dropdown', 'value'),\n",
        "     Input('hide-points-checkbox', 'value')]\n",
        ")\n",
        "def update_graph(selected_NmeComp, hide_points_option):\n",
        "    # Filtrando os dados com base no NmeComp selecionado\n",
        "    filtered_data = BDados_Temp[BDados_Temp['NmeComp'] == selected_NmeComp]\n",
        "\n",
        "    fig = px.line(filtered_data, x='Timestamp', y='Value', title=f'Série Temporal - NmeComp {selected_NmeComp}',\n",
        "                  labels={'Value': 'Valor', 'Timestamp': 'Timestamp'})\n",
        "\n",
        "    # Adicionando a média móvel de 5 horas\n",
        "    fig.add_scatter(x=filtered_data['Timestamp'], y=filtered_data['Moving_Avg_5H'], mode='lines', name='Média Móvel 5H')\n",
        "\n",
        "    # Adicionando a linha de tendência usando a regressão linear\n",
        "    X = sm.add_constant(filtered_data['Timestamp'].astype(int) // 10**9)  # Convertendo Timestamp para segundos\n",
        "    model = sm.OLS(filtered_data['Value'], X)\n",
        "    results = model.fit()\n",
        "    trendline = results.fittedvalues\n",
        "    fig.add_trace(go.Scatter(x=filtered_data['Timestamp'], y=trendline, mode='lines', name='Linha de Tendência'))\n",
        "\n",
        "    # Ocultando os pontos de dados, se a opção 'Ocultar Pontos de Dados' estiver selecionada\n",
        "    if 'hide_points' in hide_points_option:\n",
        "        fig.update_traces(mode='lines')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Executando o aplicativo\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "8iwxg3h9mxsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URL do arquivo CSV no GitHub\n",
        "github_url = \"https://raw.githubusercontent.com/CidClayQuirino/rnn-component-lIfe-cycle/main/df_BDados_Temp.csv\"\n",
        "\n",
        "# Ler o DataFrame diretamente da URL\n",
        "df_BDados_Temp = pd.read_csv(github_url)\n",
        "\n",
        "# Exemplo de dados fictícios para ilustração\n",
        "df_BDados_Temp_Hora = pd.DataFrame(df_BDados_Temp)\n",
        "\n",
        "# Converter a coluna Timestamp para datetime, caso não esteja\n",
        "df_BDados_Temp_Hora['Timestamp'] = pd.to_datetime(df_BDados_Temp_Hora['Timestamp'])  # Correção aqui\n",
        "\n",
        "# Arredondar Timestamp para a hora mais próxima\n",
        "df_BDados_Temp_Hora['Timestamp'] = df_BDados_Temp_Hora['Timestamp'].dt.floor('H')  # Correção aqui\n",
        "\n",
        "# Calcular a média dos valores para cada NmeComp e cada hora\n",
        "df_BDados_Temp_Hora = df_BDados_Temp_Hora.groupby(['NmeComp', 'Timestamp'])['Value'].mean().reset_index()  # Correção aqui\n",
        "\n",
        "print(df_BDados_Temp_Hora)\n"
      ],
      "metadata": {
        "id": "MmRwuWvbtsSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supondo que você tenha um DataFrame chamado result_df com colunas NmeComp, Timestamp e Value\n",
        "# Substitua isso pelo seu DataFrame real\n",
        "\n",
        "df_BDados_Temp_Hora['Timestamp'] = pd.to_datetime(df_BDados_Temp_Hora['Timestamp'])\n",
        "\n",
        "# Calcular a média por hora para cada NmeComp\n",
        "df_BDados_Temp_Hora['Hour'] = df_BDados_Temp_Hora['Timestamp'].dt.hour\n",
        "mean_values = df_BDados_Temp_Hora.groupby(['NmeComp', 'Hour'])['Value'].mean().reset_index()\n",
        "\n",
        "# Plotar gráficos de dispersão separados para cada NmeComp com linha de tendência\n",
        "g = sns.FacetGrid(mean_values, col='NmeComp', col_wrap=4, height=4, sharey=False)\n",
        "g.map(sns.scatterplot, 'Hour', 'Value', marker='o', color='blue')\n",
        "g.map(sns.regplot, 'Hour', 'Value', scatter=False, color='red', ci=None)\n",
        "g.set_axis_labels('Hour', 'Mean Value')\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fNLr5usotxnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "# Converter a coluna Timestamp para datetime, caso não esteja\n",
        "df_BDados_Temp_Hora['Timestamp'] = pd.to_datetime(df_BDados_Temp_Hora['Timestamp'])\n",
        "\n",
        "# Calcular a média por dia para cada NmeComp\n",
        "df_BDados_Temp_Dia = df_BDados_Temp_Hora.copy()\n",
        "df_BDados_Temp_Dia['Day'] = df_BDados_Temp_Dia['Timestamp'].dt.date\n",
        "mean_values_daily = df_BDados_Temp_Dia.groupby(['NmeComp', 'Day'])['Value'].mean().reset_index()\n",
        "\n",
        "# Converter as datas para formato numérico\n",
        "mean_values_daily['Day'] = mdates.date2num(mean_values_daily['Day'])\n",
        "\n",
        "# Plotar gráficos de dispersão separados para cada NmeComp com linha de tendência\n",
        "g = sns.FacetGrid(mean_values_daily, col='NmeComp', col_wrap=4, height=4, sharey=False)\n",
        "g.map(sns.scatterplot, 'Day', 'Value', marker='o', color='blue')\n",
        "g.map(sns.regplot, 'Day', 'Value', scatter=False, color='red', ci=None)\n",
        "g.set_axis_labels('Day', 'Mean Value')\n",
        "g.set_titles(col_template=\"{col_name}\")\n",
        "g.set_xticklabels(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KlUp2mPDBTqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from github import Github\n",
        "from io import BytesIO\n",
        "\n",
        "# Defina suas credenciais do GitHub\n",
        "seu_token = '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'\n",
        "seu_usuario = 'CidClayQuirino'\n",
        "seu_repositorio = 'rnn-component-lIfe-cycle'\n",
        "\n",
        "#TokenUSP_TCC — admin:enterprise, admin:gpg_key, admin:org, admin:org_hook, admin:public_key, admin:repo_hook, admin:ssh_signing_key, audit_log, codespace, copilot, delete:packages, delete_repo, gist, notifications, project, repo, user, workflow, write:discussion, write:packages\n",
        "#Expires on Fri, May 17 2024.\n",
        "\n",
        "# Dicionário de DataFrames com seus nomes originais\n",
        "dataframes = {\n",
        "    'df_BDados_Temp_Dia': df_BDados_Temp_Dia,\n",
        "    'df_BDados_Temp_Hora': df_BDados_Temp_Hora,\n",
        "    'BDados_Temp_MM5H': BDados_Temp_MM5H,\n",
        "    'df_combined': df_combined,\n",
        "    'DfRnn': DfRnn,\n",
        "    'BDadosRNN_boomcylinder_ld_temperature': BDadosRNN_boomcylinder_ld_temperature,\n",
        "    'result_df': result_df,\n",
        "    'BDados_Temp': BDados_Temp,\n",
        "    'BDados_Temp_Trans': BDados_Temp_Trans,\n",
        "    'df_MeanTempDay': df_MeanTempDay,\n",
        "    'BDadosRNN': BDadosRNN\n",
        "    \"BoomCylinder_LE_LAG_ACF_PACF\": BoomCylinder_LE_LAG_ACF_PACF\n",
        "}\n",
        "\n",
        "# Função para salvar e enviar para o GitHub\n",
        "def salvar_e_enviar_para_github(dataframe, nome_arquivo, usuario, repositorio, token):\n",
        "    # Salvar DataFrame como CSV em um BytesIO\n",
        "    csv_bytes = BytesIO()\n",
        "    dataframe.to_csv(csv_bytes, index=False)\n",
        "\n",
        "    # Autenticar no GitHub\n",
        "    g = Github(token)\n",
        "\n",
        "    # Obter o repositório\n",
        "    repo = g.get_user(usuario).get_repo(repositorio)\n",
        "\n",
        "    # Criar ou atualizar o arquivo no repositório\n",
        "    try:\n",
        "        arquivo = repo.get_contents(nome_arquivo)\n",
        "        repo.update_file(nome_arquivo, f'Atualizando {nome_arquivo}', csv_bytes.getvalue(), arquivo.sha)\n",
        "        print(f'{nome_arquivo} atualizado com sucesso!')\n",
        "    except Exception as e:\n",
        "        repo.create_file(nome_arquivo, f'Adicionando {nome_arquivo}', csv_bytes.getvalue())\n",
        "        print(f'{nome_arquivo} criado com sucesso!')\n",
        "\n",
        "# Iterar sobre os DataFrames e salvá-los no GitHub\n",
        "for nome, df in dataframes.items():\n",
        "    nome_arquivo = f'{nome}.csv'  # Nome do arquivo usando o nome original do DataFrame\n",
        "    salvar_e_enviar_para_github(df, nome_arquivo, seu_usuario, seu_repositorio, seu_token)\n"
      ],
      "metadata": {
        "id": "jT_aoAcbRtit"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}