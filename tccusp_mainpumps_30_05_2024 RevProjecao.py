# -*- coding: utf-8 -*-
"""TccUsp_MainPumps 01_04-2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/TccUsp_MainPumps%2001_04-2024.ipynb
"""

!pip install markdown
!pip install statsmodels
!pip install scikit-learn
!pip install PyGithub
!pip install gitpython
!pip install statsmodels
!pip install dash
!pip install xlwt
!pip install openpyxl
!pip install tensorflow
!pip install scipy
# Atualizar pacotes
import numpy as np
import os
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from IPython.display import Image
from sklearn.svm import SVR

from zipfile import ZipFile
from io import BytesIO
import requests

# URL do repositório no GitHub
repo_url = 'https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/archive/main.zip'
dataframes = []

# Baixe e extraia o arquivo zip do repositório
response = requests.get(repo_url)
with ZipFile(BytesIO(response.content)) as zip_file:
    zip_file.extractall()

# Diretório onde os arquivos .xlsx foram extraídos
extracted_dir = 'rnn-component-lIfe-cycle-main'

# Loop pelos arquivos no diretório extraído
for arquivo in os.listdir(extracted_dir):
    if arquivo.endswith('.xlsx'):
        # Construa o caminho completo para o arquivo
        caminho_completo = os.path.join(extracted_dir, arquivo)

        # Leia o arquivo Excel e adicione-o à lista de DataFrames
        df = pd.read_excel(caminho_completo)

        # Adicione uma coluna 'TagComp' contendo o nome do arquivo sem a extensão
        df['nome_arquivo'] = os.path.splitext(arquivo)[0]

        # Adicione o DataFrame à lista
        dataframes.append(df)
# Concatene todos os DataFrames em um único DataFrame
BDadosTemp = pd.concat(dataframes, ignore_index=True)
BDadosTemp = BDadosTemp[(BDadosTemp != 0).all(axis=1)]
BDadosTemp['Value'] = BDadosTemp['Value'].round(1)
BDadosTemp = BDadosTemp.rename(columns={'Tag': 'Parametro'})
BDadosTemp = BDadosTemp.rename(columns={'nome_arquivo': 'NmeComp'})
#BDadosTemp.head()

# Filtrar as linhas onde a coluna 'NmeComp' é igual a 'MainPumpP2' ou 'MainPumpP1'
df_MainPumps = BDadosTemp[BDadosTemp['NmeComp'].isin(['MainPumpP2', 'MainPumpP1'])]
#df_MainPumps = df_MainPumps.query("Parametro == 'Temperature'")

# Utilize o método groupby para agrupar os dados por 'NmeComp' e, em seguida, aplique describe() a cada grupo
summary = df_MainPumps.groupby('NmeComp').describe()

# Exiba a sumarização dos dados
print(df_MainPumps)

# Filtrar os dados para MainPumpP1 e MainPumpP2
df_MainPumpP1 = df_MainPumps[df_MainPumps['NmeComp'] == 'MainPumpP1']
df_MainPumpP2 = df_MainPumps[df_MainPumps['NmeComp'] == 'MainPumpP2']

print(df_MainPumpP1)
print(df_MainPumpP2)

# Filtrar as linhas onde a coluna 'NmeComp' é igual a 'MainPumpP2' ou 'MainPumpP1'
df_MainPumpsTemp = df_MainPumps[df_MainPumps['Parametro'].isin(['Temperature'])]
#df_MainPumps = df_MainPumps.query("Parametro == 'Temperature'")

# Utilize o método groupby para agrupar os dados por 'NmeComp' e, em seguida, aplique describe() a cada grupo
summary = df_MainPumpsTemp.groupby('NmeComp').describe()

# Exiba a sumarização dos dados

print(summary)
print(df_MainPumpsTemp)

from scipy import stats

# Supondo que 'df_MainPumps' seja o seu DataFrame

# Filtrar os valores correspondentes a 'MainPumpP1' e 'MainPumpP2'
df_MainPumpsTempP1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']['Value']
df_MainPumpsTempP2 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP2']['Value']

# Calcular o Z-score para cada valor na Series de 'MainPumpP1'
z_scores_MainPumpsTempP1 = stats.zscore(df_MainPumpsTempP1)

# Definir um limite para o Z-score (por exemplo, 1 desvio padrão)
z_score_threshold = 0.2

# Encontrar os índices dos outliers para 'MainPumpP1'
outlier_indices_MainPumpsTempP1 = abs(z_scores_MainPumpsTempP1) > z_score_threshold

# Remover as linhas que contêm outliers para 'MainPumpP1'
dfZscore_MainPumpsTempP1 = df_MainPumpsTempP1[~outlier_indices_MainPumpsTempP1]

# Calcular o Z-score para cada valor na Series de 'MainPumpP2'
z_scores_MainPumpsTempP2 = stats.zscore(df_MainPumpsTempP2)

# Encontrar os índices dos outliers para 'MainPumpP2'
outlier_indices_MainPumpsTempP2 = abs(z_scores_MainPumpsTempP2) > z_score_threshold

# Remover as linhas que contêm outliers para 'MainPumpP2'
dfZscore_MainPumpsTempP2 = df_MainPumpsTempP2[~outlier_indices_MainPumpsTempP2]

# Exibir os resultados
print("Valores sem outliers para 'MainPumpP1':")
print(dfZscore_MainPumpsTempP1)

print("\nValores sem outliers para 'MainPumpP2':")
print(dfZscore_MainPumpsTempP2)

import pandas as pd

# Supondo que 'df_MainPumps' seja o seu DataFrame

# Aqui está um exemplo de como você poderia definir o outlier_threshold usando o intervalo interquartil (IQR):
Q1 = df_MainPumpsTemp['Value'].quantile(0.25)
Q3 = df_MainPumpsTemp['Value'].quantile(0.75)
IQR = Q3 - Q1

# Definir o limite para considerar algo como outlier
outlier_threshold = 0.2  # Pode ajustar conforme necessário

# Calcular os limites inferior e superior para identificar outliers
lower_bound = Q1 - outlier_threshold * IQR
upper_bound = Q3 + outlier_threshold * IQR

# Filtrar os outliers sem remover os valores maximos superiores
df_filtered = df_MainPumpsTemp[(df_MainPumpsTemp['Value'] >= lower_bound)]

# Separar os resultados para os componentes 'MainPumpP1' e 'MainPumpP2'
df_MainPumpsTempP1IQR = df_filtered[df_filtered['NmeComp'] == 'MainPumpP1']
df_MainPumpsTempP2IQR = df_filtered[df_filtered['NmeComp'] == 'MainPumpP2']

# Coletar e registrar os valores mínimos e máximos para 'MainPumpP1'
min_MainPumpsTempP1 = df_MainPumpsTempP1IQR['Value'].min()
max_MainPumpsTempP1 = df_MainPumpsTempP1IQR['Value'].max()

# Coletar e registrar os valores mínimos e máximos para 'MainPumpP2'
min_MainPumpsTempP2 = df_MainPumpsTempP2IQR['Value'].min()
max_MainPumpsTempP2 = df_MainPumpsTempP2IQR['Value'].max()

# Exibir os valores mínimos e máximos coletados e registrados
print("Valores mínimos e máximos para MainPumpP1:")
print("Mínimo:", min_MainPumpsTempP1)
print("Máximo:", max_MainPumpsTempP1)

print("\nValores mínimos e máximos para MainPumpP2:")
print("Mínimo:", min_MainPumpsTempP2)
print("Máximo:", max_MainPumpsTempP2)

# Imprimir resumo estatístico do DataFrame limpo para 'MainPumpP1'
print("Resumo Estatístico para MainPumpP1:")
print(df_MainPumpsTempP1IQR.describe())

# Imprimir resumo estatístico do DataFrame limpo para 'MainPumpP2'
print("\nResumo Estatístico para MainPumpP2:")
print(df_MainPumpsTempP2IQR.describe())

import pandas as pd
import matplotlib.pyplot as plt

# Supondo que df_MainPumpsTemp seja o DataFrame contendo os dados
# Filtrando os dados para MainPumpP1 e MainPumpP2
data_p1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']['Value']
data_p2 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP2']['Value']

# Configurando o layout dos subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Plotando o histograma para MainPumpP1
n1, bins1, patches1 = axes[0].hist(data_p1, bins=30, color='blue', alpha=0.5)
axes[0].set_title('Histograma MainPumpP1')
axes[0].set_xlabel('Frequência Ocorrência MainPumpP1')
axes[0].set_ylabel('Valor de Temperatura')

# Adicionando os valores das barras para MainPumpP1
for rect1 in patches1:
    height1 = rect1.get_height()
    axes[0].text(rect1.get_x() + rect1.get_width()/2., height1, '%d' % int(height1),
            ha='center', va='bottom')

# Plotando o histograma para MainPumpP2
n2, bins2, patches2 = axes[1].hist(data_p2, bins=30, color='red', alpha=0.5)
axes[1].set_title('Histograma MainPumpP2')
axes[1].set_xlabel('Frequência Ocorrência MainPumpP2')
axes[1].set_ylabel('Valor de Temperatura')

# Adicionando os valores das barras para MainPumpP2
for rect2 in patches2:
    height2 = rect2.get_height()
    axes[1].text(rect2.get_x() + rect2.get_width()/2., height2, '%d' % int(height2),
            ha='center', va='bottom')

# Ajustando o layout
plt.tight_layout()

# Exibindo os subplots
plt.show()

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

df_MainPumpsTempP1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']

# Selecionar o parâmetro 'Value' como feature (X) e 'NmeComp' como o alvo (y)
X = df_MainPumpsTemp[['Value']].values
y = df_MainPumpsTemp['NmeComp']

# Dividir os dados em conjunto de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criar e treinar o modelo de árvore de decisão
tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = tree_clf.predict(X_test)

# Calcular a precisão do modelo
accuracy = accuracy_score(y_test, y_pred)
print("Precisão do modelo:", accuracy)

from sklearn.tree import export_graphviz
import graphviz

# Exportar a árvore de decisão para um arquivo DOT
export_graphviz(tree_clf, out_file="tree.dot",
                feature_names=["Value"],
                class_names=df_MainPumpsTemp['NmeComp'].unique(),
                filled=True, rounded=True)

# Converter o arquivo DOT em um formato visual (por exemplo, PNG)
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
#dot_graph.render(output_path, format='png', cleanup=True)

df_MainPumps_Trans = df_MainPumpsTemp.pivot(index='Timestamp', columns='NmeComp', values='Value').reset_index()
#BDados_Temp_Trans = BDados_Temp_Trans.drop(columns='NmeComp')
#NmeComp	Timestamp	Value
# Visualizar o DataFrame resultante
df_MainPumps_Trans.head()

import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Criar um novo dataframe com duas colunas do dataframe sem NA
BDadosTemp_MainPump = df_MainPumps_Trans[['MainPumpP1', 'MainPumpP2', 'Timestamp']].dropna()

# Limitar os valores de temperatura a 125
BDadosTemp_MainPump['MainPumpP1'] = BDadosTemp_MainPump['MainPumpP1'].apply(lambda x: min(x, 125))
BDadosTemp_MainPump['MainPumpP2'] = BDadosTemp_MainPump['MainPumpP2'].apply(lambda x: min(x, 125))

# Criar subplots com plotly
fig_BDadosTemp_MainPump = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)MainPumpP1', '(C°)MainPumpP2'])

# Adicionar traces para MainPumpP1
fig_BDadosTemp_MainPump.add_trace(go.Scatter(x=BDadosTemp_MainPump['Timestamp'], y=BDadosTemp_MainPump['MainPumpP1'],
                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP1'),
              row=1, col=1)

# Adicionar traces para MainPumpP2
fig_BDadosTemp_MainPump.add_trace(go.Scatter(x=BDadosTemp_MainPump['Timestamp'], y=BDadosTemp_MainPump['MainPumpP2'],
                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP2'),
              row=2, col=1)

# Definir limite máximo para o eixo y como 125
fig_BDadosTemp_MainPump.update_yaxes(range=[0, 125], row=1, col=1)
fig_BDadosTemp_MainPump.update_yaxes(range=[0, 125], row=2, col=1)

# Atualizar layout com títulos personalizados
fig_BDadosTemp_MainPump.update_layout(title_text='Temperatura MainPumpP1 e MainPumpP2 ao longo do tempo',
                  showlegend=False)  # Desativar a legenda global

# Exibir o gráfico interativo
fig_BDadosTemp_MainPump.show()

import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots

BDadosTemp_MainPumpAjus = BDadosTemp_MainPump[
    (BDadosTemp_MainPump['MainPumpP1'] >= min_MainPumpsTempP1) &
    (BDadosTemp_MainPump['MainPumpP1'] <= max_MainPumpsTempP1) &
    (BDadosTemp_MainPump['MainPumpP2'] >= min_MainPumpsTempP2) &
    (BDadosTemp_MainPump['MainPumpP2'] <= max_MainPumpsTempP2)
]

# Criar subplots com plotly
fig_BDadosTemp_MainPumpAjus = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)MainPumpP1', '(C°)MainPumpP2'])

# Adicionar traces para MainPumpP1
fig_BDadosTemp_MainPumpAjus.add_trace(go.Scatter(x=BDadosTemp_MainPumpAjus['Timestamp'], y=BDadosTemp_MainPumpAjus['MainPumpP1'],
                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP1'),
              row=1, col=1)

# Adicionar traces para MainPumpP2
fig_BDadosTemp_MainPumpAjus.add_trace(go.Scatter(x=BDadosTemp_MainPumpAjus['Timestamp'], y=BDadosTemp_MainPumpAjus['MainPumpP2'],
                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP2'),
              row=2, col=1)

# Definir limite máximo para o eixo y como 125
fig_BDadosTemp_MainPumpAjus.update_yaxes(range=[0, 125], row=1, col=1)
fig_BDadosTemp_MainPumpAjus.update_yaxes(range=[0, 125], row=2, col=1)

# Atualizar layout com títulos personalizados
fig_BDadosTemp_MainPumpAjus.update_layout(title_text='Temperatura MainPumpP1 e MainPumpP2 ao longo do tempo',
                  showlegend=False)  # Desativar a legenda global

# Exibir o gráfico interativo
fig_BDadosTemp_MainPumpAjus.show()

import pandas as pd
from sklearn.linear_model import LinearRegression

#Separação dos BDados de temperatura das Bombas P1 e P2
df_MainPumpsTemp_IQR = pd.concat([df_MainPumpsTempP1IQR, df_MainPumpsTempP2IQR])

# Separando os dados de temperatura e timestamp
temperatura = df_MainPumpsTemp_IQR['Value'].values.reshape(-1, 1)  # Reshape para uma matriz 2D
timestamp = df_MainPumpsTemp_IQR.index.astype(int).values.reshape(-1, 1)  # Reshape para uma matriz 2D

# Criando e treinando o modelo de regressão linear
modelo = LinearRegression()
modelo.fit(timestamp, temperatura)

# Calculando as previsões da tendência linear
previsao_tendencia = modelo.predict(timestamp)

# Visualizando os coeficientes da reta de regressão
coef_angular = modelo.coef_[0][0]
intercepto = modelo.intercept_[0]

print("Coeficiente Angular (Inclinação):", coef_angular)
print("Intercepto:", intercepto)

import pandas as pd
from sklearn.linear_model import LinearRegression

# Separação dos dados de temperatura das Bombas P1 e P2
df_MainPumpsTemp_IQR = pd.concat([df_MainPumpsTempP1IQR])

# Dividindo os dados em 4 parcelas de 25%
num_splits = 5
split_size = len(df_MainPumpsTempP1IQR) // num_splits

mod_coef_angular_list = []

for i in range(num_splits):
    start_index = i * split_size
    end_index = start_index + split_size if i < num_splits - 1 else None
    split_df = df_MainPumpsTempP1IQR.iloc[start_index:end_index]

    # Separando os dados de temperatura e timestamp
    temperatura = split_df['Value'].values.reshape(-1, 1)  # Reshape para uma matriz 2D
    timestamp = split_df.index.astype(int).values.reshape(-1, 1)  # Reshape para uma matriz 2D

    # Criando e treinando o modelo de regressão linear
    modelo = LinearRegression()
    modelo.fit(timestamp, temperatura)

    # Visualizando o coeficiente angular da reta de regressão
    coef_angular = modelo.coef_[0][0]
    mod_coef_angular = abs(coef_angular)
    mod_coef_angular_list.append(mod_coef_angular)

    print(f"Parcela {i + 1}: Módulo do Coeficiente Angular: {mod_coef_angular}")

# Registre os resultados conforme necessário

df_MainPumpsTemp.head()

import pandas as pd
from statsmodels.tsa.stattools import adfuller

# Supondo que df_MainPumpsTemp contenha a série temporal com as colunas 'Parametro', 'Timestamp', 'Value', 'NmeComp'
df_MainPumpsTempP2 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP2']
# Primeiro, vamos filtrar a série temporal desejada (por exemplo, usando a coluna 'Parametro' para selecionar)
serie_temporalP2 = df_MainPumpsTempP2['Value']

# Função para testar estacionariedade da série temporal
def test_stationarity(timeseries):
    # Verificar se a série temporal não está vazia
    if timeseries.empty:
        print("Série temporal está vazia.")
        return

    # Estatísticas de rolamento
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    # Plotar estatísticas de rolamento
    import matplotlib.pyplot as plt
    plt.plot(timeseries, color='blue',label='Original')
    plt.plot(rolmean, color='red', label='Média Móvel')
    plt.plot(rolstd, color='black', label = 'Desvio Padrão Móvel')
    plt.legend(loc='best')
    plt.title('Estatísticas de Rolamento')
    plt.show()

    # Teste de Dickey-Fuller:
    print('Resultados do Teste Dickey-Fuller:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Estatística do Teste','Valor-p','#Lags Usados','Número de Observações Usadas'])
    for key,value in dftest[4].items():
        dfoutput['Valor Crítico (%s)'%key] = value
    print(dfoutput)

# Aplicar teste de estacionariedade à série temporal selecionada
test_stationarity(serie_temporalP2)

import pandas as pd
from statsmodels.tsa.stattools import adfuller

# Supondo que df_MainPumpsTemp contenha a série temporal com as colunas 'Parametro', 'Timestamp', 'Value', 'NmeComp'
df_MainPumpsTempP1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']
# Primeiro, vamos filtrar a série temporal desejada (por exemplo, usando a coluna 'Parametro' para selecionar)
serie_temporalP1 = df_MainPumpsTempP1['Value']

# Função para testar estacionariedade da série temporal
def test_stationarity(timeseries):
    # Verificar se a série temporal não está vazia
    if timeseries.empty:
        print("Série temporal está vazia.")
        return

    # Estatísticas de rolamento
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    # Plotar estatísticas de rolamento
    import matplotlib.pyplot as plt
    plt.plot(timeseries, color='blue',label='Original')
    plt.plot(rolmean, color='red', label='Média Móvel')
    plt.plot(rolstd, color='black', label = 'Desvio Padrão Móvel')
    plt.legend(loc='best')
    plt.title('Estatísticas de Rolamento')
    plt.show()

    # Teste de Dickey-Fuller:
    print('Resultados do Teste Dickey-Fuller:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Estatística do Teste','Valor-p','#Lags Usados','Número de Observações Usadas'])
    for key,value in dftest[4].items():
        dfoutput['Valor Crítico (%s)'%key] = value
    print(dfoutput)

# Aplicar teste de estacionariedade à série temporal selecionada
test_stationarity(serie_temporalP1)

import pandas as pd
import numpy as np
df_MainPumpsTemp['Timestamp'] = pd.to_datetime(df_MainPumpsTemp['Timestamp'])

# Criar um novo DataFrame com a coluna de data (sem o tempo) adicionada
df_MainPumpsTempDate = df_MainPumpsTemp.copy()  # Copiar o DataFrame original para manter os dados originais
df_MainPumpsTempDate['Date'] = df_MainPumpsTemp['Timestamp'].dt.date

# Agrupar pelos parâmetros e datas, calculando a média dos valores
df_MainPumpsTempDate = df_MainPumpsTempDate.groupby(['Parametro', 'Date']).agg({'Value': 'mean'}).reset_index()

# Renomear a coluna 'Value' para 'MeanValue' (média do valor)
df_MainPumpsTempDate = df_MainPumpsTempDate.rename(columns={'Value': 'MeanValue'})

# Exibir o novo DataFrame agrupado
print(df_MainPumpsTempDate)

df_MainPumpsTemp.head()

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from statsmodels.tsa.arima.model import ARIMA
from scipy.stats import linregress

# Supondo que df contém os dados de MeanValue com colunas Parametro, Date e MeanValue
df = df_MainPumpsTempP1

# Inicializar a figura
fig = go.Figure()

# Adicionar os dados originais ao gráfico
for parametro in df['Parametro'].unique():
    df_parametro = df[df['Parametro'] == parametro]
    fig.add_trace(go.Scatter(x=df_parametro['Timestamp'], y=df_parametro['Value'], mode='lines', name=f'Dados Originais - {parametro}'))

# Iterar sobre cada parâmetro
for parametro in df['Parametro'].unique():
    # Filtrar os dados para o parâmetro atual
    df_parametro = df[df['Parametro'] == parametro]

    # Ajustar o modelo ARIMA aos dados históricos
    order = (5, 1, 0)  # Ajuste conforme necessário
    model = ARIMA(df_parametro['Value'], order=order)
    arima_model = model.fit()

    # Fazer previsões para o período futuro (próximos 50 dias)
    n_days_future = 50
    future_dates = pd.date_range(start=df_parametro['Timestamp'].max(), periods=n_days_future + 1)[1:]  # Iniciar após o último dia conhecido
    future_values = []

    # Calcular a projeção para cada dia futuro usando o coeficiente angular
    for i in range(n_days_future):
        slope, intercept, r_value, p_value, std_err = linregress(range(len(df_parametro)), df_parametro['Value'])
        future_value = df_parametro['Value'].iloc[-1] + slope * (i + 1)
        future_values.append(future_value)

    # Criar DataFrame com as projeções para o parâmetro atual
    df_proj_parametro = pd.DataFrame({'Timestamp': future_dates, 'Value': future_values})

    # Adicionar as projeções ao gráfico
    fig.add_trace(go.Scatter(x=df_proj_parametro['Timestamp'], y=df_proj_parametro['Value'], mode='lines', name=f'Projeção - {parametro}'))

# Adicionar layout ao gráfico
fig.update_layout(title='Projeções com ARIMA para os próximos 50 dias',
                  xaxis_title='Timestamp',
                  yaxis_title='Valor Médio',
                  template='plotly_white')

# Exibir o gráfico
fig.show()

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from statsmodels.tsa.arima.model import ARIMA
from scipy.stats import linregress

# Supondo que df contém os dados de MeanValue com colunas Parametro, Date e MeanValue
df = df_MainPumpsTempP1

# Inicializar a figura
fig = go.Figure()

# Adicionar os dados originais ao gráfico
for parametro in df['Parametro'].unique():
    df_parametro = df[df['Parametro'] == parametro]
    fig.add_trace(go.Scatter(x=df_parametro['Timestamp'], y=df_parametro['Value'], mode='lines', name=f'Dados Originais - {parametro}'))

# Iterar sobre cada parâmetro
for parametro in df['Parametro'].unique():
    # Filtrar os dados para o parâmetro atual
    df_parametro = df[df['Parametro'] == parametro]

    # Ajustar o modelo ARIMA aos dados históricos
    order = (5, 1, 0)  # Ajuste conforme necessário
    model = ARIMA(df_parametro['Value'], order=order)
    arima_model = model.fit()

    # Fazer previsões para o período futuro (próximos 50 dias)
    n_days_future = 50
    future_dates = pd.date_range(start=df_parametro['Timestamp'].max(), periods=n_days_future + 1)[1:]  # Iniciar após o último dia conhecido
    future_values = []

    # Calcular a projeção para cada dia futuro usando o coeficiente angular
    for i in range(n_days_future):
        slope, intercept, r_value, p_value, std_err = linregress(range(len(df_parametro)), df_parametro['Value'])
        future_value = df_parametro['Value'].iloc[-1] + slope * (i + 1)
        future_values.append(future_value)

    # Criar DataFrame com as projeções para o parâmetro atual
    df_proj_parametro = pd.DataFrame({'Timestamp': future_dates, 'Value': future_values})

    # Adicionar as projeções ao gráfico
    fig.add_trace(go.Scatter(x=df_proj_parametro['Timestamp'], y=df_proj_parametro['Value'], mode='lines', name=f'Projeção - {parametro}'))

# Adicionar layout ao gráfico
fig.update_layout(title='Projeções com ARIMA para os próximos 50 dias',
                  xaxis_title='Timestamp',
                  yaxis_title='Valor Médio',
                  template='plotly_white')

# Exibir o gráfico
fig.show()

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error



# Converter a coluna 'Timestamp' para o tipo datetime
df_MainPumpsTempDate['Date'] = pd.to_datetime(df_MainPumpsTempDate['Date'])

# Definir o número de valores anteriores como 20% do volume de dados originais
n_prev_values = int(0.20 * len(df_MainPumpsTempDate))

# Criar features e target
X = []
y = []
for i in range(n_prev_values, len(df_MainPumpsTempDate)):
    X.append(df_MainPumpsTempDate['MeanValue'].values[i - n_prev_values:i])
    y.append(df_MainPumpsTempDate['MeanValue'].values[i])

X = np.array(X)
y = np.array(y)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar o modelo LinearRegression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Fazer previsões para o período futuro
# Supondo que você queira fazer previsões para os próximos 50 dias
n_days_future = 50
future_dates = pd.date_range(start=df_MainPumpsTempDate['Date'].iloc[-1], periods=n_days_future + 1)[1:]  # Iniciar após o último dia conhecido
last_values = X[-1]  # Últimos valores conhecidos
future_values = []

for _ in range(n_days_future):
    # Fazer previsão para o próximo dia
    next_value = linear_model.predict([last_values])[0]
    future_values.append(next_value)

    # Atualizar os últimos valores conhecidos para incluir a nova previsão
    last_values = np.roll(last_values, -1)
    last_values[-1] = next_value

# Calcular os limites mínimo e máximo para o intervalo de confiança
y_pred = linear_model.predict(X_test)
std_residuals = np.std(y_test - y_pred)  # Desvio padrão dos resíduos
z_critical = 1.96  # Para intervalo de confiança de 95%
lower_bound = np.array(future_values) - z_critical * std_residuals
upper_bound = np.array(future_values) + z_critical * std_residuals

# Criar DataFrame com as previsões
df_future = pd.DataFrame({'Date': future_dates, 'MeanValue': future_values, 'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound})

# Criar o gráfico interativo com Plotly
fig = go.Figure()

# Adicionar os dados originais
fig.add_trace(go.Scatter(x=df_MainPumpsTempDate['Date'], y=df_MainPumpsTempDate['MeanValue'], mode='lines', name='Dados Originais'))

# Adicionar a projeção e o intervalo de confiança
fig.add_trace(go.Scatter(x=df_future['Date'], y=df_future['MeanValue'], mode='lines', name='Projeção'))
fig.add_trace(go.Scatter(x=df_future['Date'], y=df_future['Lower_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), name='Limite Inferior'))
fig.add_trace(go.Scatter(x=df_future['Date'], y=df_future['Upper_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', name='Limite Superior'))

# Personalizar o layout
fig.update_layout(title='Projeção de Temperatura com Intervalo de Confiança para Bomba P1',
                  xaxis_title='Date',
                  yaxis_title='MeanValue',
                  hovermode='x',
                  template='plotly_white')

# Exibir o gráfico
fig.show()

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Converter a coluna 'Timestamp' para o tipo datetime
df_MainPumpsTempDate['Date'] = pd.to_datetime(df_MainPumpsTempDate['Date'])

# Definir o número de valores anteriores como 20% do volume de dados originais
n_prev_values = int(0.20 * len(df_MainPumpsTempDate))

# Criar features e target
X = []
y = []
for i in range(n_prev_values, len(df_MainPumpsTempDate)):
    X.append(df_MainPumpsTempDate['MeanValue'].values[i - n_prev_values:i])
    y.append(df_MainPumpsTempDate['MeanValue'].values[i])

X = np.array(X)
y = np.array(y)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar o modelo LinearRegression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Fazer previsões para o período futuro
# Supondo que você queira fazer previsões para os próximos 50 dias
n_days_future = 50
future_dates = pd.date_range(start=df_MainPumpsTempDate['Date'].iloc[-1], periods=n_days_future + 1)[1:]  # Iniciar após o último dia conhecido
last_values = X[-1]  # Últimos valores conhecidos
future_values = []

for _ in range(n_days_future):
    # Fazer previsão para o próximo dia
    next_value = linear_model.predict([last_values])[0]
    future_values.append(next_value)

    # Atualizar os últimos valores conhecidos para incluir a nova previsão
    last_values = np.roll(last_values, -1)
    last_values[-1] = next_value

# Calcular os limites mínimo e máximo para o intervalo de confiança
y_pred = linear_model.predict(X_test)
std_residuals = np.std(y_test - y_pred)  # Desvio padrão dos resíduos
z_critical = 1.96  # Para intervalo de confiança de 95%
lower_bound = np.array(future_values) - z_critical * std_residuals
upper_bound = np.array(future_values) + z_critical * std_residuals

# Criar DataFrame com as previsões
df_future = pd.DataFrame({'Date': future_dates, 'MeanValue': future_values, 'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound})

# Criar o gráfico interativo com Plotly
fig = go.Figure()

# Adicionar os dados originais
fig.add_trace(go.Scatter(x=df_MainPumpsTempDate['Date'], y=df_MainPumpsTempDate['MeanValue'], mode='lines', name='Dados Originais'))

# Adicionar a projeção e o intervalo de confiança
fig.add_trace(go.Scatter(x=df_future['Date'], y=df_future['MeanValue'], mode='lines', name='Projeção'))
fig.add_trace(go.Scatter(x=df_future['Date'], y=df_future['Lower_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), name='Limite Inferior'))
fig.add_trace(go.Scatter(x=df_future['Date'], y=df_future['Upper_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', name='Limite Superior'))

# Personalizar o layout
fig.update_layout(title='Projeção de Temperatura com Intervalo de Confiança para Bomba P2/P1',
                  xaxis_title='Date',
                  yaxis_title='MeanValue',
                  hovermode='x',
                  template='plotly_white')

# Exibir o gráfico
fig.show()

df_MainPumpsTemp.head()

import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Supondo que df_MainPumpsTempP1IQR e df_MainPumpsTemp_IQR já estejam definidos

# Criar DataFrame vazio com as colunas especificadas
df_results_Correlacao = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Calcular a correlação entre as séries temporais
correlation = df_MainPumpsTempP1IQR['Value'].corr(df_MainPumpsTemp_IQR['Value'].shift(1))

# Calcular as métricas de erro (MAE, MSE, R^2)
y_true = df_MainPumpsTempP1IQR['Value'].iloc[1:]  # Remover o primeiro valor, pois não há valor anterior para comparar
y_pred = df_MainPumpsTempP1IQR['Value'].shift(1).iloc[1:]  # Remover o primeiro valor, pois não há valor anterior para comparar

mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['Correlação'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results
df_results_Correlacao = pd.concat([df_results_Correlacao, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_Correlacao)

import pandas as pd
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Converter a coluna 'Timestamp' para o tipo datetime, se necessário
df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])

# Criar DataFrame para armazenar os resultados
df_results_SVR = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Definir as features (X) e o target (y)
X = df_MainPumpsTemp_IQR['Timestamp'].values.reshape(-1, 1)  # Feature é o timestamp
y = df_MainPumpsTemp_IQR['Value']

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar o modelo SVR
svr_model = SVR(kernel='rbf')  # Use o kernel 'rbf' para SVR
svr_model.fit(X_train, y_train)

# Fazer previsões com o modelo SVR
y_pred_svr = svr_model.predict(X_test)

# Avaliar o desempenho do modelo SVR
mae = mean_absolute_error(y_test, y_pred_svr)
mse = mean_squared_error(y_test, y_pred_svr)
r2 = r2_score(y_test, y_pred_svr)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['SVR'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results_SVR
df_results_SVR = pd.concat([df_results_SVR, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_SVR)

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Converter a coluna 'Timestamp' para o tipo datetime, se necessário
df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])

# Criar DataFrame para armazenar os resultados
df_results_ARIMA = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Definir os dados de treinamento e teste
train_size = int(len(df_MainPumpsTemp_IQR) * 0.8)  # 80% dos dados para treinamento
train_data = df_MainPumpsTemp_IQR['Value'].iloc[:train_size]
test_data = df_MainPumpsTemp_IQR['Value'].iloc[train_size:]

# Ajustar o modelo ARIMA aos dados de treinamento
order = (5, 1, 0)  # Parâmetros p, d e q do ARIMA (ajuste conforme necessário)
model = ARIMA(train_data, order=order)
arima_model = model.fit()

# Fazer previsões com o modelo ajustado
start_index = len(train_data)
end_index = start_index + len(test_data) - 1
predictions = arima_model.predict(start=start_index, end=end_index, typ='levels')

# Calcular as métricas de erro (MAE, MSE, R^2)
mae = mean_absolute_error(test_data, predictions)
mse = mean_squared_error(test_data, predictions)
r2 = r2_score(test_data, predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['ARIMA'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results_ARIMA
df_results_ARIMA = pd.concat([df_results_ARIMA, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_ARIMA)

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Converter a coluna 'Timestamp' para o tipo datetime, se necessário
df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])

# Criar DataFrame para armazenar os resultados
df_results_RLM = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Converter a coluna 'Timestamp' para um formato numérico (por exemplo, número de dias desde o início da época)
df_MainPumpsTemp_IQR['NumericTimestamp'] = df_MainPumpsTemp_IQR['Timestamp'].astype(int) / 10**9 / 86400  # Converter nanossegundos para dias

# Definir as features (X) e o target (y)
X = df_MainPumpsTemp_IQR[['NumericTimestamp']]  # Feature é o timestamp numérico
y = df_MainPumpsTemp_IQR['Value']

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ajustar o modelo de regressão linear aos dados de treinamento
model = LinearRegression()
model.fit(X_train, y_train)

# Fazer previsões com o modelo ajustado
y_pred = model.predict(X_test)

# Calcular as métricas de erro (MAE, MSE, R^2)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['Regressão Linear Múltipla'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results_RLM
df_results_RLM = pd.concat([df_results_RLM, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_RLM)

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Inicializar o DataFrame para armazenar os resultados
df_results_ModelAR = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])


# Ajustar o modelo AR aos dados de temperatura
lags = 1  # Especificando o número de lags
X = df_MainPumpsTemp_IQR['Value'].shift(lags).dropna()  # Variável de entrada (com lag)
y = df_MainPumpsTemp_IQR['Value'][lags:]  # Variável de saída (sem lag)

# Dividir os dados em conjuntos de treinamento e teste
split_index = int(len(X) * 0.8)  # 80% dos dados para treinamento
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Ajustar o modelo AR aos dados de treinamento
model = sm.OLS(y_train, sm.add_constant(X_train))
ar_model = model.fit()

# Fazer previsões com o modelo ajustado
predictions = ar_model.predict(sm.add_constant(X_test))

# Calcular as métricas de erro (MAE, MSE, R^2)
mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['Model AR'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Adicionar as métricas de erro ao DataFrame df_results
df_results_ModelAR = pd.concat([df_results_ModelAR, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_ModelAR)

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Definir o tamanho da janela da Média Móvel
window_size = 7  # Por exemplo, usar uma janela de 7 dias

# Calcular a Média Móvel
df_MainPumpsTemp_IQR['Moving_Average'] = df_MainPumpsTemp_IQR['Value'].rolling(window=window_size).mean()

# Inicializar o DataFrame para armazenar os resultados
df_results_MediaMovel = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Remover os valores nulos resultantes da Média Móvel
df_MainPumpsTemp_IQR.dropna(inplace=True)

# Calcular as métricas de erro (MAE, MSE, R²)
mae = mean_absolute_error(df_MainPumpsTemp_IQR['Value'], df_MainPumpsTemp_IQR['Moving_Average'])
mse = mean_squared_error(df_MainPumpsTemp_IQR['Value'], df_MainPumpsTemp_IQR['Moving_Average'])
r2 = r2_score(df_MainPumpsTemp_IQR['Value'], df_MainPumpsTemp_IQR['Moving_Average'])

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['MediaMovel'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Adicionar as métricas de erro ao DataFrame df_results
df_results_MediaMovel = pd.concat([df_results_MediaMovel, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_MediaMovel)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Carregar os dados
# Suponha que você tenha um DataFrame df_MainPumpsTemp_IQR com as colunas 'Timestamp' e 'Value'

# Inicializar o DataFrame para armazenar os resultados
df_results_RNNLSTM = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Normalizar os dados
scaler = MinMaxScaler()
df_MainPumpsTemp_IQR['Value'] = scaler.fit_transform(df_MainPumpsTemp_IQR[['Value']])

# Função para preparar os dados em sequências para RNN
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

# Definir o comprimento da sequência (número de passos de tempo)
seq_length = 10

# Criar sequências de dados
X, y = create_sequences(df_MainPumpsTemp_IQR['Value'].values, seq_length)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Construir o modelo RNN
model = Sequential([
    LSTM(units=15, input_shape=(X_train.shape[1], 1)),
    Dense(1)
])

# Compilar o modelo
model.compile(optimizer='adam', loss='mean_squared_error')

# Treinar o modelo
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# Fazer previsões
predictions = model.predict(X_test)

# Calcular as métricas de erro
mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['RNNLSTM'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Adicionar as métricas de erro ao DataFrame df_results
df_results_RNNLSTM = pd.concat([df_results_RNNLSTM, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_RNNLSTM)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Carregar os dados
# Suponha que você tenha um DataFrame df_MainPumps com as colunas 'Timestamp' e 'Value'

# Inicializar o DataFrame para armazenar os resultados
df_results_RNNGRU = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Normalizar os dados
scaler = MinMaxScaler()
df_MainPumpsTemp_IQR['Value'] = scaler.fit_transform(df_MainPumpsTemp_IQR[['Value']])

# Função para preparar os dados em sequências para GRU
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i+seq_length])
        y.append(data[i+seq_length])
    return np.array(X), np.array(y)

# Definir o comprimento da sequência (número de passos de tempo)
seq_length = 10

# Criar sequências de dados
X, y = create_sequences(df_MainPumpsTemp_IQR['Value'].values, seq_length)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Construir o modelo GRU
model = Sequential([
    GRU(units=15, input_shape=(X_train.shape[1], 1)),
    Dense(1)
])

# Compilar o modelo
model.compile(optimizer='adam', loss='mean_squared_error')

# Treinar o modelo
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# Fazer previsões
predictions = model.predict(X_test)

# Calcular as métricas de erro
mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['RNNGRU'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Adicionar as métricas de erro ao DataFrame df_results
df_results_RNNGRU = pd.concat([df_results_RNNGRU, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_RNNGRU)

import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Inicializar o DataFrame para armazenar os resultados
df_results_StateSpaceModel = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Definindo as datas como índice
df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])
df_MainPumpsTemp_IQR.set_index('Timestamp', inplace=True)

# Dividindo os dados em treinamento e teste
train_data, test_data = train_test_split(df_MainPumpsTemp_IQR, test_size=0.2, shuffle=False)

# Ajustando o modelo State Space Model (SSM)
model = sm.tsa.UnobservedComponents(train_data['Value'], 'local linear trend')
results = model.fit()

# Fazendo previsões
predictions = results.forecast(steps=len(test_data))

# Calculando as métricas de erro
mae = mean_absolute_error(test_data['Value'], predictions)
mse = mean_squared_error(test_data['Value'], predictions)
r2 = r2_score(test_data['Value'], predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['State Space Model'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results_RLM
df_results_StateSpaceModel = pd.concat([df_results_StateSpaceModel, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_StateSpaceModel)

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Inicializar o DataFrame para armazenar os resultados
df_results_ExponentialSmoothing = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Ajustar o modelo de suavização exponencial aos dados de temperatura
model = ExponentialSmoothing(df_MainPumpsTemp_IQR['Value'])
exp_smoothing_model = model.fit()

# Fazer previsões com o modelo ajustado
predictions = exp_smoothing_model.predict(start=len(df_MainPumpsTemp_IQR), end=len(df_MainPumpsTemp_IQR) + len(X_test) - 1)

# Calcular as métricas de erro (MAE, MSE, R^2)
mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['Exponential Smoothing'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results_RLM
df_results_ExponentialSmoothing = pd.concat([df_results_ExponentialSmoothing, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_ExponentialSmoothing)

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Inicializar o DataFrame para armazenar os resultados
df_results_SARIMAX = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])

# Ajustar o modelo SARIMAX aos dados de temperatura
order = (1, 0, 1)  # Ordem do modelo SARIMA (p, d, q)
seasonal_order = (1, 1, 1, 12)  # Ordem sazonal do modelo SARIMA (P, D, Q, S)

model = SARIMAX(df_MainPumpsTemp_IQR['Value'], order=order, seasonal_order=seasonal_order)
sarimax_model = model.fit()

# Fazer previsões com o modelo ajustado
predictions = sarimax_model.predict(start=len(df_MainPumpsTemp_IQR), end=len(df_MainPumpsTemp_IQR) + len(X_test) - 1)

# Calcular as métricas de erro (MAE, MSE, R^2)
mae = mean_absolute_error(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Criar um DataFrame temporário com os novos dados
new_row = pd.DataFrame({'Modelo': ['SARIMAX'], 'MAE': [mae], 'MSE': [mse], 'R2': [r2]})

# Usar pd.concat para adicionar a nova linha ao DataFrame df_results_RLM
df_results_SARIMAX = pd.concat([df_results_SARIMAX, new_row], ignore_index=True)

# Exibir o DataFrame df_results
print(df_results_SARIMAX)

import pandas as pd

# Suponha que você tenha os DataFrames contendo os resultados de diferentes modelos
df_results = pd.concat([df_results_StateSpaceModel,
                        df_results_SARIMAX,
                        df_results_RNNGRU,
                        df_results_RNNLSTM,
                        df_results_RLM,
                        df_results_MediaMovel,
                        df_results_ModelAR,
                        df_results_ARIMA,
                        df_results_SVR,
                        df_results_ExponentialSmoothing,
                        df_results_Correlacao], ignore_index=True)

# Exibir o DataFrame resultante
print(df_results)
df_results.head(10)

import pandas as pd
from github import Github
from io import BytesIO

# Defina suas credenciais do GitHub
seu_token = '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'
seu_usuario = 'CidClayQuirino'
seu_repositorio = 'rnn-component-lIfe-cycle'
# Dicionário de DataFrames com seus nomes originais
dataframes = {
    'df_MainPumpsTemp': df_MainPumpsTemp,
    'df_MainPumpsTemp_IQR': df_MainPumpsTemp_IQR,
    'df_MainPumps': df_MainPumps,
    'df_results': df_results,
    'BDadosTemp':BDadosTemp,
}

# Função para salvar e enviar para o GitHub
def salvar_e_enviar_para_github(dataframe, nome_arquivo, usuario, repositorio, token):
    # Salvar DataFrame como CSV em um BytesIO
    csv_bytes = BytesIO()
    dataframe.to_csv(csv_bytes, index=False)

    # Autenticar no GitHub
    g = Github(token)

    # Obter o repositório
    repo = g.get_user(usuario).get_repo(repositorio)

    # Criar ou atualizar o arquivo no repositório
    try:
        arquivo = repo.get_contents(nome_arquivo)
        repo.update_file(nome_arquivo, f'Atualizando {nome_arquivo}', csv_bytes.getvalue(), arquivo.sha)
        print(f'{nome_arquivo} atualizado com sucesso!')
    except Exception as e:
        repo.create_file(nome_arquivo, f'Adicionando {nome_arquivo}', csv_bytes.getvalue())
        print(f'{nome_arquivo} criado com sucesso!')

# Iterar sobre os DataFrames e salvá-los no GitHub
for nome, df in dataframes.items():
    nome_arquivo = f'{nome}.csv'  # Nome do arquivo usando o nome original do DataFrame
    salvar_e_enviar_para_github(df, nome_arquivo, seu_usuario, seu_repositorio, seu_token)

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Separação dos Dados de temperatura das Bombas P1 e P2

# Converter a coluna 'Timestamp' para o tipo datetime
df_MainPumpsTempP1['Timestamp'] = pd.to_datetime(df_MainPumpsTempP1['Timestamp'])

# Definir o número de valores anteriores como 20% do volume de dados originais
n_prev_values = int(0.20 * len(df_MainPumpsTempP1))

# Criar features e target
X = []
y = []
for i in range(n_prev_values, len(df_MainPumpsTempP1)):
    X.append(df_MainPumpsTempP1['Value'].values[i - n_prev_values:i])
    y.append(df_MainPumpsTempP1['Value'].values[i])

X = np.array(X)
y = np.array(y)

# Normalizar os dados de entrada
scaler = MinMaxScaler(feature_range=(0, 1))
X = scaler.fit_transform(X)

# Reformular os dados para o formato [samples, timesteps, features]
X = X.reshape(X.shape[0], X.shape[1], 1)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criar e compilar o modelo LSTM
model = Sequential()
model.add(LSTM(100, activation='tanh', input_shape=(X_train.shape[1], 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Treinar o modelo LSTM
model.fit(X_train, y_train, epochs=15, batch_size=32, verbose=1)

# Fazer previsões para o período futuro
n_days_future = 50
future_dates = pd.date_range(start=df_MainPumpsTempP1['Timestamp'].iloc[-1], periods=n_days_future + 1)[1:]  # Ignorar o primeiro dia, que já temos
last_values = X[-10]  # Últimos valores conhecidos
future_values = []

for _ in range(n_days_future):
    # Fazer previsão para o próximo dia
    next_value = model.predict(last_values.reshape(1, n_prev_values, 1))[0][0]
    future_values.append(next_value)

    # Atualizar os últimos valores conhecidos para incluir a nova previsão
    last_values = np.roll(last_values, -1)
    last_values[-1] = next_value

# Calcular os limites mínimo e máximo para o intervalo de confiança
y_pred = model.predict(X_test).flatten()
std_residuals = np.std(y_test - y_pred)  # Desvio padrão dos resíduos
z_critical = 1.8  # Para intervalo de confiança de 80%
lower_bound = future_values - z_critical * std_residuals
upper_bound = future_values + z_critical * std_residuals

# Criar DataFrame com as previsões
df_future = pd.DataFrame({'Timestamp': future_dates, 'Value': future_values, 'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound})

# Criar o gráfico interativo com Plotly
fig = go.Figure()

# Adicionar os dados originais
fig.add_trace(go.Scatter(x=df_MainPumpsTempP1['Timestamp'], y=df_MainPumpsTempP1['Value'], mode='lines', name='Dados Originais'))

# Adicionar a projeção e o intervalo de confiança
fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Value'], mode='lines', name='Projeção'))
fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Lower_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), name='Limite Inferior'))
fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Upper_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', name='Limite Superior'))

# Personalizar o layout
fig.update_layout(title='Projeção de Temperatura com Intervalo de Confiança (LSTM)',
                  xaxis_title='Timestamp',
                  yaxis_title='Value',
                  hovermode='x',
                  template='plotly_white')

# Exibir o gráfico
fig.show()

import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Separação dos Dados de temperatura das Bombas P1 e P2

# Converter a coluna 'Timestamp' para o tipo datetime
df_MainPumpsTempP2['Timestamp'] = pd.to_datetime(df_MainPumpsTempP2['Timestamp'])

# Definir o número de valores anteriores como 20% do volume de dados originais
n_prev_values = int(0.20 * len(df_MainPumpsTempP2))

# Criar features e target
X = []
y = []
for i in range(n_prev_values, len(df_MainPumpsTempP2)):
    X.append(df_MainPumpsTempP2['Value'].values[i - n_prev_values:i])
    y.append(df_MainPumpsTempP2['Value'].values[i])

X = np.array(X)
y = np.array(y)

# Normalizar os dados de entrada
scaler = MinMaxScaler(feature_range=(0, 1))
X = scaler.fit_transform(X)

# Reformular os dados para o formato [samples, timesteps, features]
X = X.reshape(X.shape[0], X.shape[1], 1)

# Dividir os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criar e compilar o modelo LSTM
model = Sequential()
model.add(LSTM(100, activation='tanh', input_shape=(X_train.shape[1], 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Treinar o modelo LSTM
model.fit(X_train, y_train, epochs=15, batch_size=32, verbose=1)

# Fazer previsões para o período futuro
n_days_future = 50
future_dates = pd.date_range(start=df_MainPumpsTempP2['Timestamp'].iloc[-1], periods=n_days_future + 1)[1:]  # Ignorar o primeiro dia, que já temos
last_values = X[-10]  # Últimos valores conhecidos
future_values = []

for _ in range(n_days_future):
    # Fazer previsão para o próximo dia
    next_value = model.predict(last_values.reshape(1, n_prev_values, 1))[0][0]
    future_values.append(next_value)

    # Atualizar os últimos valores conhecidos para incluir a nova previsão
    last_values = np.roll(last_values, -1)
    last_values[-1] = next_value

# Calcular os limites mínimo e máximo para o intervalo de confiança
y_pred = model.predict(X_test).flatten()
std_residuals = np.std(y_test - y_pred)  # Desvio padrão dos resíduos
z_critical = 1.8  # Para intervalo de confiança de 80%
lower_bound = future_values - z_critical * std_residuals
upper_bound = future_values + z_critical * std_residuals

# Criar DataFrame com as previsões
df_future = pd.DataFrame({'Timestamp': future_dates, 'Value': future_values, 'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound})

# Criar o gráfico interativo com Plotly
fig = go.Figure()

# Adicionar os dados originais
fig.add_trace(go.Scatter(x=df_MainPumpsTempP2['Timestamp'], y=df_MainPumpsTempP2['Value'], mode='lines', name='Dados Originais'))

# Adicionar a projeção e o intervalo de confiança
fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Value'], mode='lines', name='Projeção'))
fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Lower_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), name='Limite Inferior'))
fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Upper_Bound'], mode='lines', line=dict(width=0), marker=dict(color="#444"), fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', name='Limite Superior'))

# Personalizar o layout
fig.update_layout(title='Projeção de Temperatura com Intervalo de Confiança (LSTM)',
                  xaxis_title='Timestamp',
                  yaxis_title='Value',
                  hovermode='x',
                  template='plotly_white')

# Exibir o gráfico
fig.show()