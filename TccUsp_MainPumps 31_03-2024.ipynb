{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR36jqlYq8MFSMheBWXPCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CidClayQuirino/rnn-component-lIfe-cycle/blob/main/TccUsp_MainPumps%2031_03-2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markdown\n",
        "!pip install statsmodels\n",
        "!pip install scikit-learn\n",
        "!pip install PyGithub\n",
        "!pip install gitpython\n",
        "!pip install statsmodels\n",
        "!pip install dash\n",
        "!pip install xlwt\n",
        "!pip install openpyxl\n",
        "!pip install tensorflow\n",
        "# Atualizar pacotes\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from IPython.display import Image\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "uXQisK0UIQIr",
        "outputId": "17a09747-8e9e-4ab8-cdaa-982bd19128eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (3.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Collecting PyGithub\n",
            "  Downloading PyGithub-2.3.0-py3-none-any.whl (354 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.31.0)\n",
            "Collecting pyjwt[crypto]>=2.4.0 (from PyGithub)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.10.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.0.7)\n",
            "Collecting Deprecated (from PyGithub)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.2.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
            "Installing collected packages: pyjwt, Deprecated, pynacl, PyGithub\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "Successfully installed Deprecated-1.2.14 PyGithub-2.3.0 pyjwt-2.8.0 pynacl-1.5.0\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.25.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
            "Collecting dash\n",
            "  Downloading dash-2.16.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash) (3.0.1)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash) (7.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash) (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash) (2.31.0)\n",
            "Collecting retrying (from dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash) (67.7.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<3.1->dash) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash) (1.16.0)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, retrying, dash\n",
            "Successfully installed dash-2.16.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 retrying-1.3.4\n",
            "Collecting xlwt\n",
            "  Downloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlwt\n",
            "Successfully installed xlwt-1.3.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# URL do repositório no GitHub\n",
        "repo_url = 'https://github.com/CidClayQuirino/rnn-component-lIfe-cycle/archive/main.zip'\n",
        "dataframes = []\n",
        "\n",
        "# Baixe e extraia o arquivo zip do repositório\n",
        "response = requests.get(repo_url)\n",
        "with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "    zip_file.extractall()\n",
        "\n",
        "# Diretório onde os arquivos .xlsx foram extraídos\n",
        "extracted_dir = 'rnn-component-lIfe-cycle-main'\n",
        "\n",
        "# Loop pelos arquivos no diretório extraído\n",
        "for arquivo in os.listdir(extracted_dir):\n",
        "    if arquivo.endswith('.xlsx'):\n",
        "        # Construa o caminho completo para o arquivo\n",
        "        caminho_completo = os.path.join(extracted_dir, arquivo)\n",
        "\n",
        "        # Leia o arquivo Excel e adicione-o à lista de DataFrames\n",
        "        df = pd.read_excel(caminho_completo)\n",
        "\n",
        "        # Adicione uma coluna 'TagComp' contendo o nome do arquivo sem a extensão\n",
        "        df['nome_arquivo'] = os.path.splitext(arquivo)[0]\n",
        "\n",
        "        # Adicione o DataFrame à lista\n",
        "        dataframes.append(df)\n",
        "# Concatene todos os DataFrames em um único DataFrame\n",
        "BDadosTemp = pd.concat(dataframes, ignore_index=True)\n",
        "BDadosTemp = BDadosTemp[(BDadosTemp != 0).all(axis=1)]\n",
        "BDadosTemp['Value'] = BDadosTemp['Value'].round(1)\n",
        "BDadosTemp = BDadosTemp.rename(columns={'Tag': 'Parametro'})\n",
        "BDadosTemp = BDadosTemp.rename(columns={'nome_arquivo': 'NmeComp'})\n",
        "#BDadosTemp.head()"
      ],
      "metadata": {
        "id": "kZSx7sncFJF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar as linhas onde a coluna 'NmeComp' é igual a 'MainPumpP2' ou 'MainPumpP1'\n",
        "df_MainPumps = BDadosTemp[BDadosTemp['NmeComp'].isin(['MainPumpP2', 'MainPumpP1'])]\n",
        "#df_MainPumps = df_MainPumps.query(\"Parametro == 'Temperature'\")\n",
        "\n",
        "# Utilize o método groupby para agrupar os dados por 'NmeComp' e, em seguida, aplique describe() a cada grupo\n",
        "summary = df_MainPumps.groupby('NmeComp').describe()\n",
        "\n",
        "# Exiba a sumarização dos dados\n",
        "print(df_MainPumps)"
      ],
      "metadata": {
        "id": "fkXDQ9bpPBHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar os dados para MainPumpP1 e MainPumpP2\n",
        "df_MainPumpP1 = df_MainPumps[df_MainPumps['NmeComp'] == 'MainPumpP1']\n",
        "df_MainPumpP2 = df_MainPumps[df_MainPumps['NmeComp'] == 'MainPumpP2']\n",
        "\n",
        "print(df_MainPumpP1)\n",
        "print(df_MainPumpP2)"
      ],
      "metadata": {
        "id": "zsCrBWA9DYkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar as linhas onde a coluna 'NmeComp' é igual a 'MainPumpP2' ou 'MainPumpP1'\n",
        "df_MainPumpsTemp = df_MainPumps[df_MainPumps['Parametro'].isin(['Temperature'])]\n",
        "#df_MainPumps = df_MainPumps.query(\"Parametro == 'Temperature'\")\n",
        "\n",
        "# Utilize o método groupby para agrupar os dados por 'NmeComp' e, em seguida, aplique describe() a cada grupo\n",
        "summary = df_MainPumpsTemp.groupby('NmeComp').describe()\n",
        "\n",
        "# Exiba a sumarização dos dados\n",
        "\n",
        "print(summary)\n",
        "print(df_MainPumpsTemp)"
      ],
      "metadata": {
        "id": "2RN1h8HfSWiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Supondo que 'df_MainPumps' seja o seu DataFrame\n",
        "\n",
        "# Filtrar os valores correspondentes a 'MainPumpP1' e 'MainPumpP2'\n",
        "df_MainPumpsTempP1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']['Value']\n",
        "df_MainPumpsTempP2 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP2']['Value']\n",
        "\n",
        "# Calcular o Z-score para cada valor na Series de 'MainPumpP1'\n",
        "z_scores_MainPumpsTempP1 = stats.zscore(df_MainPumpsTempP1)\n",
        "\n",
        "# Definir um limite para o Z-score (por exemplo, 1 desvio padrão)\n",
        "z_score_threshold = 0.2\n",
        "\n",
        "# Encontrar os índices dos outliers para 'MainPumpP1'\n",
        "outlier_indices_MainPumpsTempP1 = abs(z_scores_MainPumpsTempP1) > z_score_threshold\n",
        "\n",
        "# Remover as linhas que contêm outliers para 'MainPumpP1'\n",
        "dfZscore_MainPumpsTempP1 = df_MainPumpsTempP1[~outlier_indices_MainPumpsTempP1]\n",
        "\n",
        "# Calcular o Z-score para cada valor na Series de 'MainPumpP2'\n",
        "z_scores_MainPumpsTempP2 = stats.zscore(df_MainPumpsTempP2)\n",
        "\n",
        "# Encontrar os índices dos outliers para 'MainPumpP2'\n",
        "outlier_indices_MainPumpsTempP2 = abs(z_scores_MainPumpsTempP2) > z_score_threshold\n",
        "\n",
        "# Remover as linhas que contêm outliers para 'MainPumpP2'\n",
        "dfZscore_MainPumpsTempP2 = df_MainPumpsTempP2[~outlier_indices_MainPumpsTempP2]\n",
        "\n",
        "# Imprimir resumo estatístico do DataFrame limpo para 'MainPumpP1'\n",
        "print(\"Resumo Estatístico para MainPumpP1:\")\n",
        "print(dfZscore_MainPumpsTempP1.describe())\n",
        "\n",
        "# Imprimir resumo estatístico do DataFrame limpo para 'MainPumpP2'\n",
        "print(\"\\nResumo Estatístico para MainPumpP2:\")\n",
        "print(dfZscore_MainPumpsTempP2.describe())\n"
      ],
      "metadata": {
        "id": "W_9tMgD6oVm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Supondo que 'df_MainPumps' seja o seu DataFrame\n",
        "\n",
        "# Aqui está um exemplo de como você poderia definir o outlier_threshold usando o intervalo interquartil (IQR):\n",
        "Q1 = df_MainPumpsTemp['Value'].quantile(0.25)\n",
        "Q3 = df_MainPumpsTemp['Value'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Definir o limite para considerar algo como outlier\n",
        "outlier_threshold = 0.2  # Pode ajustar conforme necessário\n",
        "\n",
        "# Calcular os limites inferior e superior para identificar outliers\n",
        "lower_bound = Q1 - outlier_threshold * IQR\n",
        "upper_bound = Q3 + outlier_threshold * IQR\n",
        "\n",
        "# Filtrar os outliers sem remover os valores maximos superiores\n",
        "df_filtered = df_MainPumpsTemp[(df_MainPumpsTemp['Value'] >= lower_bound)]\n",
        "\n",
        "# Separar os resultados para os componentes 'MainPumpP1' e 'MainPumpP2'\n",
        "df_MainPumpsTempP1IQR = df_filtered[df_filtered['NmeComp'] == 'MainPumpP1']\n",
        "df_MainPumpsTempP2IQR = df_filtered[df_filtered['NmeComp'] == 'MainPumpP2']\n",
        "\n",
        "# Coletar e registrar os valores mínimos e máximos para 'MainPumpP1'\n",
        "min_MainPumpsTempP1 = df_MainPumpsTempP1IQR['Value'].min()\n",
        "max_MainPumpsTempP1 = df_MainPumpsTempP1IQR['Value'].max()\n",
        "\n",
        "# Coletar e registrar os valores mínimos e máximos para 'MainPumpP2'\n",
        "min_MainPumpsTempP2 = df_MainPumpsTempP2IQR['Value'].min()\n",
        "max_MainPumpsTempP2 = df_MainPumpsTempP2IQR['Value'].max()\n",
        "\n",
        "# Exibir os valores mínimos e máximos coletados e registrados\n",
        "print(\"Valores mínimos e máximos para MainPumpP1:\")\n",
        "print(\"Mínimo:\", min_MainPumpsTempP1)\n",
        "print(\"Máximo:\", max_MainPumpsTempP1)\n",
        "\n",
        "print(\"\\nValores mínimos e máximos para MainPumpP2:\")\n",
        "print(\"Mínimo:\", min_MainPumpsTempP2)\n",
        "print(\"Máximo:\", max_MainPumpsTempP2)\n",
        "\n",
        "# Imprimir resumo estatístico do DataFrame limpo para 'MainPumpP1'\n",
        "print(\"Resumo Estatístico para MainPumpP1:\")\n",
        "print(df_MainPumpsTempP1IQR.describe())\n",
        "\n",
        "# Imprimir resumo estatístico do DataFrame limpo para 'MainPumpP2'\n",
        "print(\"\\nResumo Estatístico para MainPumpP2:\")\n",
        "print(df_MainPumpsTempP2IQR.describe())"
      ],
      "metadata": {
        "id": "BYBm-obGMhfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Supondo que df_MainPumpsTemp seja o DataFrame contendo os dados\n",
        "# Filtrando os dados para MainPumpP1 e MainPumpP2\n",
        "data_p1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']['Value']\n",
        "data_p2 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP2']['Value']\n",
        "\n",
        "# Configurando o layout dos subplots\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "# Plotando o histograma para MainPumpP1\n",
        "n1, bins1, patches1 = axes[0].hist(data_p1, bins=30, color='blue', alpha=0.5)\n",
        "axes[0].set_title('Histograma MainPumpP1')\n",
        "axes[0].set_xlabel('Frequência Ocorrência MainPumpP1')\n",
        "axes[0].set_ylabel('Valor de Temperatura')\n",
        "\n",
        "# Adicionando os valores das barras para MainPumpP1\n",
        "for rect1 in patches1:\n",
        "    height1 = rect1.get_height()\n",
        "    axes[0].text(rect1.get_x() + rect1.get_width()/2., height1, '%d' % int(height1),\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "# Plotando o histograma para MainPumpP2\n",
        "n2, bins2, patches2 = axes[1].hist(data_p2, bins=30, color='red', alpha=0.5)\n",
        "axes[1].set_title('Histograma MainPumpP2')\n",
        "axes[1].set_xlabel('Frequência Ocorrência MainPumpP2')\n",
        "axes[1].set_ylabel('Valor de Temperatura')\n",
        "\n",
        "# Adicionando os valores das barras para MainPumpP2\n",
        "for rect2 in patches2:\n",
        "    height2 = rect2.get_height()\n",
        "    axes[1].text(rect2.get_x() + rect2.get_width()/2., height2, '%d' % int(height2),\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "# Ajustando o layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Exibindo os subplots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VobgZL37PXEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df_MainPumpsTempP1 = df_MainPumpsTemp[df_MainPumpsTemp['NmeComp'] == 'MainPumpP1']\n",
        "\n",
        "# Selecionar o parâmetro 'Value' como feature (X) e 'NmeComp' como o alvo (y)\n",
        "X = df_MainPumpsTemp[['Value']].values\n",
        "y = df_MainPumpsTemp['NmeComp']\n",
        "\n",
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criar e treinar o modelo de árvore de decisão\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred = tree_clf.predict(X_test)\n",
        "\n",
        "# Calcular a precisão do modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisão do modelo:\", accuracy)"
      ],
      "metadata": {
        "id": "O4L9C6nO-yc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Exportar a árvore de decisão para um arquivo DOT\n",
        "export_graphviz(tree_clf, out_file=\"tree.dot\",\n",
        "                feature_names=[\"Value\"],\n",
        "                class_names=df_MainPumpsTemp['NmeComp'].unique(),\n",
        "                filled=True, rounded=True)\n",
        "\n",
        "# Converter o arquivo DOT em um formato visual (por exemplo, PNG)\n",
        "with open(\"tree.dot\") as f:\n",
        "    dot_graph = f.read()\n",
        "graphviz.Source(dot_graph)"
      ],
      "metadata": {
        "id": "xsljuOf0ARos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_MainPumps_Trans = df_MainPumpsTemp.pivot(index='Timestamp', columns='NmeComp', values='Value').reset_index()\n",
        "#BDados_Temp_Trans = BDados_Temp_Trans.drop(columns='NmeComp')\n",
        "#NmeComp\tTimestamp\tValue\n",
        "# Visualizar o DataFrame resultante\n",
        "df_MainPumps_Trans.head()"
      ],
      "metadata": {
        "id": "jijC4lkXPPrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Criar um novo dataframe com duas colunas do dataframe sem NA\n",
        "BDadosTemp_MainPump = df_MainPumps_Trans[['MainPumpP1', 'MainPumpP2', 'Timestamp']].dropna()\n",
        "\n",
        "# Limitar os valores de temperatura a 125\n",
        "BDadosTemp_MainPump['MainPumpP1'] = BDadosTemp_MainPump['MainPumpP1'].apply(lambda x: min(x, 125))\n",
        "BDadosTemp_MainPump['MainPumpP2'] = BDadosTemp_MainPump['MainPumpP2'].apply(lambda x: min(x, 125))\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_BDadosTemp_MainPump = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)MainPumpP1', '(C°)MainPumpP2'])\n",
        "\n",
        "# Adicionar traces para MainPumpP1\n",
        "fig_BDadosTemp_MainPump.add_trace(go.Scatter(x=BDadosTemp_MainPump['Timestamp'], y=BDadosTemp_MainPump['MainPumpP1'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP1'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para MainPumpP2\n",
        "fig_BDadosTemp_MainPump.add_trace(go.Scatter(x=BDadosTemp_MainPump['Timestamp'], y=BDadosTemp_MainPump['MainPumpP2'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP2'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Definir limite máximo para o eixo y como 125\n",
        "fig_BDadosTemp_MainPump.update_yaxes(range=[0, 125], row=1, col=1)\n",
        "fig_BDadosTemp_MainPump.update_yaxes(range=[0, 125], row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_BDadosTemp_MainPump.update_layout(title_text='Temperatura MainPumpP1 e MainPumpP2 ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_BDadosTemp_MainPump.show()\n"
      ],
      "metadata": {
        "id": "9p6VXmcYByT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "BDadosTemp_MainPumpAjus = BDadosTemp_MainPump[\n",
        "    (BDadosTemp_MainPump['MainPumpP1'] >= min_MainPumpsTempP1) &\n",
        "    (BDadosTemp_MainPump['MainPumpP1'] <= max_MainPumpsTempP1) &\n",
        "    (BDadosTemp_MainPump['MainPumpP2'] >= min_MainPumpsTempP2) &\n",
        "    (BDadosTemp_MainPump['MainPumpP2'] <= max_MainPumpsTempP2)\n",
        "]\n",
        "\n",
        "# Criar subplots com plotly\n",
        "fig_BDadosTemp_MainPumpAjus = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=['(C°)MainPumpP1', '(C°)MainPumpP2'])\n",
        "\n",
        "# Adicionar traces para MainPumpP1\n",
        "fig_BDadosTemp_MainPumpAjus.add_trace(go.Scatter(x=BDadosTemp_MainPumpAjus['Timestamp'], y=BDadosTemp_MainPumpAjus['MainPumpP1'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP1'),\n",
        "              row=1, col=1)\n",
        "\n",
        "# Adicionar traces para MainPumpP2\n",
        "fig_BDadosTemp_MainPumpAjus.add_trace(go.Scatter(x=BDadosTemp_MainPumpAjus['Timestamp'], y=BDadosTemp_MainPumpAjus['MainPumpP2'],\n",
        "                         mode='lines', fill='tozeroy', line=dict(color='blue'), name='(C°)MainPumpP2'),\n",
        "              row=2, col=1)\n",
        "\n",
        "# Definir limite máximo para o eixo y como 125\n",
        "fig_BDadosTemp_MainPumpAjus.update_yaxes(range=[0, 125], row=1, col=1)\n",
        "fig_BDadosTemp_MainPumpAjus.update_yaxes(range=[0, 125], row=2, col=1)\n",
        "\n",
        "# Atualizar layout com títulos personalizados\n",
        "fig_BDadosTemp_MainPumpAjus.update_layout(title_text='Temperatura MainPumpP1 e MainPumpP2 ao longo do tempo',\n",
        "                  showlegend=False)  # Desativar a legenda global\n",
        "\n",
        "# Exibir o gráfico interativo\n",
        "fig_BDadosTemp_MainPumpAjus.show()"
      ],
      "metadata": {
        "id": "L_3FxfVwStVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Separação dos BDados de temperatura das Bombas P1 e P2\n",
        "df_MainPumpsTemp_IQR = pd.concat([df_MainPumpsTempP1IQR, df_MainPumpsTempP2IQR])\n",
        "\n",
        "# Separando os dados de temperatura e timestamp\n",
        "temperatura = df_MainPumpsTemp_IQR['Value'].values.reshape(-1, 1)  # Reshape para uma matriz 2D\n",
        "timestamp = df_MainPumpsTemp_IQR.index.astype(int).values.reshape(-1, 1)  # Reshape para uma matriz 2D\n",
        "\n",
        "# Criando e treinando o modelo de regressão linear\n",
        "modelo = LinearRegression()\n",
        "modelo.fit(timestamp, temperatura)\n",
        "\n",
        "# Calculando as previsões da tendência linear\n",
        "previsao_tendencia = modelo.predict(timestamp)\n",
        "\n",
        "# Visualizando os coeficientes da reta de regressão\n",
        "coef_angular = modelo.coef_[0][0]\n",
        "intercepto = modelo.intercept_[0]\n",
        "\n",
        "print(\"Coeficiente Angular (Inclinação):\", coef_angular)\n",
        "print(\"Intercepto:\", intercepto)\n"
      ],
      "metadata": {
        "id": "RY_oSzsODG_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Separação dos dados de temperatura das Bombas P1 e P2\n",
        "df_MainPumpsTemp_IQR = pd.concat([df_MainPumpsTempP1IQR])\n",
        "\n",
        "# Dividindo os dados em 4 parcelas de 25%\n",
        "num_splits = 5\n",
        "split_size = len(df_MainPumpsTempP1IQR) // num_splits\n",
        "\n",
        "mod_coef_angular_list = []\n",
        "\n",
        "for i in range(num_splits):\n",
        "    start_index = i * split_size\n",
        "    end_index = start_index + split_size if i < num_splits - 1 else None\n",
        "    split_df = df_MainPumpsTempP1IQR.iloc[start_index:end_index]\n",
        "\n",
        "    # Separando os dados de temperatura e timestamp\n",
        "    temperatura = split_df['Value'].values.reshape(-1, 1)  # Reshape para uma matriz 2D\n",
        "    timestamp = split_df.index.astype(int).values.reshape(-1, 1)  # Reshape para uma matriz 2D\n",
        "\n",
        "    # Criando e treinando o modelo de regressão linear\n",
        "    modelo = LinearRegression()\n",
        "    modelo.fit(timestamp, temperatura)\n",
        "\n",
        "    # Visualizando o coeficiente angular da reta de regressão\n",
        "    coef_angular = modelo.coef_[0][0]\n",
        "    mod_coef_angular = abs(coef_angular)\n",
        "    mod_coef_angular_list.append(mod_coef_angular)\n",
        "\n",
        "    print(f\"Parcela {i + 1}: Módulo do Coeficiente Angular: {mod_coef_angular}\")\n",
        "\n",
        "# Registre os resultados conforme necessário\n"
      ],
      "metadata": {
        "id": "WwiJxVzniDke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Converter a coluna 'Timestamp' para o tipo datetime\n",
        "df_MainPumpsTempP1IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTempP1IQR['Timestamp'])\n",
        "\n",
        "# Definir o número de valores anteriores como 20% do volume de dados originais\n",
        "n_prev_values = int(0.20 * len(df_MainPumpsTempP1IQR))\n",
        "\n",
        "# Criar features e target\n",
        "X = []\n",
        "y = []\n",
        "for i in range(n_prev_values, len(df_MainPumpsTempP1IQR)):\n",
        "    X.append(df_MainPumpsTempP1IQR['Value'].values[i - n_prev_values:i])\n",
        "    y.append(df_MainPumpsTempP1IQR['Value'].values[i])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinar o modelo LinearRegression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões para o período futuro\n",
        "# Supondo que você queira fazer previsões para os próximos 1000 dias\n",
        "n_days_future = 50\n",
        "future_dates = pd.date_range(start=df_MainPumpsTempP1IQR['Timestamp'].iloc[-1], periods=n_days_future + 1)[1:]  # Ignorar o primeiro dia, que já temos\n",
        "last_values = X[-1]  # Últimos valores conhecidos\n",
        "future_values = []\n",
        "\n",
        "for _ in range(n_days_future):\n",
        "    # Fazer previsão para o próximo dia\n",
        "    next_value = linear_model.predict([last_values])[0]\n",
        "    future_values.append(next_value)\n",
        "\n",
        "    # Atualizar os últimos valores conhecidos para incluir a nova previsão\n",
        "    last_values = np.roll(last_values, -1)\n",
        "    last_values[-1] = next_value\n",
        "\n",
        "# Calcular os limites mínimo e máximo para o intervalo de confiança\n",
        "y_pred = linear_model.predict(X_test)\n",
        "std_residuals = np.std(y_test - y_pred)  # Desvio padrão dos resíduos\n",
        "z_critical = 1.96  # Para intervalo de confiança de 95%\n",
        "lower_bound = future_values - z_critical * std_residuals\n",
        "upper_bound = future_values + z_critical * std_residuals\n",
        "\n",
        "# Criar DataFrame com as previsões\n",
        "df_future = pd.DataFrame({'Timestamp': future_dates, 'Value': future_values, 'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound})\n",
        "\n",
        "# Criar o gráfico interativo com Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Adicionar os dados originais\n",
        "fig.add_trace(go.Scatter(x=df_MainPumpsTempP1IQR['Timestamp'], y=df_MainPumpsTempP1IQR['Value'], mode='lines', name='Dados Originais'))\n",
        "\n",
        "# Adicionar a projeção e o intervalo de confiança\n",
        "fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Value'], mode='lines', name='Projeção'))\n",
        "fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Lower_Bound'], mode='lines', line=dict(width=0), marker=dict(color=\"#444\"), name='Limite Inferior'))\n",
        "fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Upper_Bound'], mode='lines', line=dict(width=0), marker=dict(color=\"#444\"), fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', name='Limite Superior'))\n",
        "\n",
        "# Personalizar o layout\n",
        "fig.update_layout(title='Projeção de Temperatura com Intervalo de Confiança',\n",
        "                  xaxis_title='Timestamp',\n",
        "                  yaxis_title='Value',\n",
        "                  hovermode='x',\n",
        "                  template='plotly_white')\n",
        "\n",
        "# Exibir o gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "VgcTZXfZh632"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df_results_corelacao = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Calcular a correlação entre as séries temporais\n",
        "correlation = df_MainPumpsTemp_IQR['Value'].corr(df_MainPumpsTemp_IQR['Value'].shift(1))\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R^2)\n",
        "y_true = df_MainPumpsTemp_IQR['Value'].iloc[1:]  # Remover o primeiro valor, pois não há valor anterior para comparar\n",
        "y_pred = df_MainPumpsTemp_IQR['Value'].shift(1).iloc[1:]  # Remover o primeiro valor, pois não há valor anterior para comparar\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_corelacao = df_results_corelacao.append({'Modelo': 'Correlação', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_corelacao)"
      ],
      "metadata": {
        "id": "Umrh2zzHgnID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Converter a coluna 'Timestamp' para o tipo datetime, se necessário\n",
        "df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])\n",
        "\n",
        "df_results_SVR = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Definir as features (X) e o target (y)\n",
        "X = df_MainPumpsTemp_IQR['Timestamp'].values.reshape(-1, 1)  # Feature é o timestamp\n",
        "y = df_MainPumpsTemp_IQR['Value']\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Treinar o modelo SVR\n",
        "svr_model = SVR(kernel='rbf')  # Use o kernel 'rbf' para SVR\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões com o modelo SVR\n",
        "y_pred_svr = svr_model.predict(X_test)\n",
        "\n",
        "# Avaliar o desempenho do modelo SVR\n",
        "mae = mean_absolute_error(y_test, y_pred_svr)\n",
        "mse = mean_squared_error(y_test, y_pred_svr)\n",
        "r2 = r2_score(y_test, y_pred_svr)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_SVR = df_results_SVR.append({'Modelo': 'SVR', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_SVR)"
      ],
      "metadata": {
        "id": "J9Vuan2H-kxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Converter a coluna 'Timestamp' para o tipo datetime, se necessário\n",
        "df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])\n",
        "\n",
        "df_results_ARIMA = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Definir os dados de treinamento e teste\n",
        "train_size = int(len(df_MainPumpsTemp_IQR) * 0.8)  # 80% dos dados para treinamento\n",
        "train_data = df_MainPumpsTemp_IQR['Value'].iloc[:train_size]\n",
        "test_data = df_MainPumpsTemp_IQR['Value'].iloc[train_size:]\n",
        "\n",
        "# Ajustar o modelo ARIMA aos dados de treinamento\n",
        "order = (5, 1, 0)  # Parâmetros p, d e q do ARIMA (ajuste conforme necessário)\n",
        "model = ARIMA(train_data, order=order)\n",
        "arima_model = model.fit()\n",
        "\n",
        "# Fazer previsões com o modelo ajustado\n",
        "start_index = len(train_data)\n",
        "end_index = start_index + len(test_data) - 1\n",
        "predictions = arima_model.predict(start=start_index, end=end_index, typ='levels')\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R^2)\n",
        "mae = mean_absolute_error(test_data, predictions)\n",
        "mse = mean_squared_error(test_data, predictions)\n",
        "r2 = r2_score(test_data, predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_ARIMA = df_results_ARIMA.append({'Modelo': 'ARIMA', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_ARIMA)"
      ],
      "metadata": {
        "id": "dd91hwQK_WZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Converter a coluna 'Timestamp' para o tipo datetime, se necessário\n",
        "df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])\n",
        "\n",
        "df_results_RLM = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Converter a coluna 'Timestamp' para um formato numérico (por exemplo, número de dias desde o início da época)\n",
        "df_MainPumpsTemp_IQR['NumericTimestamp'] = df_MainPumpsTemp_IQR['Timestamp'].astype(int) / 10**9 / 86400  # Converter nanossegundos para dias\n",
        "\n",
        "# Definir as features (X) e o target (y)\n",
        "X = df_MainPumpsTemp_IQR[['NumericTimestamp']]  # Feature é o timestamp numérico\n",
        "y = df_MainPumpsTemp_IQR['Value']\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ajustar o modelo de regressão linear aos dados de treinamento\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões com o modelo ajustado\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R^2)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_RLM = df_results_RLM.append({'Modelo': 'Regressão Linear Múltipla', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_RLM)"
      ],
      "metadata": {
        "id": "pVecKDQ4C4To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_ModelAR = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "\n",
        "# Ajustar o modelo AR aos dados de temperatura\n",
        "lags = 1  # Especificando o número de lags\n",
        "X = df_MainPumpsTemp_IQR['Value'].shift(lags).dropna()  # Variável de entrada (com lag)\n",
        "y = df_MainPumpsTemp_IQR['Value'][lags:]  # Variável de saída (sem lag)\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "split_index = int(len(X) * 0.8)  # 80% dos dados para treinamento\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Ajustar o modelo AR aos dados de treinamento\n",
        "model = sm.OLS(y_train, sm.add_constant(X_train))\n",
        "ar_model = model.fit()\n",
        "\n",
        "# Fazer previsões com o modelo ajustado\n",
        "predictions = ar_model.predict(sm.add_constant(X_test))\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R^2)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_ModelAR = df_results_ModelAR.append({'Modelo': 'Model AR', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_ModelAR)"
      ],
      "metadata": {
        "id": "vCmO5_RkLYTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Definir o tamanho da janela da Média Móvel\n",
        "window_size = 7  # Por exemplo, usar uma janela de 7 dias\n",
        "\n",
        "# Calcular a Média Móvel\n",
        "df_MainPumpsTemp_IQR['Moving_Average'] = df_MainPumpsTemp_IQR['Value'].rolling(window=window_size).mean()\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_MediaMovel = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Remover os valores nulos resultantes da Média Móvel\n",
        "df_MainPumpsTemp_IQR.dropna(inplace=True)\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R²)\n",
        "mae = mean_absolute_error(df_MainPumpsTemp_IQR['Value'], df_MainPumpsTemp_IQR['Moving_Average'])\n",
        "mse = mean_squared_error(df_MainPumpsTemp_IQR['Value'], df_MainPumpsTemp_IQR['Moving_Average'])\n",
        "r2 = r2_score(df_MainPumpsTemp_IQR['Value'], df_MainPumpsTemp_IQR['Moving_Average'])\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_MediaMovel = df_results_MediaMovel.append({'Modelo': 'Media Movel', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_MediaMovel)"
      ],
      "metadata": {
        "id": "6jceoZ_PLuEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Carregar os dados\n",
        "# Suponha que você tenha um DataFrame df_MainPumpsTemp_IQR com as colunas 'Timestamp' e 'Value'\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_RNNLSTM = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = MinMaxScaler()\n",
        "df_MainPumpsTemp_IQR['Value'] = scaler.fit_transform(df_MainPumpsTemp_IQR[['Value']])\n",
        "\n",
        "# Função para preparar os dados em sequências para RNN\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Definir o comprimento da sequência (número de passos de tempo)\n",
        "seq_length = 10\n",
        "\n",
        "# Criar sequências de dados\n",
        "X, y = create_sequences(df_MainPumpsTemp_IQR['Value'].values, seq_length)\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir o modelo RNN\n",
        "model = Sequential([\n",
        "    LSTM(units=15, input_shape=(X_train.shape[1], 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Fazer previsões\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calcular as métricas de erro\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_RNNLSTM = df_results_RNNLSTM.append({'Modelo': 'RNN-LSTM', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_RNNLSTM)"
      ],
      "metadata": {
        "id": "SfFxvDumyGx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Carregar os dados\n",
        "# Suponha que você tenha um DataFrame df_MainPumps com as colunas 'Timestamp' e 'Value'\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_RNNGRU = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = MinMaxScaler()\n",
        "df_MainPumpsTemp_IQR['Value'] = scaler.fit_transform(df_MainPumpsTemp_IQR[['Value']])\n",
        "\n",
        "# Função para preparar os dados em sequências para GRU\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Definir o comprimento da sequência (número de passos de tempo)\n",
        "seq_length = 10\n",
        "\n",
        "# Criar sequências de dados\n",
        "X, y = create_sequences(df_MainPumpsTemp_IQR['Value'].values, seq_length)\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir o modelo GRU\n",
        "model = Sequential([\n",
        "    GRU(units=15, input_shape=(X_train.shape[1], 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Treinar o modelo\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Fazer previsões\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Calcular as métricas de erro\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_RNNGRU = df_results_RNNGRU.append({'Modelo': 'RNN-GRU', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_RNNGRU)"
      ],
      "metadata": {
        "id": "VRg7uhUqyfcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_StateSpaceModel = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Definindo as datas como índice\n",
        "df_MainPumpsTemp_IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTemp_IQR['Timestamp'])\n",
        "df_MainPumpsTemp_IQR.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Dividindo os dados em treinamento e teste\n",
        "train_data, test_data = train_test_split(df_MainPumpsTemp_IQR, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Ajustando o modelo State Space Model (SSM)\n",
        "model = sm.tsa.UnobservedComponents(train_data['Value'], 'local linear trend')\n",
        "results = model.fit()\n",
        "\n",
        "# Fazendo previsões\n",
        "predictions = results.forecast(steps=len(test_data))\n",
        "\n",
        "# Calculando as métricas de erro\n",
        "mae = mean_absolute_error(test_data['Value'], predictions)\n",
        "mse = mean_squared_error(test_data['Value'], predictions)\n",
        "r2 = r2_score(test_data['Value'], predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_StateSpaceModel = df_results_StateSpaceModel.append({'Modelo': 'State Space Model', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_StateSpaceModel)"
      ],
      "metadata": {
        "id": "hjag8wgT05xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_ExponentialSmoothing = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Ajustar o modelo de suavização exponencial aos dados de temperatura\n",
        "model = ExponentialSmoothing(df_MainPumpsTemp_IQR['Value'])\n",
        "exp_smoothing_model = model.fit()\n",
        "\n",
        "# Fazer previsões com o modelo ajustado\n",
        "predictions = exp_smoothing_model.predict(start=len(df_MainPumpsTemp_IQR), end=len(df_MainPumpsTemp_IQR) + len(X_test) - 1)\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R^2)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_ExponentialSmoothing = df_results_ExponentialSmoothing.append({'Modelo': 'Exponential Smoothing', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_ExponentialSmoothing)\n"
      ],
      "metadata": {
        "id": "RisqlakNc0A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Inicializar o DataFrame para armazenar os resultados\n",
        "df_results_SARIMAX = pd.DataFrame(columns=['Modelo', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "# Ajustar o modelo SARIMAX aos dados de temperatura\n",
        "order = (1, 0, 1)  # Ordem do modelo SARIMA (p, d, q)\n",
        "seasonal_order = (1, 1, 1, 12)  # Ordem sazonal do modelo SARIMA (P, D, Q, S)\n",
        "\n",
        "model = SARIMAX(df_MainPumpsTemp_IQR['Value'], order=order, seasonal_order=seasonal_order)\n",
        "sarimax_model = model.fit()\n",
        "\n",
        "# Fazer previsões com o modelo ajustado\n",
        "predictions = sarimax_model.predict(start=len(df_MainPumpsTemp_IQR), end=len(df_MainPumpsTemp_IQR) + len(X_test) - 1)\n",
        "\n",
        "# Calcular as métricas de erro (MAE, MSE, R^2)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "# Adicionar as métricas de erro ao DataFrame df_results\n",
        "df_results_SARIMAX = df_results_SARIMAX.append({'Modelo': 'SARIMAX', 'MAE': mae, 'MSE': mse, 'R2': r2}, ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame df_results\n",
        "print(df_results_SARIMAX)\n"
      ],
      "metadata": {
        "id": "XSfanaXhiasj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Suponha que você tenha os DataFrames contendo os resultados de diferentes modelos\n",
        "df_results = pd.concat([df_results_StateSpaceModel,\n",
        "                        df_results_SARIMAX,\n",
        "                        df_results_RNNGRU,\n",
        "                        df_results_RNNLSTM,\n",
        "                        df_results_RLM,\n",
        "                        df_results_MediaMovel,\n",
        "                        df_results_ModelAR,\n",
        "                        df_results_ARIMA,\n",
        "                        df_results_SVR,\n",
        "                        df_results_ExponentialSmoothing,\n",
        "                        df_results_corelacao], ignore_index=True)\n",
        "\n",
        "# Exibir o DataFrame resultante\n",
        "print(df_results)\n",
        "df_results.head(10)"
      ],
      "metadata": {
        "id": "XYBIIxopdNeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from github import Github\n",
        "from io import BytesIO\n",
        "\n",
        "# Defina suas credenciais do GitHub\n",
        "seu_token = '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'\n",
        "seu_usuario = 'CidClayQuirino'\n",
        "seu_repositorio = 'rnn-component-lIfe-cycle'\n",
        "# Dicionário de DataFrames com seus nomes originais\n",
        "dataframes = {\n",
        "    'df_MainPumpsTemp': df_MainPumpsTemp,\n",
        "    'df_MainPumpsTemp_IQR': df_MainPumpsTemp_IQR,\n",
        "    'df_MainPumps': df_MainPumps,\n",
        "    'df_results': df_results,\n",
        "    'BDadosTemp':BDadosTemp,\n",
        "}\n",
        "\n",
        "# Função para salvar e enviar para o GitHub\n",
        "def salvar_e_enviar_para_github(dataframe, nome_arquivo, usuario, repositorio, token):\n",
        "    # Salvar DataFrame como CSV em um BytesIO\n",
        "    csv_bytes = BytesIO()\n",
        "    dataframe.to_csv(csv_bytes, index=False)\n",
        "\n",
        "    # Autenticar no GitHub\n",
        "    g = Github(token)\n",
        "\n",
        "    # Obter o repositório\n",
        "    repo = g.get_user(usuario).get_repo(repositorio)\n",
        "\n",
        "    # Criar ou atualizar o arquivo no repositório\n",
        "    try:\n",
        "        arquivo = repo.get_contents(nome_arquivo)\n",
        "        repo.update_file(nome_arquivo, f'Atualizando {nome_arquivo}', csv_bytes.getvalue(), arquivo.sha)\n",
        "        print(f'{nome_arquivo} atualizado com sucesso!')\n",
        "    except Exception as e:\n",
        "        repo.create_file(nome_arquivo, f'Adicionando {nome_arquivo}', csv_bytes.getvalue())\n",
        "        print(f'{nome_arquivo} criado com sucesso!')\n",
        "\n",
        "# Iterar sobre os DataFrames e salvá-los no GitHub\n",
        "for nome, df in dataframes.items():\n",
        "    nome_arquivo = f'{nome}.csv'  # Nome do arquivo usando o nome original do DataFrame\n",
        "    salvar_e_enviar_para_github(df, nome_arquivo, seu_usuario, seu_repositorio, seu_token)\n"
      ],
      "metadata": {
        "id": "fd_ULMRsICx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "opRnSu3wqNqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Separação dos Dados de temperatura das Bombas P1 e P2\n",
        "\n",
        "# Converter a coluna 'Timestamp' para o tipo datetime\n",
        "df_MainPumpsTempP1IQR['Timestamp'] = pd.to_datetime(df_MainPumpsTempP1IQR['Timestamp'])\n",
        "\n",
        "# Definir o número de valores anteriores como 20% do volume de dados originais\n",
        "n_prev_values = int(0.20 * len(df_MainPumpsTempP1IQR))\n",
        "\n",
        "# Criar features e target\n",
        "X = []\n",
        "y = []\n",
        "for i in range(n_prev_values, len(df_MainPumpsTempP1IQR)):\n",
        "    X.append(df_MainPumpsTempP1IQR['Value'].values[i - n_prev_values:i])\n",
        "    y.append(df_MainPumpsTempP1IQR['Value'].values[i])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Normalizar os dados de entrada\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reformular os dados para o formato [samples, timesteps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criar e compilar o modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='tanh', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Treinar o modelo LSTM\n",
        "model.fit(X_train, y_train, epochs=15, batch_size=32, verbose=1)\n",
        "\n",
        "# Fazer previsões para o período futuro\n",
        "n_days_future = 50\n",
        "future_dates = pd.date_range(start=df_MainPumpsTempP1IQR['Timestamp'].iloc[-1], periods=n_days_future + 1)[1:]  # Ignorar o primeiro dia, que já temos\n",
        "last_values = X[-10]  # Últimos valores conhecidos\n",
        "future_values = []\n",
        "\n",
        "for _ in range(n_days_future):\n",
        "    # Fazer previsão para o próximo dia\n",
        "    next_value = model.predict(last_values.reshape(1, n_prev_values, 1))[0][0]\n",
        "    future_values.append(next_value)\n",
        "\n",
        "    # Atualizar os últimos valores conhecidos para incluir a nova previsão\n",
        "    last_values = np.roll(last_values, -1)\n",
        "    last_values[-1] = next_value\n",
        "\n",
        "# Calcular os limites mínimo e máximo para o intervalo de confiança\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "std_residuals = np.std(y_test - y_pred)  # Desvio padrão dos resíduos\n",
        "z_critical = 1.8  # Para intervalo de confiança de 80%\n",
        "lower_bound = future_values - z_critical * std_residuals\n",
        "upper_bound = future_values + z_critical * std_residuals\n",
        "\n",
        "# Criar DataFrame com as previsões\n",
        "df_future = pd.DataFrame({'Timestamp': future_dates, 'Value': future_values, 'Lower_Bound': lower_bound, 'Upper_Bound': upper_bound})\n",
        "\n",
        "# Criar o gráfico interativo com Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Adicionar os dados originais\n",
        "fig.add_trace(go.Scatter(x=df_MainPumpsTempP1IQR['Timestamp'], y=df_MainPumpsTempP1IQR['Value'], mode='lines', name='Dados Originais'))\n",
        "\n",
        "# Adicionar a projeção e o intervalo de confiança\n",
        "fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Value'], mode='lines', name='Projeção'))\n",
        "fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Lower_Bound'], mode='lines', line=dict(width=0), marker=dict(color=\"#444\"), name='Limite Inferior'))\n",
        "fig.add_trace(go.Scatter(x=df_future['Timestamp'], y=df_future['Upper_Bound'], mode='lines', line=dict(width=0), marker=dict(color=\"#444\"), fillcolor='rgba(68, 68, 68, 0.3)', fill='tonexty', name='Limite Superior'))\n",
        "\n",
        "# Personalizar o layout\n",
        "fig.update_layout(title='Projeção de Temperatura com Intervalo de Confiança (LSTM)',\n",
        "                  xaxis_title='Timestamp',\n",
        "                  yaxis_title='Value',\n",
        "                  hovermode='x',\n",
        "                  template='plotly_white')\n",
        "\n",
        "# Exibir o gráfico\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "hoMj5sKwwRbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}